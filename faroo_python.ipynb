{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import scipy as sp\n",
    "#import matplotlib as mpl\n",
    "#import matplotlib.cm as cm\n",
    "#import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "#import time\n",
    "#pd.set_option('display.width', 500)\n",
    "#pd.set_option('display.max_columns', 100)\n",
    "#pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Based on SymSpell:\n",
    "\n",
    "Originally written in C#:\n",
    "\n",
    "// SymSpell: 1 million times faster through Symmetric Delete spelling correction algorithm\n",
    "//\n",
    "// The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup \n",
    "// for a given Damerau-Levenshtein distance. It is six orders of magnitude faster and language independent.\n",
    "// Opposite to other algorithms only deletes are required, no transposes + replaces + inserts.\n",
    "// Transposes + replaces + inserts of the input term are transformed into deletes of the dictionary term.\n",
    "// Replaces and inserts are expensive and language dependent: e.g. Chinese has 70,000 Unicode Han characters!\n",
    "//\n",
    "// Copyright (C) 2015 Wolf Garbe\n",
    "// Version: 3.0\n",
    "// Author: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// Maintainer: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// URL: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "// Description: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "//\n",
    "// License:\n",
    "// This program is free software; you can redistribute it and/or modify\n",
    "// it under the terms of the GNU Lesser General Public License, \n",
    "// version 3.0 (LGPL-3.0) as published by the Free Software Foundation.\n",
    "// http://www.opensource.org/licenses/LGPL-3.0\n",
    "//\n",
    "// Usage: single word + Enter:  Display spelling suggestions\n",
    "//        Enter without input:  Terminate the program\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import re, collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_edit_distance = 3\n",
    "dictionary = {}\n",
    "longest_word_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deletes_list(w):\n",
    "    '''given a word, derive strings with up to max_edit_distance characters deleted'''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(max_edit_distance):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_deletes_list(\"tomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary_entry(w):\n",
    "    '''add word and its derived deletions to dictionary'''\n",
    "    # check if word is already in dictionary\n",
    "    # dictionary entries are in the form: (list of suggested corrections, frequency of word in corpus)\n",
    "    global longest_word_length\n",
    "    new_real_word_added = False\n",
    "    if w in dictionary:\n",
    "        dictionary[w] = (dictionary[w][0], dictionary[w][1] + 1)  # increment count of word in corpus\n",
    "    else:\n",
    "        dictionary[w] = ([], 1)  \n",
    "        longest_word_length = max(longest_word_length, len(w))\n",
    "        \n",
    "    if dictionary[w][1]==1:\n",
    "        # first appearance of word in corpus\n",
    "        # n.b. word may already be in dictionary as a derived word (deleting character from a real word)\n",
    "        # but counter of frequency of word in corpus is not incremented in those cases)\n",
    "        new_real_word_added = True\n",
    "        deletes = get_deletes_list(w)\n",
    "        for item in deletes:\n",
    "            if item in dictionary:\n",
    "                # add (correct) word to delete's suggested correction list if not already there\n",
    "                if item not in dictionary[item][0]:\n",
    "                    dictionary[item][0].append(w)\n",
    "            else:\n",
    "                dictionary[item] = ([w], 0)  # note frequency of word in corpus is not incremented\n",
    "        \n",
    "    return new_real_word_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary(fname):\n",
    "    word_count = 0\n",
    "    print \"Creating dictionary...\" \n",
    "    \n",
    "    with open(fname) as file:    \n",
    "        for line in file:\n",
    "            words = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for word in words:\n",
    "                if create_dictionary_entry(word):\n",
    "                    word_count += 1\n",
    "                    \n",
    "    print \"total unique words in corpus: %i\" % word_count\n",
    "    print \"total items in dictionary (corpus words and deletions): %i\" % len(dictionary)\n",
    "    print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    print \"  length of longest word in corpus: %i\" % longest_word_length\n",
    "        \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "total unique words in corpus: 29157\n",
      "total items in dictionary (corpus words and deletions): 2151998\n",
      "  edit distance for deletions: 3\n",
      "  length of longest word in corpus: 18\n",
      "CPU times: user 27 s, sys: 729 ms, total: 27.7 s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_dictionary(\"/Users/K-Lo/Desktop/big.txt\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b> <p>\n",
    "Can look up a specific entry in the dictionary below. <br>\n",
    "shows (possible corrections, and frequency that entry itself is in corpus - 0 if not a real word) <br>\n",
    "Note: will return key error if there are no corrections (because we are accessing dictionary directly here)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['essentially', 'essentials'], 92)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"essential\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['wrack'], 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"wack\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    \n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    for x in xrange(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suggestions(string, silent=False):\n",
    "    '''return list of suggested corrections for potentially incorrectly spelled word'''\n",
    "    if (len(string) - longest_word_length) > max_edit_distance:\n",
    "        if not silent:\n",
    "            print \"no items in dictionary within maximum edit distance\"\n",
    "        return []\n",
    "    \n",
    "    # suggestions = []\n",
    "    # s_dictionary = {}\n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        # print \"processing '%s'\" % q_item\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus, and not already in suggestion list\n",
    "            # so add to suggestion dictionary, indexed by the word with value (frequency in corpus, edit distance)\n",
    "            # note q_items that are not the input string are shorter than input string \n",
    "            # since only deletes are added (unless manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            ## the suggested corrections for q_item as stored in dictionary (whether or not\n",
    "            ## q_item itself is a valid word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    # compute edit distance\n",
    "                    # if len(sc_item)==len(q_item):\n",
    "                    #    item_dist = len(string) - len(q_item)\n",
    "                    # suggested items should always be longer (unless manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "                    #elif len(q_item)==len(string):\n",
    "                        # a suggestion could be longer or shorter than original string (bug in original FAROO?)\n",
    "                        # if suggestion is from string's suggestion list, sc_item will be longer\n",
    "                        # if suggestion is from a delete's suggestion list, sc_item may be shorter\n",
    "                    #   item_dist = abs(len(sc_item) - len(q_item))\n",
    "                    #else:\n",
    "                    # check in original code, but probably not necessary because string has already checked\n",
    "                    assert sc_item!=string\n",
    "                    \n",
    "                    # calculate edit distance using, for example, Damerau-Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=max_edit_distance:\n",
    "                        assert sc_item in dictionary  # should already be in dictionary if in suggestion list\n",
    "                        suggest_dict[sc_item] = (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a delete) from the queue item\n",
    "        # as additional items to check -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<max_edit_distance and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
    "             \n",
    "    # queue is now empty: convert suggestions in dictionary to list for output\n",
    "    \n",
    "    if not silent:\n",
    "        print \"number of possible corrections: %i\" %len(suggest_dict)\n",
    "        print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    \n",
    "    # output option 1\n",
    "    # sort results by ascending order of edit distance and descending order of frequency\n",
    "    #     and return list of suggested corrections only:\n",
    "    # return sorted(suggest_dict, key = lambda x: (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "    # output option 2\n",
    "    # return list of suggestions with (correction, (frequency in corpus, edit distance)):\n",
    "    as_list = suggest_dict.items()\n",
    "    return sorted(as_list, key = lambda (term, (freq, dist)): (dist, -freq))\n",
    "\n",
    "    '''\n",
    "    Option 1:\n",
    "    get_suggestions(\"file\")\n",
    "    ['file', 'five', 'fire', 'fine', ...]\n",
    "    \n",
    "    Option 2:\n",
    "    get_suggestions(\"file\")\n",
    "    [('file', (5, 0)),\n",
    "     ('five', (67, 1)),\n",
    "     ('fire', (54, 1)),\n",
    "     ('fine', (17, 1))...]  \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Type in word to correct below, to test and get whole list of possible suggestions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible corrections: 97\n",
      "  edit distance for deletions: 3\n",
      "CPU times: user 13.3 ms, sys: 5.18 ms, total: 18.4 ms\n",
      "Wall time: 14.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('coach', (26, 2)),\n",
       " ('chatham', (3, 2)),\n",
       " ('which', (4842, 3)),\n",
       " ('each', (411, 3)),\n",
       " ('change', (150, 3)),\n",
       " ('chair', (135, 3)),\n",
       " ('church', (117, 3)),\n",
       " ('chance', (96, 3)),\n",
       " ('reach', (85, 3)),\n",
       " ('charge', (70, 3)),\n",
       " ('coachman', (52, 3)),\n",
       " ('catch', (45, 3)),\n",
       " ('china', (43, 3)),\n",
       " ('check', (38, 3)),\n",
       " ('charm', (31, 3)),\n",
       " ('chain', (30, 3)),\n",
       " ('chancre', (30, 3)),\n",
       " ('chairs', (29, 3)),\n",
       " ('teach', (27, 3)),\n",
       " ('crack', (20, 3)),\n",
       " ('chase', (20, 3)),\n",
       " ('charcot', (19, 3)),\n",
       " ('chances', (18, 3)),\n",
       " ('clash', (16, 3)),\n",
       " ('couch', (15, 3)),\n",
       " ('chains', (14, 3)),\n",
       " ('chalk', (13, 3)),\n",
       " ('crash', (12, 3)),\n",
       " ('chart', (12, 3)),\n",
       " ('cash', (11, 3)),\n",
       " ('charcoal', (10, 3)),\n",
       " ('chap', (9, 3)),\n",
       " ('chat', (9, 3)),\n",
       " ('chanced', (8, 3)),\n",
       " ('chapel', (8, 3)),\n",
       " ('chaos', (8, 3)),\n",
       " ('masha', (8, 3)),\n",
       " ('hath', (7, 3)),\n",
       " ('chaps', (6, 3)),\n",
       " ('checks', (6, 3)),\n",
       " ('trachea', (5, 3)),\n",
       " ('chased', (5, 3)),\n",
       " ('hack', (4, 3)),\n",
       " ('hoch', (4, 3)),\n",
       " ('ache', (4, 3)),\n",
       " ('cava', (3, 3)),\n",
       " ('chateau', (3, 3)),\n",
       " ('cracks', (3, 3)),\n",
       " ('chale', (3, 3)),\n",
       " ('chafed', (3, 3)),\n",
       " ('beach', (3, 3)),\n",
       " ('charms', (3, 3)),\n",
       " ('coaches', (3, 3)),\n",
       " ('cloaca', (2, 3)),\n",
       " ('hata', (2, 3)),\n",
       " ('tache', (2, 3)),\n",
       " ('cahd', (2, 3)),\n",
       " ('sasha', (2, 3)),\n",
       " ('charon', (2, 3)),\n",
       " ('carta', (2, 3)),\n",
       " ('tasha', (2, 3)),\n",
       " ('clara', (2, 3)),\n",
       " ('charts', (2, 3)),\n",
       " ('chasm', (2, 3)),\n",
       " ('chalky', (2, 3)),\n",
       " ('yacht', (2, 3)),\n",
       " ('pachy', (1, 3)),\n",
       " ('chatty', (1, 3)),\n",
       " ('chuck', (1, 3)),\n",
       " ('bache', (1, 3)),\n",
       " ('czech', (1, 3)),\n",
       " ('czechs', (1, 3)),\n",
       " ('apache', (1, 3)),\n",
       " ('chafe', (1, 3)),\n",
       " ('chaff', (1, 3)),\n",
       " ('chant', (1, 3)),\n",
       " ('chaste', (1, 3)),\n",
       " ('champs', (1, 3)),\n",
       " ('cocoa', (1, 3)),\n",
       " ('champ', (1, 3)),\n",
       " ('ahahah', (1, 3)),\n",
       " ('chats', (1, 3)),\n",
       " ('thatch', (1, 3)),\n",
       " ('percha', (1, 3)),\n",
       " ('chalme', (1, 3)),\n",
       " ('chas', (1, 3)),\n",
       " ('acacia', (1, 3)),\n",
       " ('chante', (1, 3)),\n",
       " ('chorea', (1, 3)),\n",
       " ('chaucer', (1, 3)),\n",
       " ('chaise', (1, 3)),\n",
       " ('shah', (1, 3)),\n",
       " ('chary', (1, 3)),\n",
       " ('charme', (1, 3)),\n",
       " ('omaha', (1, 3)),\n",
       " ('hanna', (1, 3)),\n",
       " ('chalks', (1, 3))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_suggestions(\"chacha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.74 s, sys: 19.5 ms, total: 1.76 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"acamodation\", silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 14.1 ms, total: 2.11 s\n",
      "Wall time: 2.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"acomodation\", silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 s, sys: 114 ms, total: 38.1 s\n",
      "Wall time: 38.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"hous\", silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get best word\n",
    "def best_word(s, silent=False):\n",
    "    try:\n",
    "        return get_suggestions(s, silent)[0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Type in word to correct below, to test and get most suggested word.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible corrections: 337\n",
      "  edit distance for deletions: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('hello', (1, 0))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_word(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_document(fname):\n",
    "    with open(fname) as file:\n",
    "        doc_word_count = 0\n",
    "        corrected_word_count = 0\n",
    "        unknown_word_count = 0\n",
    "        print \"Finding misspelled words in your document...\" \n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            doc_words = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for doc_word in doc_words:\n",
    "                doc_word_count += 1\n",
    "                suggestion = best_word(doc_word, silent=True)\n",
    "                if suggestion is None:\n",
    "                    print \"In line %i, the word %s was not found (no suggested correction)\" % (i, doc_word)\n",
    "                    unknown_word_count += 1\n",
    "                elif suggestion[0]!=doc_word:\n",
    "                    print \"In line %i, %s: suggested correction is %s\" % (i, doc_word, suggestion[0])\n",
    "                    corrected_word_count += 1\n",
    "        \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % doc_word_count\n",
    "    print \"total potential errors found: %i\" % corrected_word_count\n",
    "    print \"total unknown words: %i\" % unknown_word_count\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Provide test text file to correct, and give best suggestion (word level correction only) for errors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "-----\n",
      "total words checked: 8\n",
      "total potential errors found: 0\n",
      "total unknown words: 0\n"
     ]
    }
   ],
   "source": [
    "correct_document(\"/Users/K-Lo/Desktop/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "In line 3, taiths: suggested correction is faith\n",
      "In line 11, the word oonipiittee was not found (no suggested correction)\n",
      "In line 13, tj: suggested correction is to\n",
      "In line 13, mnnff: suggested correction is snuff\n",
      "In line 13, gjpt: suggested correction is get\n",
      "In line 15, bh: suggested correction is by\n",
      "In line 15, snc: suggested correction is sac\n",
      "In line 15, uth: suggested correction is th\n",
      "In line 15, unuer: suggested correction is under\n",
      "In line 20, mthiitt: suggested correction is thirty\n",
      "In line 21, cas: suggested correction is was\n",
      "In line 22, pythian: suggested correction is scythian\n",
      "In line 26, brainin: suggested correction is brain\n",
      "In line 27, jfl: suggested correction is of\n",
      "In line 28, ji: suggested correction is i\n",
      "In line 28, stice: suggested correction is stick\n",
      "In line 28, blaci: suggested correction is black\n",
      "In line 28, eug: suggested correction is dug\n",
      "In line 28, debbs: suggested correction is debts\n",
      "In line 29, nericans: suggested correction is americans\n",
      "In line 30, ainin: suggested correction is again\n",
      "In line 30, ergs: suggested correction is eggs\n",
      "In line 31, trumped: suggested correction is trumpet\n",
      "In line 32, erican: suggested correction is american\n",
      "In line 33, unorthodox: suggested correction is orthodox\n",
      "In line 33, nenance: suggested correction is penance\n",
      "In line 33, thg: suggested correction is the\n",
      "In line 34, sln: suggested correction is son\n",
      "In line 34, rgs: suggested correction is rags\n",
      "In line 38, williaij: suggested correction is william\n",
      "In line 38, eu: suggested correction is e\n",
      "In line 40, fcsf: suggested correction is ff\n",
      "In line 40, ber: suggested correction is be\n",
      "In line 42, unorthodoxy: suggested correction is orthodox\n",
      "In line 42, thpt: suggested correction is that\n",
      "In line 42, the word senbrnrgs was not found (no suggested correction)\n",
      "In line 44, fascism: suggested correction is fascia\n",
      "In line 62, loo: suggested correction is look\n",
      "In line 65, ththn: suggested correction is then\n",
      "In line 65, scbell: suggested correction is bell\n",
      "In line 65, ife: suggested correction is if\n",
      "In line 65, yktcn: suggested correction is skin\n",
      "In line 65, thl: suggested correction is the\n",
      "In line 66, thi: suggested correction is the\n",
      "In line 68, saij: suggested correction is said\n",
      "In line 69, defendants: suggested correction is defendant\n",
      "In line 69, cornr: suggested correction is corner\n",
      "In line 69, nists: suggested correction is fists\n",
      "In line 72, ro: suggested correction is to\n",
      "In line 74, ath: suggested correction is at\n",
      "In line 75, tti: suggested correction is ti\n",
      "In line 75, rg: suggested correction is re\n",
      "In line 75, acrific: suggested correction is pacific\n",
      "In line 77, korea: suggested correction is more\n",
      "In line 78, ro: suggested correction is to\n",
      "In line 78, doatli: suggested correction is death\n",
      "In line 81, ith: suggested correction is it\n",
      "In line 81, ry: suggested correction is by\n",
      "In line 81, kl: suggested correction is ll\n",
      "In line 81, ech: suggested correction is each\n",
      "In line 82, rb: suggested correction is re\n",
      "In line 82, the word ghmhvestigat was not found (no suggested correction)\n",
      "In line 82, nb: suggested correction is no\n",
      "In line 82, rg: suggested correction is re\n",
      "In line 83, rosenbt: suggested correction is rodent\n",
      "In line 83, rgs: suggested correction is rags\n",
      "In line 84, coriritted: suggested correction is committed\n",
      "In line 86, fighti: suggested correction is fight\n",
      "In line 88, bths: suggested correction is baths\n",
      "In line 88, tchf: suggested correction is the\n",
      "In line 91, ro: suggested correction is to\n",
      "In line 91, ijb: suggested correction is in\n",
      "In line 92, telegrnm: suggested correction is telegram\n",
      "In line 92, jillia: suggested correction is william\n",
      "In line 92, patt: suggested correction is part\n",
      "In line 92, rson: suggested correction is son\n",
      "In line 93, ecretdry: suggested correction is secretary\n",
      "In line 95, purview: suggested correction is purves\n",
      "In line 95, rder: suggested correction is order\n",
      "In line 99, gor: suggested correction is for\n",
      "In line 99, dthethg: suggested correction is teeth\n",
      "In line 99, ared: suggested correction is are\n",
      "In line 99, ro: suggested correction is to\n",
      "In line 99, enb: suggested correction is end\n",
      "In line 99, rg: suggested correction is re\n",
      "In line 100, sacc: suggested correction is sac\n",
      "In line 100, vthnz: suggested correction is the\n",
      "In line 100, dri: suggested correction is dry\n",
      "In line 100, yfu: suggested correction is you\n",
      "In line 101, ile: suggested correction is ill\n",
      "In line 101, rosi: suggested correction is rose\n",
      "In line 101, rg: suggested correction is re\n",
      "In line 102, fnir: suggested correction is fair\n",
      "In line 102, jhy: suggested correction is why\n",
      "In line 102, azi: suggested correction is ami\n",
      "In line 103, fascist: suggested correction is fascia\n",
      "In line 104, nb: suggested correction is no\n",
      "-----\n",
      "total words checked: 700\n",
      "total potential errors found: 94\n",
      "total unknown words: 3\n"
     ]
    }
   ],
   "source": [
    "# from http://www.columbia.edu/acis/cria/rosenberg/sample/\n",
    "correct_document(\"/Users/K-Lo/Desktop/OCRsample.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
