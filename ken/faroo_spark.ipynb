{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init('/Users/K-Lo/spark-1.5.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Based on SymSpell:\n",
    "\n",
    "Originally written in C#:\n",
    "\n",
    "// SymSpell: 1 million times faster through Symmetric Delete spelling correction algorithm\n",
    "//\n",
    "// The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup \n",
    "// for a given Damerau-Levenshtein distance. It is six orders of magnitude faster and language independent.\n",
    "// Opposite to other algorithms only deletes are required, no transposes + replaces + inserts.\n",
    "// Transposes + replaces + inserts of the input term are transformed into deletes of the dictionary term.\n",
    "// Replaces and inserts are expensive and language dependent: e.g. Chinese has 70,000 Unicode Han characters!\n",
    "//\n",
    "// Copyright (C) 2015 Wolf Garbe\n",
    "// Version: 3.0\n",
    "// Author: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// Maintainer: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// URL: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "// Description: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "//\n",
    "// License:\n",
    "// This program is free software; you can redistribute it and/or modify\n",
    "// it under the terms of the GNU Lesser General Public License, \n",
    "// version 3.0 (LGPL-3.0) as published by the Free Software Foundation.\n",
    "// http://www.opensource.org/licenses/LGPL-3.0\n",
    "//\n",
    "// Usage: single word + Enter:  Display spelling suggestions\n",
    "//        Enter without input:  Terminate the program\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_partitions = 6  # number of partitions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_edit_distance = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we generate and count all words for the corpus,\n",
    "# then add deletes to the dictionary\n",
    "# this is a slightly different approach from the Faroo algorithm\n",
    "# that may be more appropriate for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general helper functions\n",
    "def get_deletes_list(word):\n",
    "    '''given a word, derive strings with one character deleted'''\n",
    "    # takes a string as input and returns all 1-deletes in a list\n",
    "    # allows for duplicates to be created, will deal with duplicates later to minimize shuffling\n",
    "    if len(word)>1:\n",
    "        return ([word[:c] + word[c+1:] for c in range(len(word))])\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copartitioned(RDD1, RDD2):\n",
    "    '''check if two RDDs are copartitioned'''\n",
    "    return RDD1.partitioner == RDD2.partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_joined_lists(tup):\n",
    "    '''takes as input a tuple in the form (a, b) where each of a, b may be None (but not both) or a list\n",
    "       and returns a concatenated list of unique elements'''\n",
    "    concat_list = []\n",
    "    if tup[1] is None:\n",
    "        concat_list = tup[0]\n",
    "    elif tup[0] is None:\n",
    "        concat_list = tup[1]\n",
    "    else:\n",
    "        concat_list = tup[0] + tup[1]\n",
    "        \n",
    "    return list(set(concat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parallel_create_dictionary(fname):\n",
    "\n",
    "    print \"Creating dictionary...\" \n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # process corpus\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    print \">>> processing corpus words...\"\n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "\n",
    "    # convert file into one long sequence of words\n",
    "    make_all_lower = sc.textFile(fname).map(lambda line: line.lower())\n",
    "    replace_nonalphs = make_all_lower.map(lambda line: regex.sub(' ', line))\n",
    "    all_words = replace_nonalphs.flatMap(lambda line: line.split())\n",
    "\n",
    "    # create core corpus dictionary (i.e. only words appearing in file, no \"deletes\") and cache it\n",
    "    # output RDD of unique_words_with_count: [(word1, count1), (word2, count2), (word3, count3)...]\n",
    "    count_once = all_words.map(lambda word: (word, 1))\n",
    "    unique_words_with_count = count_once.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions).cache()\n",
    "    \n",
    "    # output stats on core corpus\n",
    "    print \"total words processed: %i\" % unique_words_with_count.map(lambda (k, v): v).reduce(lambda a, b: a + b)\n",
    "    print \"total unique words in corpus: %i\" % unique_words_with_count.count()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate deletes list\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # generate list of n-deletes from words in a corpus of the form: [(word1, count1), (word2, count2), ...]\n",
    "    # we will handle possible duplicates after map/reduce:\n",
    "    #     our thinking is the resulting suggestions lists for each delete will be much smaller than the\n",
    "    #     list of potential deletes, and it is more efficient to reduce first, then remove duplicates \n",
    "    #     from these smaller lists (at each worker node), rather than calling `distinct()` on  \n",
    "    #     flattened `expand_deletes` which would require a large shuffle\n",
    "\n",
    "    ##\n",
    "    ## generate 1-deletes\n",
    "    ##\n",
    "     \n",
    "    assert max_edit_distance>0  \n",
    "    print \">>> processing deletions from corpus...\"\n",
    "    \n",
    "    generate_deletes = unique_words_with_count.map(lambda (parent, count): (parent, get_deletes_list(parent)), \n",
    "                                                      preservesPartitioning=True)\n",
    "    expand_deletes = generate_deletes.flatMapValues(lambda x: x)\n",
    "    \n",
    "    # swap and combine, resulting RDD after processing 1-deletes has elements:\n",
    "    # [(delete1, [correct1, correct2...]), (delete2, [correct1, correct2...])...]\n",
    "    swap = expand_deletes.map(lambda (orig, delete): (delete, [orig]))\n",
    "    combine = swap.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)\n",
    "\n",
    "    # cache \"master\" deletes RDD, list of (deletes, [unique suggestions]), for use in loop\n",
    "    deletes = combine.mapValues(lambda sl: list(set(sl))).cache()\n",
    "    \n",
    "    ##\n",
    "    ## generate 2+ deletes\n",
    "    ##\n",
    "    \n",
    "    d_remaining = max_edit_distance - 1  # decreasing counter\n",
    "    queue = deletes\n",
    "\n",
    "    while d_remaining>0:\n",
    "\n",
    "        # generate further deletes\n",
    "        #'expand_new_deletes' will be of the form [(parent \"delete\", [new child \"deletes\"]), ...]\n",
    "        # n.b. this will filter out elements with no new child deletes\n",
    "        gen_new_deletes = queue.map(lambda (x, y): (x, get_deletes_list(x)), preservesPartitioning=True)\n",
    "        expand_new_deletes = gen_new_deletes.flatMapValues(lambda x: x)  \n",
    "\n",
    "        # associate each new child delete with same corpus word suggestions that applied for parent delete\n",
    "        # update queue with [(new child delete, [corpus suggestions]) ...] and cache for next iteration\n",
    "        \n",
    "        assert copartitioned(queue, expand_new_deletes)   # check partitioning for efficient join\n",
    "        get_sugglist_from_parent = expand_new_deletes.join(queue)\n",
    "        new_deletes = get_sugglist_from_parent.map(lambda (p, (c, sl)): (c, sl))\n",
    "        combine_new = new_deletes.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)\n",
    "        queue = combine_new.mapValues(lambda sl: list(set(sl))).cache()\n",
    "\n",
    "        # update \"master\" deletes list with new deletes, and cache for next iteration\n",
    "        \n",
    "        assert copartitioned(deletes, queue)    # check partitioning for efficient join\n",
    "        join_delete_lists = deletes.fullOuterJoin(queue)\n",
    "        deletes = join_delete_lists.mapValues(lambda y: combine_joined_lists(y)).cache()\n",
    "\n",
    "        d_remaining -= 1\n",
    "        \n",
    "    ############\n",
    "    #\n",
    "    # merge deletes with unique corpus words to construct main dictionary\n",
    "    #\n",
    "    ############\n",
    "\n",
    "    # dictionary entries are in the form: (list of suggested corrections, frequency of word in corpus)\n",
    "    # note frequency of word in corpus is not incremented for deletes\n",
    "    deletes_for_dict = deletes.mapValues(lambda sl: (sl, 0)) \n",
    "    unique_words_for_dict = unique_words_with_count.mapValues(lambda count: ([], count))\n",
    "\n",
    "    assert copartitioned(unique_words_for_dict, deletes_for_dict)  # check partitioning for efficient join\n",
    "    join_deletes = unique_words_for_dict.fullOuterJoin(deletes_for_dict)\n",
    "    '''\n",
    "    entries now in form of (word, ( ([], count), ([suggestions], 0) )) for words in both corpus/deletes\n",
    "                           (word, ( ([], count), None               )) for (real) words in corpus only\n",
    "                           (word, ( None       , ([suggestions], 0) )) for (fake) words in deletes only\n",
    "    '''\n",
    "\n",
    "    # if entry has deletes and is a real word, take suggestion list from deletes and count from corpus\n",
    "    dictionary_RDD = join_deletes.mapValues(lambda (xtup, ytup): \n",
    "                                                xtup if ytup is None\n",
    "                                                else ytup if xtup is None\n",
    "                                                else (ytup[0], xtup[1])).cache()\n",
    "\n",
    "    print \"total items in dictionary (corpus words and deletions): %i\" % dictionary_RDD.count()\n",
    "    print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    longest_word_length = unique_words_with_count.map(lambda (k, v): len(k)).reduce(max)\n",
    "    print \"  length of longest word in corpus: %i\" % longest_word_length\n",
    "        \n",
    "    return dictionary_RDD, longest_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      ">>> processing corpus words...\n",
      "total words processed: 1105285\n",
      "total unique words in corpus: 29157\n",
      ">>> processing deletions from corpus...\n",
      "total items in dictionary (corpus words and deletions): 2151998\n",
      "  edit distance for deletions: 3\n",
      "  length of longest word in corpus: 18\n",
      "CPU times: user 101 ms, sys: 26.3 ms, total: 127 ms\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#create_dictionary(\"/Users/K-Lo/Desktop/big.txt\").collect()\n",
    "a, lwl = parallel_create_dictionary(\"/Users/K-Lo/Desktop/big.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b> <p>\n",
    "Can look up a specific entry in the dictionary below. <br>\n",
    "shows (possible corrections, and frequency that entry itself is in corpus - 0 if not a real word)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "a.lookup(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.lookup(\"zzfftt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.take(2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creating dictionary...\n",
    ">>> processing corpus words...\n",
    "total words processed: 1105285\n",
    "total unique words in corpus: 29157\n",
    ">>> processing deletions from corpus...\n",
    "total items in dictionary (corpus words and deletions): 2151998\n",
    "  edit distance for deletions: 3\n",
    "  length of longest word in corpus: 18\n",
    "CPU times: user 108 ms, sys: 26.9 ms, total: 135 ms\n",
    "Wall time: 4min 23s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance (an integer) between sequences (e.g. strings).\n",
    "\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    \n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    for x in xrange(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_n_deletes_list(w, n):\n",
    "    '''given a word, derive strings with up to n characters deleted'''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(n):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parallel_get_suggestions(s, dictRDD, longest_word_length=float('inf'), silent=False):\n",
    "    '''return list of suggested corrections for potentially incorrectly spelled word.\n",
    "    \n",
    "    s: input string\n",
    "    dictRDD: the main dictionary, which includes deletes\n",
    "             entries are in the form of: [(word, ([suggested corrections], frequency of word in corpus)), ...]\n",
    "    longest_word_length: optional identifier of longest real word in dictRDD\n",
    "    silent: verbose output\n",
    "    '''\n",
    "    \n",
    "    if (len(s) - longest_word_length) > max_edit_distance:\n",
    "        if not silent:\n",
    "            print \"no items in dictionary within maximum edit distance\"\n",
    "        return []\n",
    "\n",
    "    ##########\n",
    "    #\n",
    "    # initialize suggestions RDD\n",
    "    # suggestRDD entries: (word, (frequency of word in corpus, edit distance))\n",
    "    #\n",
    "    ##########\n",
    "    \n",
    "    if not silent:\n",
    "        print \">>> looking up suggestions based on input word...\"\n",
    "    \n",
    "    # ensure input RDDs are partitioned\n",
    "    dictRDD = dictRDD.partitionBy(n_partitions).cache()\n",
    "    \n",
    "    # check if input word is in dictionary, and is a word from the corpus (edit distance = 0)\n",
    "    # if so, add input word itself to suggestRDD\n",
    "    exact_match = dictRDD.filter(lambda (w, (sl, freq)): w==s).cache()\n",
    "    suggestRDD = exact_match.mapValues(lambda (sl, freq): (freq, 0)).cache()\n",
    "\n",
    "    ##########\n",
    "    #\n",
    "    # add suggestions for input word\n",
    "    #\n",
    "    ##########\n",
    "    \n",
    "    # the suggested corrections for the item in dictionary (whether or not\n",
    "    # the input string s itself is a valid word or merely a delete) can be valid corrections\n",
    "    sc_items = exact_match.flatMap(lambda (w, (sl, freq)): sl)\n",
    "    calc_dist = sc_items.map(lambda sc: (sc, len(sc)-len(s))).partitionBy(n_partitions).cache()\n",
    "    \n",
    "    assert copartitioned(dictRDD, calc_dist)  # check partitioning for efficient join\n",
    "    get_freq = dictRDD.join(calc_dist)\n",
    "    parent_sugg = get_freq.mapValues(lambda ((sl, freq), dist): (freq, dist))\n",
    "    suggestRDD = suggestRDD.union(parent_sugg).cache()\n",
    "    assert copartitioned(parent_sugg, suggestRDD)  # check partitioning\n",
    "\n",
    "    ##########\n",
    "    #\n",
    "    # process deletes on the input string\n",
    "    #\n",
    "    ##########\n",
    "     \n",
    "    assert max_edit_distance>0\n",
    "    if not silent:\n",
    "        print \">>> processing deletions for input word...\"\n",
    "    \n",
    "    list_deletes_of_s = sc.parallelize(get_n_deletes_list(s, max_edit_distance))\n",
    "    deletes_of_s = list_deletes_of_s.map(lambda k: (k, 0)).partitionBy(n_partitions).cache()\n",
    "    \n",
    "    assert copartitioned(dictRDD, deletes_of_s) # check partitioning for efficient join\n",
    "    check_matches = dictRDD.join(deletes_of_s).cache()\n",
    "    \n",
    "    # if delete is a real word in corpus, add it to suggestion list\n",
    "    del_exact_match = check_matches.filter(lambda (w, ((sl, freq), _)): freq>0)\n",
    "    del_sugg = del_exact_match.map(lambda (w, ((s1, freq), _)): (w, (freq, len(s)-len(w))),\n",
    "                                   preservesPartitioning=True)\n",
    "    suggestRDD = suggestRDD.union(del_sugg).cache()\n",
    "    \n",
    "    # the suggested corrections for the item in dictionary (whether or not\n",
    "    # the delete itself is a valid word or merely a delete) can be valid corrections    \n",
    "    list_sl = check_matches.mapValues(lambda ((sl, freq), _): sl).flatMapValues(lambda x: x)\n",
    "    swap_del = list_sl.map(lambda (w, sc): (sc, 0))\n",
    "    combine_del = swap_del.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions).cache()\n",
    "\n",
    "    # need to recalculate actual Deverau Levenshtein distance to be within max_edit_distance for all deletes\n",
    "    calc_dist = combine_del.map(lambda (w, _): (w, dameraulevenshtein(s, w)),\n",
    "                                       preservesPartitioning=True)\n",
    "    filter_by_dist = calc_dist.filter(lambda (w, dist): dist<=max_edit_distance)\n",
    "    \n",
    "    # get frequencies from main dictionary and add to suggestions list\n",
    "    assert copartitioned(dictRDD, filter_by_dist)  # check partitioning for efficient join\n",
    "    get_freq = dictRDD.join(filter_by_dist)\n",
    "    del_parent_sugg = get_freq.mapValues(lambda ((sl, freq), dist): (freq, dist))\n",
    "    \n",
    "    suggestRDD = suggestRDD.union(del_parent_sugg).distinct().cache()    \n",
    "    \n",
    "    if not silent:\n",
    "        print \"number of possible corrections: %i\" %suggestRDD.count()\n",
    "        print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "\n",
    "    ##########\n",
    "    #\n",
    "    # sort RDD for output\n",
    "    #\n",
    "    ##########\n",
    "    \n",
    "    # suggest_RDD is in the form: [(word, (freq, editdist)), (word, (freq, editdist)), ...]\n",
    "    # there does not seem to be a straightforward way to sort by both primary and secondary keys in Spark\n",
    "    # this is a documented issue: one option is to simply work with a list since there are likely not\n",
    "    # going to be an extremely large number of recommended suggestions\n",
    "    \n",
    "    output = suggestRDD.collect()\n",
    "    \n",
    "    # output option 1\n",
    "    # sort results by ascending order of edit distance and descending order of frequency\n",
    "    #     and return list of suggested corrections only:\n",
    "    # return sorted(output, key = lambda x: (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "    # output option 2\n",
    "    # return list of suggestions with (correction, (frequency in corpus, edit distance)):\n",
    "    # return sorted(output, key = lambda (term, (freq, dist)): (dist, -freq))\n",
    "\n",
    "    return sorted(output, key = lambda (term, (freq, dist)): (dist, -freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> looking up suggestions based on input word...\n",
      ">>> processing deletions for input word...\n",
      "number of possible corrections: 604\n",
      "  edit distance for deletions: 3\n",
      "CPU times: user 82.2 ms, sys: 22.1 ms, total: 104 ms\n",
      "Wall time: 1min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'there', (2972, 0)),\n",
       " (u'these', (1231, 1)),\n",
       " (u'where', (977, 1)),\n",
       " (u'here', (691, 1)),\n",
       " (u'three', (584, 1)),\n",
       " (u'thee', (26, 1)),\n",
       " (u'chere', (9, 1)),\n",
       " (u'theme', (8, 1)),\n",
       " (u'the', (80030, 2)),\n",
       " (u'her', (5284, 2)),\n",
       " (u'were', (4289, 2)),\n",
       " (u'they', (3938, 2)),\n",
       " (u'their', (2955, 2)),\n",
       " (u'them', (2241, 2)),\n",
       " (u'then', (1558, 2)),\n",
       " (u'other', (1502, 2)),\n",
       " (u'those', (1201, 2)),\n",
       " (u'others', (410, 2)),\n",
       " (u'third', (239, 2)),\n",
       " (u'term', (133, 2)),\n",
       " (u'threw', (96, 2)),\n",
       " (u'mere', (79, 2)),\n",
       " (u'theory', (79, 2)),\n",
       " (u'share', (69, 2)),\n",
       " (u'hero', (55, 2)),\n",
       " (u'tree', (42, 2)),\n",
       " (u'hare', (36, 2)),\n",
       " (u'thereby', (32, 2)),\n",
       " (u'sphere', (31, 2)),\n",
       " (u'hers', (30, 2)),\n",
       " (u'thereof', (26, 2)),\n",
       " (u'cher', (25, 2)),\n",
       " (u'tore', (18, 2)),\n",
       " (u'herd', (15, 2)),\n",
       " (u'theirs', (14, 2)),\n",
       " (u'thiers', (13, 2)),\n",
       " (u'shore', (11, 2)),\n",
       " (u'thence', (10, 2)),\n",
       " (u'tete', (9, 2)),\n",
       " (u'sheer', (8, 2)),\n",
       " (u'adhere', (8, 2)),\n",
       " (u'ether', (8, 2)),\n",
       " (u'tver', (7, 2)),\n",
       " (u'therein', (6, 2)),\n",
       " (u'tier', (5, 2)),\n",
       " (u'cheer', (5, 2)),\n",
       " (u'thermo', (5, 2)),\n",
       " (u'herb', (5, 2)),\n",
       " (u'hire', (5, 2)),\n",
       " (u'pere', (4, 2)),\n",
       " (u'theatre', (4, 2)),\n",
       " (u'threes', (4, 2)),\n",
       " (u'thine', (3, 2)),\n",
       " (u'theresa', (3, 2)),\n",
       " (u'thwee', (3, 2)),\n",
       " (u'teres', (3, 2)),\n",
       " (u'theft', (3, 2)),\n",
       " (u'themes', (3, 2)),\n",
       " (u'herr', (3, 2)),\n",
       " (u'tire', (3, 2)),\n",
       " (u'thorn', (2, 2)),\n",
       " (u'vere', (2, 2)),\n",
       " (u'frere', (2, 2)),\n",
       " (u'theah', (1, 2)),\n",
       " (u'thyreo', (1, 2)),\n",
       " (u'thereon', (1, 2)),\n",
       " (u'tyre', (1, 2)),\n",
       " (u'terse', (1, 2)),\n",
       " (u'thebes', (1, 2)),\n",
       " (u'thereto', (1, 2)),\n",
       " (u'zere', (1, 2)),\n",
       " (u'ere', (1, 2)),\n",
       " (u'dere', (1, 2)),\n",
       " (u'tierce', (1, 2)),\n",
       " (u'tiers', (1, 2)),\n",
       " (u'that', (12512, 3)),\n",
       " (u'he', (12401, 3)),\n",
       " (u'this', (4063, 3)),\n",
       " (u'she', (3946, 3)),\n",
       " (u'are', (3630, 3)),\n",
       " (u'have', (3493, 3)),\n",
       " (u'when', (2923, 3)),\n",
       " (u'more', (1997, 3)),\n",
       " (u'pierre', (1964, 3)),\n",
       " (u'time', (1529, 3)),\n",
       " (u'very', (1340, 3)),\n",
       " (u'over', (1282, 3)),\n",
       " (u'than', (1206, 3)),\n",
       " (u'see', (1101, 3)),\n",
       " (u'while', (768, 3)),\n",
       " (u'whole', (744, 3)),\n",
       " (u'head', (725, 3)),\n",
       " (u'every', (650, 3)),\n",
       " (u'heard', (636, 3)),\n",
       " (u'take', (616, 3)),\n",
       " (u'think', (557, 3)),\n",
       " (u'father', (533, 3)),\n",
       " (u'tell', (492, 3)),\n",
       " (u'free', (421, 3)),\n",
       " (u'white', (353, 3)),\n",
       " (u'horse', (334, 3)),\n",
       " (u'nerve', (324, 3)),\n",
       " (u'mother', (312, 3)),\n",
       " (u'thing', (303, 3)),\n",
       " (u'table', (296, 3)),\n",
       " (u'home', (295, 3)),\n",
       " (u'either', (293, 3)),\n",
       " (u'held', (287, 3)),\n",
       " (u'fire', (274, 3)),\n",
       " (u'ever', (274, 3)),\n",
       " (u'heart', (256, 3)),\n",
       " (u'short', (236, 3)),\n",
       " (u'help', (230, 3)),\n",
       " (u'rather', (219, 3)),\n",
       " (u'ten', (219, 3)),\n",
       " (u'trade', (217, 3)),\n",
       " (u'thus', (212, 3)),\n",
       " (u'true', (205, 3)),\n",
       " (u're', (189, 3)),\n",
       " (u'turn', (188, 3)),\n",
       " (u'whose', (188, 3)),\n",
       " (u'hear', (183, 3)),\n",
       " (u'hard', (180, 3)),\n",
       " (u'severe', (173, 3)),\n",
       " (u'tears', (172, 3)),\n",
       " (u'knee', (171, 3)),\n",
       " (u'thin', (166, 3)),\n",
       " (u'tone', (166, 3)),\n",
       " (u'hope', (149, 3)),\n",
       " (u'terms', (148, 3)),\n",
       " (u'sure', (123, 3)),\n",
       " (u'thirty', (123, 3)),\n",
       " (u'eye', (110, 3)),\n",
       " (u'tea', (107, 3)),\n",
       " (u'care', (106, 3)),\n",
       " (u'thank', (105, 3)),\n",
       " (u'berg', (98, 3)),\n",
       " (u'huge', (89, 3)),\n",
       " (u'try', (87, 3)),\n",
       " (u'type', (87, 3)),\n",
       " (u'per', (86, 3)),\n",
       " (u'twice', (84, 3)),\n",
       " (u'heat', (83, 3)),\n",
       " (u'rare', (83, 3)),\n",
       " (u'sharp', (83, 3)),\n",
       " (u'sore', (81, 3)),\n",
       " (u'chest', (81, 3)),\n",
       " (u'agree', (77, 3)),\n",
       " (u'thick', (77, 3)),\n",
       " (u'teeth', (76, 3)),\n",
       " (u'vera', (75, 3)),\n",
       " (u'bare', (74, 3)),\n",
       " (u'shed', (74, 3)),\n",
       " (u'cure', (71, 3)),\n",
       " (u'charge', (70, 3)),\n",
       " (u'stern', (65, 3)),\n",
       " (u'serve', (64, 3)),\n",
       " (u'shape', (62, 3)),\n",
       " (u'piece', (61, 3)),\n",
       " (u'torn', (60, 3)),\n",
       " (u'gathered', (59, 3)),\n",
       " (u'wore', (58, 3)),\n",
       " (u'uttered', (58, 3)),\n",
       " (u'tsar', (56, 3)),\n",
       " (u'pure', (54, 3)),\n",
       " (u'test', (53, 3)),\n",
       " (u'tend', (52, 3)),\n",
       " (u'aware', (52, 3)),\n",
       " (u'th', (51, 3)),\n",
       " (u'trees', (51, 3)),\n",
       " (u'thumb', (51, 3)),\n",
       " (u'toes', (51, 3)),\n",
       " (u'henry', (51, 3)),\n",
       " (u'shirt', (50, 3)),\n",
       " (u'scene', (49, 3)),\n",
       " (u'hart', (49, 3)),\n",
       " (u'throw', (48, 3)),\n",
       " (u'thy', (47, 3)),\n",
       " (u'bore', (46, 3)),\n",
       " (u'tube', (45, 3)),\n",
       " (u'twelve', (43, 3)),\n",
       " (u'ahead', (43, 3)),\n",
       " (u'dare', (43, 3)),\n",
       " (u'thirds', (43, 3)),\n",
       " (u'cheek', (43, 3)),\n",
       " (u'thou', (42, 3)),\n",
       " (u'harm', (42, 3)),\n",
       " (u'shone', (41, 3)),\n",
       " (u'utter', (41, 3)),\n",
       " (u'shell', (41, 3)),\n",
       " (u'hide', (40, 3)),\n",
       " (u'tired', (40, 3)),\n",
       " (u'tied', (40, 3)),\n",
       " (u'heal', (40, 3)),\n",
       " (u'thigh', (39, 3)),\n",
       " (u'title', (39, 3)),\n",
       " (u'check', (38, 3)),\n",
       " (u'wheat', (38, 3)),\n",
       " (u'toe', (36, 3)),\n",
       " (u'trace', (36, 3)),\n",
       " (u'shade', (35, 3)),\n",
       " (u'chose', (34, 3)),\n",
       " (u'hurt', (34, 3)),\n",
       " (u'peri', (33, 3)),\n",
       " (u'tide', (33, 3)),\n",
       " (u'rhode', (33, 3)),\n",
       " (u'shame', (32, 3)),\n",
       " (u'hence', (32, 3)),\n",
       " (u'sire', (32, 3)),\n",
       " (u'charm', (31, 3)),\n",
       " (u'treat', (31, 3)),\n",
       " (u'eve', (30, 3)),\n",
       " (u'sheet', (29, 3)),\n",
       " (u'fee', (29, 3)),\n",
       " (u'theodore', (28, 3)),\n",
       " (u'spare', (27, 3)),\n",
       " (u'lee', (27, 3)),\n",
       " (u'throne', (26, 3)),\n",
       " (u'text', (26, 3)),\n",
       " (u'theater', (25, 3)),\n",
       " (u'scheme', (25, 3)),\n",
       " (u'clerk', (25, 3)),\n",
       " (u'shared', (25, 3)),\n",
       " (u'heroes', (25, 3)),\n",
       " (u'tent', (24, 3)),\n",
       " (u'bier', (24, 3)),\n",
       " (u'ties', (24, 3)),\n",
       " (u'nowhere', (24, 3)),\n",
       " (u'tear', (24, 3)),\n",
       " (u'sheep', (24, 3)),\n",
       " (u'taste', (23, 3)),\n",
       " (u'whence', (23, 3)),\n",
       " (u'tense', (23, 3)),\n",
       " (u'wire', (23, 3)),\n",
       " (u'phase', (21, 3)),\n",
       " (u'niece', (21, 3)),\n",
       " (u'hive', (21, 3)),\n",
       " (u'heel', (21, 3)),\n",
       " (u'tenure', (21, 3)),\n",
       " (u'theories', (21, 3)),\n",
       " (u'tale', (21, 3)),\n",
       " (u'wheel', (21, 3)),\n",
       " (u'chase', (20, 3)),\n",
       " (u'hate', (20, 3)),\n",
       " (u'thorax', (19, 3)),\n",
       " (u'gather', (19, 3)),\n",
       " (u'irene', (18, 3)),\n",
       " (u'geese', (18, 3)),\n",
       " (u'horn', (18, 3)),\n",
       " (u'fathers', (18, 3)),\n",
       " (u'hole', (18, 3)),\n",
       " (u'heap', (17, 3)),\n",
       " (u'truce', (17, 3)),\n",
       " (u'score', (17, 3)),\n",
       " (u'serf', (16, 3)),\n",
       " (u'thread', (16, 3)),\n",
       " (u'tens', (16, 3)),\n",
       " (u'heed', (15, 3)),\n",
       " (u'cheap', (15, 3)),\n",
       " (u'termed', (15, 3)),\n",
       " (u'tyler', (15, 3)),\n",
       " (u'tune', (15, 3)),\n",
       " (u'pre', (15, 3)),\n",
       " (u'tie', (15, 3)),\n",
       " (u'queer', (15, 3)),\n",
       " (u'bee', (15, 3)),\n",
       " (u'erie', (14, 3)),\n",
       " (u'shake', (14, 3)),\n",
       " (u'tavern', (14, 3)),\n",
       " (u'alert', (13, 3)),\n",
       " (u'thirst', (13, 3)),\n",
       " (u'teno', (13, 3)),\n",
       " (u'pierce', (13, 3)),\n",
       " (u'hue', (12, 3)),\n",
       " (u'moore', (12, 3)),\n",
       " (u'tread', (12, 3)),\n",
       " (u'mothers', (12, 3)),\n",
       " (u'chart', (12, 3)),\n",
       " (u'fierce', (12, 3)),\n",
       " (u'store', (12, 3)),\n",
       " (u'thief', (12, 3)),\n",
       " (u'rhine', (12, 3)),\n",
       " (u'tour', (12, 3)),\n",
       " (u'threat', (11, 3)),\n",
       " (u'hemp', (11, 3)),\n",
       " (u'fete', (11, 3)),\n",
       " (u'avert', (11, 3)),\n",
       " (u'flee', (11, 3)),\n",
       " (u'opera', (11, 3)),\n",
       " (u'shoe', (11, 3)),\n",
       " (u'whereas', (11, 3)),\n",
       " (u'hears', (11, 3)),\n",
       " (u'thud', (10, 3)),\n",
       " (u'era', (10, 3)),\n",
       " (u'heir', (10, 3)),\n",
       " (u'acre', (10, 3)),\n",
       " (u'cheered', (10, 3)),\n",
       " (u'hey', (10, 3)),\n",
       " (u'tower', (9, 3)),\n",
       " (u'sabre', (9, 3)),\n",
       " (u'shores', (9, 3)),\n",
       " (u'beer', (9, 3)),\n",
       " (u'cherry', (9, 3)),\n",
       " (u'sera', (9, 3)),\n",
       " (u'user', (8, 3)),\n",
       " (u'thieves', (8, 3)),\n",
       " (u'whereby', (8, 3)),\n",
       " (u'harp', (8, 3)),\n",
       " (u'ushered', (8, 3)),\n",
       " (u'faire', (8, 3)),\n",
       " (u'chyle', (8, 3)),\n",
       " (u'trend', (8, 3)),\n",
       " (u'exert', (8, 3)),\n",
       " (u'team', (8, 3)),\n",
       " (u'thrice', (8, 3)),\n",
       " (u'weren', (8, 3)),\n",
       " (u'heirs', (8, 3)),\n",
       " (u'shelf', (8, 3)),\n",
       " (u'emerge', (8, 3)),\n",
       " (u'verge', (8, 3)),\n",
       " (u'lure', (8, 3)),\n",
       " (u'verse', (8, 3)),\n",
       " (u'fiery', (8, 3)),\n",
       " (u'siege', (8, 3)),\n",
       " (u'core', (7, 3)),\n",
       " (u'averse', (7, 3)),\n",
       " (u'hired', (7, 3)),\n",
       " (u'germ', (7, 3)),\n",
       " (u'fare', (7, 3)),\n",
       " (u'tar', (7, 3)),\n",
       " (u'scherer', (7, 3)),\n",
       " (u'swore', (7, 3)),\n",
       " (u'helen', (6, 3)),\n",
       " (u'chord', (6, 3)),\n",
       " (u'howe', (6, 3)),\n",
       " (u'sneer', (6, 3)),\n",
       " (u'withered', (6, 3)),\n",
       " (u'cheers', (6, 3)),\n",
       " (u'shove', (6, 3)),\n",
       " (u'tinge', (6, 3)),\n",
       " (u'tweed', (6, 3)),\n",
       " (u'whew', (6, 3)),\n",
       " (u'whirl', (6, 3)),\n",
       " (u'sherren', (6, 3)),\n",
       " (u'jerk', (6, 3)),\n",
       " (u'sero', (6, 3)),\n",
       " (u'merge', (6, 3)),\n",
       " (u'spheres', (6, 3)),\n",
       " (u'hen', (6, 3)),\n",
       " (u'adhered', (5, 3)),\n",
       " (u'mare', (5, 3)),\n",
       " (u'wharf', (5, 3)),\n",
       " (u'taper', (5, 3)),\n",
       " (u'zero', (5, 3)),\n",
       " (u'der', (5, 3)),\n",
       " (u'chess', (5, 3)),\n",
       " (u'erb', (5, 3)),\n",
       " (u'deer', (5, 3)),\n",
       " (u'hither', (5, 3)),\n",
       " (u'sheds', (5, 3)),\n",
       " (u'tory', (5, 3)),\n",
       " (u'stare', (5, 3)),\n",
       " (u'glare', (5, 3)),\n",
       " (u'er', (5, 3)),\n",
       " (u'peered', (5, 3)),\n",
       " (u'ordre', (5, 3)),\n",
       " (u'cheese', (5, 3)),\n",
       " (u'inert', (4, 3)),\n",
       " (u'tapers', (4, 3)),\n",
       " (u'hebrew', (4, 3)),\n",
       " (u'herein', (4, 3)),\n",
       " (u'bother', (4, 3)),\n",
       " (u'haze', (4, 3)),\n",
       " (u'attire', (4, 3)),\n",
       " (u'tribe', (4, 3)),\n",
       " (u'peru', (4, 3)),\n",
       " (u'adore', (4, 3)),\n",
       " (u'cheat', (4, 3)),\n",
       " (u'tres', (4, 3)),\n",
       " (u'etre', (4, 3)),\n",
       " (u'steer', (4, 3)),\n",
       " (u'hem', (4, 3)),\n",
       " (u'hell', (4, 3)),\n",
       " (u'users', (4, 3)),\n",
       " (u'wherein', (4, 3)),\n",
       " (u'overt', (4, 3)),\n",
       " (u'whereof', (4, 3)),\n",
       " (u'zheg', (4, 3)),\n",
       " (u'heave', (4, 3)),\n",
       " (u'thorns', (4, 3)),\n",
       " (u'peer', (4, 3)),\n",
       " (u'shine', (4, 3)),\n",
       " (u'rete', (4, 3)),\n",
       " (u'hedge', (4, 3)),\n",
       " (u'hors', (3, 3)),\n",
       " (u'shares', (3, 3)),\n",
       " (u'whale', (3, 3)),\n",
       " (u'notre', (3, 3)),\n",
       " (u'ore', (3, 3)),\n",
       " (u'herds', (3, 3)),\n",
       " (u'ware', (3, 3)),\n",
       " (u'pier', (3, 3)),\n",
       " (u'ashore', (3, 3)),\n",
       " (u'oder', (3, 3)),\n",
       " (u'tsars', (3, 3)),\n",
       " (u'towers', (3, 3)),\n",
       " (u'fera', (3, 3)),\n",
       " (u'dire', (3, 3)),\n",
       " (u'glee', (3, 3)),\n",
       " (u'scare', (3, 3)),\n",
       " (u'tenn', (3, 3)),\n",
       " (u'turf', (3, 3)),\n",
       " (u'tease', (3, 3)),\n",
       " (u'query', (3, 3)),\n",
       " (u'spore', (3, 3)),\n",
       " (u'tiger', (3, 3)),\n",
       " (u'cheery', (3, 3)),\n",
       " (u'turk', (3, 3)),\n",
       " (u'wee', (3, 3)),\n",
       " (u'shave', (3, 3)),\n",
       " (u'thierry', (3, 3)),\n",
       " (u'chale', (3, 3)),\n",
       " (u'tilde', (3, 3)),\n",
       " (u'barre', (3, 3)),\n",
       " (u'tethered', (3, 3)),\n",
       " (u'thecal', (3, 3)),\n",
       " (u'withers', (2, 3)),\n",
       " (u'coerce', (2, 3)),\n",
       " (u'whirr', (2, 3)),\n",
       " (u'jeered', (2, 3)),\n",
       " (u'phone', (2, 3)),\n",
       " (u'cheque', (2, 3)),\n",
       " (u'verb', (2, 3)),\n",
       " (u'fibre', (2, 3)),\n",
       " (u'gathers', (2, 3)),\n",
       " (u'heh', (2, 3)),\n",
       " (u'whine', (2, 3)),\n",
       " (u'neve', (2, 3)),\n",
       " (u'ted', (2, 3)),\n",
       " (u'chef', (2, 3)),\n",
       " (u'azure', (2, 3)),\n",
       " (u'thames', (2, 3)),\n",
       " (u'ver', (2, 3)),\n",
       " (u'thaw', (2, 3)),\n",
       " (u'piers', (2, 3)),\n",
       " (u'spree', (2, 3)),\n",
       " (u'hewn', (2, 3)),\n",
       " (u'heresy', (2, 3)),\n",
       " (u'hur', (2, 3)),\n",
       " (u'lather', (2, 3)),\n",
       " (u'seers', (2, 3)),\n",
       " (u'adheres', (2, 3)),\n",
       " (u'herpes', (2, 3)),\n",
       " (u'votre', (2, 3)),\n",
       " (u'treble', (2, 3)),\n",
       " (u'tape', (2, 3)),\n",
       " (u'doers', (2, 3)),\n",
       " (u'hark', (2, 3)),\n",
       " (u'thresh', (2, 3)),\n",
       " (u'outre', (2, 3)),\n",
       " (u'spire', (2, 3)),\n",
       " (u'tendre', (2, 3)),\n",
       " (u'garre', (2, 3)),\n",
       " (u'tache', (2, 3)),\n",
       " (u'hume', (2, 3)),\n",
       " (u'shred', (2, 3)),\n",
       " (u'sherry', (2, 3)),\n",
       " (u'choke', (2, 3)),\n",
       " (u'horde', (2, 3)),\n",
       " (u'sacre', (2, 3)),\n",
       " (u'shorn', (2, 3)),\n",
       " (u'veered', (2, 3)),\n",
       " (u'hearer', (2, 3)),\n",
       " (u'andre', (2, 3)),\n",
       " (u'hereby', (2, 3)),\n",
       " (u'rhyme', (2, 3)),\n",
       " (u'hares', (2, 3)),\n",
       " (u'guerre', (2, 3)),\n",
       " (u'thesis', (2, 3)),\n",
       " (u'err', (2, 3)),\n",
       " (u'thrive', (2, 3)),\n",
       " (u'tra', (2, 3)),\n",
       " (u'peers', (2, 3)),\n",
       " (u'athlete', (2, 3)),\n",
       " (u'throb', (2, 3)),\n",
       " (u'beri', (2, 3)),\n",
       " (u'adele', (1, 3)),\n",
       " (u'queue', (1, 3)),\n",
       " (u'boire', (1, 3)),\n",
       " (u'thaler', (1, 3)),\n",
       " (u'snore', (1, 3)),\n",
       " (u'thwart', (1, 3)),\n",
       " (u'chorea', (1, 3)),\n",
       " (u'sheen', (1, 3)),\n",
       " (u'clare', (1, 3)),\n",
       " (u'thefts', (1, 3)),\n",
       " (u'usher', (1, 3)),\n",
       " (u'metre', (1, 3)),\n",
       " (u'steve', (1, 3)),\n",
       " (u'tarred', (1, 3)),\n",
       " (u'charme', (1, 3)),\n",
       " (u'boer', (1, 3)),\n",
       " (u'ober', (1, 3)),\n",
       " (u'meme', (1, 3)),\n",
       " (u'trent', (1, 3)),\n",
       " (u'shew', (1, 3)),\n",
       " (u'thoreau', (1, 3)),\n",
       " (u'flare', (1, 3)),\n",
       " (u'theorise', (1, 3)),\n",
       " (u'steered', (1, 3)),\n",
       " (u'nee', (1, 3)),\n",
       " (u'vert', (1, 3)),\n",
       " (u'eerie', (1, 3)),\n",
       " (u'hew', (1, 3)),\n",
       " (u'chime', (1, 3)),\n",
       " (u'berne', (1, 3)),\n",
       " (u'mete', (1, 3)),\n",
       " (u'swede', (1, 3)),\n",
       " (u'phoebe', (1, 3)),\n",
       " (u'henri', (1, 3)),\n",
       " (u'padre', (1, 3)),\n",
       " (u'ghent', (1, 3)),\n",
       " (u'inheres', (1, 3)),\n",
       " (u'tyne', (1, 3)),\n",
       " (u'wert', (1, 3)),\n",
       " (u'avare', (1, 3)),\n",
       " (u'chile', (1, 3)),\n",
       " (u'teemed', (1, 3)),\n",
       " (u'wheal', (1, 3)),\n",
       " (u'therapy', (1, 3)),\n",
       " (u'tires', (1, 3)),\n",
       " (u'helm', (1, 3)),\n",
       " (u'fore', (1, 3)),\n",
       " (u'shale', (1, 3)),\n",
       " (u'thermal', (1, 3)),\n",
       " (u'hyde', (1, 3)),\n",
       " (u'luthers', (1, 3)),\n",
       " (u'luther', (1, 3)),\n",
       " (u'heah', (1, 3)),\n",
       " (u'chary', (1, 3)),\n",
       " (u'chafe', (1, 3)),\n",
       " (u'afore', (1, 3)),\n",
       " (u'lyre', (1, 3)),\n",
       " (u'hale', (1, 3)),\n",
       " (u'sexe', (1, 3)),\n",
       " (u'mather', (1, 3)),\n",
       " (u'yore', (1, 3)),\n",
       " (u'tame', (1, 3)),\n",
       " (u'tante', (1, 3)),\n",
       " (u'tonne', (1, 3)),\n",
       " (u'amene', (1, 3)),\n",
       " (u'diese', (1, 3)),\n",
       " (u'trove', (1, 3)),\n",
       " (u'utters', (1, 3)),\n",
       " (u'ire', (1, 3)),\n",
       " (u'esther', (1, 3)),\n",
       " (u'etherege', (1, 3)),\n",
       " (u'gare', (1, 3)),\n",
       " (u'yer', (1, 3)),\n",
       " (u'trite', (1, 3)),\n",
       " (u'thins', (1, 3)),\n",
       " (u'gene', (1, 3)),\n",
       " (u'quire', (1, 3)),\n",
       " (u'ethel', (1, 3)),\n",
       " (u'tate', (1, 3)),\n",
       " (u'tile', (1, 3)),\n",
       " (u'rire', (1, 3)),\n",
       " (u'madere', (1, 3)),\n",
       " (u'snare', (1, 3)),\n",
       " (u'autre', (1, 3)),\n",
       " (u'sheaf', (1, 3)),\n",
       " (u'chewed', (1, 3)),\n",
       " (u'tigers', (1, 3)),\n",
       " (u'utero', (1, 3)),\n",
       " (u'herder', (1, 3)),\n",
       " (u'hewed', (1, 3)),\n",
       " (u'chiene', (1, 3)),\n",
       " (u'tr', (1, 3)),\n",
       " (u'tiens', (1, 3)),\n",
       " (u'lore', (1, 3)),\n",
       " (u'galere', (1, 3)),\n",
       " (u'gee', (1, 3)),\n",
       " (u'nether', (1, 3)),\n",
       " (u'thwow', (1, 3)),\n",
       " (u'vierge', (1, 3)),\n",
       " (u'tiara', (1, 3)),\n",
       " (u'sterne', (1, 3)),\n",
       " (u'crewe', (1, 3)),\n",
       " (u'truer', (1, 3)),\n",
       " (u'hereof', (1, 3)),\n",
       " (u'twue', (1, 3)),\n",
       " (u'hise', (1, 3)),\n",
       " (u'toured', (1, 3)),\n",
       " (u'obese', (1, 3)),\n",
       " (u'towered', (1, 3)),\n",
       " (u'mele', (1, 3)),\n",
       " (u'chew', (1, 3)),\n",
       " (u'hesse', (1, 3)),\n",
       " (u'te', (1, 3)),\n",
       " (u'litre', (1, 3)),\n",
       " (u'euer', (1, 3)),\n",
       " (u'athens', (1, 3)),\n",
       " (u'freer', (1, 3)),\n",
       " (u'tarry', (1, 3))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "parallel_get_suggestions(\"there\", a, lwl)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "number of possible corrections: 604\n",
    "  edit distance for deletions: 3\n",
    "CPU times: user 82.2 ms, sys: 22.1 ms, total: 104 ms\n",
    "Wall time: 1min 39s\n",
    "Out[21]:\n",
    "[(u'there', (2972, 0)),\n",
    " (u'these', (1231, 1)),\n",
    " (u'where', (977, 1)),\n",
    " (u'here', (691, 1)),\n",
    " (u'three', (584, 1)),\n",
    " (u'thee', (26, 1)),\n",
    " (u'chere', (9, 1)),\n",
    " (u'theme', (8, 1)),\n",
    " (u'the', (80030, 2)),\n",
    " (u'her', (5284, 2)),\n",
    " (u'were', (4289, 2)), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
