{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init('/Users/K-Lo/spark-1.5.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = (pyspark.SparkConf()\n",
    "    .setMaster('local')\n",
    "    .setAppName('pyspark')\n",
    "    .set(\"spark.executor.memory\", \"2g\"))\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Based on SymSpell:\n",
    "\n",
    "Originally written in C#:\n",
    "\n",
    "// SymSpell: 1 million times faster through Symmetric Delete spelling correction algorithm\n",
    "//\n",
    "// The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup \n",
    "// for a given Damerau-Levenshtein distance. It is six orders of magnitude faster and language independent.\n",
    "// Opposite to other algorithms only deletes are required, no transposes + replaces + inserts.\n",
    "// Transposes + replaces + inserts of the input term are transformed into deletes of the dictionary term.\n",
    "// Replaces and inserts are expensive and language dependent: e.g. Chinese has 70,000 Unicode Han characters!\n",
    "//\n",
    "// Copyright (C) 2015 Wolf Garbe\n",
    "// Version: 3.0\n",
    "// Author: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// Maintainer: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// URL: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "// Description: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "//\n",
    "// License:\n",
    "// This program is free software; you can redistribute it and/or modify\n",
    "// it under the terms of the GNU Lesser General Public License, \n",
    "// version 3.0 (LGPL-3.0) as published by the Free Software Foundation.\n",
    "// http://www.opensource.org/licenses/LGPL-3.0\n",
    "//\n",
    "// Usage: single word + Enter:  Display spelling suggestions\n",
    "//        Enter without input:  Terminate the program\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_partitions = 6  # number of partitions to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_edit_distance = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we generate and count all words for the corpus,\n",
    "# then add deletes to the dictionary\n",
    "# this is a slightly different approach from the Faroo algorithm\n",
    "# that may be more appropriate for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general helper functions\n",
    "def get_deletes_list(word):\n",
    "    '''given a word, derive strings with up to max_edit_distance characters deleted'''\n",
    "    # takes a string as input and returns all 1-deletes in a list\n",
    "    # allows for duplicates to be created, will deal with duplicates later to minimize shuffling\n",
    "    if len(word)>1:\n",
    "        return ([word[:c] + word[c+1:] for c in range(len(word))])\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def copartitioned(RDD1, RDD2):\n",
    "    '''check if two RDDs are copartitioned'''\n",
    "    return RDD1.partitioner == RDD2.partitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_joined_lists(tup):\n",
    "    '''takes as input a tuple in the form (a, b) where each of a, b may be None (but not both) or a list\n",
    "       and returns a concatenated list of unique elements'''\n",
    "    concat_list = []\n",
    "    if tup[1] is None:\n",
    "        concat_list = tup[0]\n",
    "    elif tup[0] is None:\n",
    "        concat_list = tup[1]\n",
    "    else:\n",
    "        concat_list = tup[0] + tup[1]\n",
    "        \n",
    "    return list(set(concat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parallel_create_dictionary(fname):\n",
    "\n",
    "    print \"Creating dictionary...\" \n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # process corpus\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    print \">>> processing corpus words...\"\n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "\n",
    "    # convert file into one long sequence of words\n",
    "    make_all_lower = sc.textFile(fname).map(lambda line: line.lower())\n",
    "    replace_nonalphs = make_all_lower.map(lambda line: regex.sub(' ', line))\n",
    "    all_words = replace_nonalphs.flatMap(lambda line: line.split())\n",
    "\n",
    "    # create core corpus dictionary (i.e. only words appearing in file, no \"deletes\") and cache it\n",
    "    # output RDD of unique_words_with_count: [(word1, count1), (word2, count2), (word3, count3)...]\n",
    "    count_once = all_words.map(lambda word: (word, 1))\n",
    "    unique_words_with_count = count_once.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions).cache()\n",
    "    \n",
    "    # output stats on core corpus\n",
    "    print \"total words processed: %i\" % unique_words_with_count.map(lambda (k, v): v).reduce(lambda a, b: a + b)\n",
    "    print \"total unique words in corpus: %i\" % unique_words_with_count.count()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate deletes list\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # generate list of n-deletes from words in a corpus of the form: [(word1, count1), (word2, count2), ...]\n",
    "    # we will handle possible duplicates after map/reduce:\n",
    "    #     our thinking is the resulting suggestions lists for each delete will be much smaller than the\n",
    "    #     list of potential deletes, and it is more efficient to reduce first, then remove duplicates \n",
    "    #     from these smaller lists (at each worker node), rather than calling `distinct()` on  \n",
    "    #     flattened `expand_deletes` which would require a large shuffle\n",
    "\n",
    "    ##\n",
    "    ## generate 1-deletes\n",
    "    ##\n",
    "     \n",
    "    assert max_edit_distance>0  \n",
    "    print \">>> processing deletions from corpus...\"\n",
    "    \n",
    "    generate_deletes = unique_words_with_count.map(lambda (parent, count): (parent, get_deletes_list(parent)), \n",
    "                                                      preservesPartitioning=True)\n",
    "    expand_deletes = generate_deletes.flatMapValues(lambda x: x)\n",
    "    \n",
    "    # swap and combine, resulting RDD after processing 1-deletes has elements:\n",
    "    # [(delete1, [correct1, correct2...]), (delete2, [correct1, correct2...])...]\n",
    "    swap = expand_deletes.map(lambda (orig, delete): (delete, [orig]))\n",
    "    combine = swap.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)\n",
    "\n",
    "    # cache \"master\" deletes RDD, list of (deletes, [unique suggestions]), for use in loop\n",
    "    deletes = combine.mapValues(lambda sl: list(set(sl))).cache()\n",
    "    \n",
    "    ##\n",
    "    ## generate 2+ deletes\n",
    "    ##\n",
    "    \n",
    "    d_remaining = max_edit_distance - 1  # decreasing counter\n",
    "    queue = deletes\n",
    "\n",
    "    while d_remaining>0:\n",
    "\n",
    "        # generate further deletes\n",
    "        #'expand_new_deletes' will be of the form [(parent \"delete\", [new child \"deletes\"]), ...]\n",
    "        # n.b. this will filter out elements with no new child deletes\n",
    "        gen_new_deletes = queue.map(lambda (x, y): (x, get_deletes_list(x)), preservesPartitioning=True)\n",
    "        expand_new_deletes = gen_new_deletes.flatMapValues(lambda x: x)  \n",
    "\n",
    "        # associate each new child delete with same corpus word suggestions that applied for parent delete\n",
    "        # update queue with [(new child delete, [corpus suggestions]) ...] and cache for next iteration\n",
    "        \n",
    "        assert copartitioned(queue, expand_new_deletes)   # check partitioning for efficient join\n",
    "        get_sugglist_from_parent = expand_new_deletes.join(queue)\n",
    "        new_deletes = get_sugglist_from_parent.map(lambda (p, (c, sl)): (c, sl))\n",
    "        combine_new = new_deletes.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)\n",
    "        queue = combine_new.mapValues(lambda sl: list(set(sl))).cache()\n",
    "\n",
    "        # update \"master\" deletes list with new deletes, and cache for next iteration\n",
    "        \n",
    "        assert copartitioned(deletes, queue)    # check partitioning for efficient join\n",
    "        join_delete_lists = deletes.fullOuterJoin(queue)\n",
    "        deletes = join_delete_lists.mapValues(lambda y: combine_joined_lists(y)).cache()\n",
    "\n",
    "        d_remaining -= 1\n",
    "        \n",
    "    ############\n",
    "    #\n",
    "    # merge deletes with unique corpus words to construct main dictionary\n",
    "    #\n",
    "    ############\n",
    "\n",
    "    # dictionary entries are in the form: (list of suggested corrections, frequency of word in corpus)\n",
    "    # note frequency of word in corpus is not incremented for deletes\n",
    "    deletes_for_dict = deletes.mapValues(lambda sl: (sl, 0)) \n",
    "    unique_words_for_dict = unique_words_with_count.mapValues(lambda count: ([], count))\n",
    "\n",
    "    assert copartitioned(unique_words_for_dict, deletes_for_dict)  # check partitioning for efficient join\n",
    "    join_deletes = unique_words_for_dict.fullOuterJoin(deletes_for_dict)\n",
    "    '''\n",
    "    entries now in form of (word, ( ([], count), ([suggestions], 0) )) for words in both corpus/deletes\n",
    "                           (word, ( ([], count), None               )) for (real) words in corpus only\n",
    "                           (word, ( None       , ([suggestions], 0) )) for (fake) words in deletes only\n",
    "    '''\n",
    "\n",
    "    # if entry has deletes and is a real word, take suggestion list from deletes and count from corpus\n",
    "    dictionary_RDD = join_deletes.mapValues(lambda (xtup, ytup): \n",
    "                                                xtup if ytup is None\n",
    "                                                else ytup if xtup is None\n",
    "                                                else (ytup[0], xtup[1])).cache()\n",
    "\n",
    "    print \"total items in dictionary (corpus words and deletions): %i\" % dictionary_RDD.count()\n",
    "    print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    longest_word_length = unique_words_with_count.map(lambda (k, v): len(k)).reduce(max)\n",
    "    print \"  length of longest word in corpus: %i\" % longest_word_length\n",
    "        \n",
    "    return dictionary_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      ">>> processing corpus words...\n",
      "total words processed: 1105285\n",
      "total unique words in corpus: 29157\n",
      ">>> processing deletions from corpus...\n",
      "total items in dictionary (corpus words and deletions): 2151998\n",
      "  edit distance for deletions: 3\n",
      "  length of longest word in corpus: 18\n",
      "CPU times: user 108 ms, sys: 26.9 ms, total: 135 ms\n",
      "Wall time: 4min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#create_dictionary(\"/Users/K-Lo/Desktop/big.txt\").collect()\n",
    "a = parallel_create_dictionary(\"/Users/K-Lo/Desktop/big.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Creating dictionary...\n",
    ">>> processing corpus words...\n",
    "total words processed: 1105285\n",
    "total unique words in corpus: 29157\n",
    ">>> processing deletions from corpus...\n",
    "total items in dictionary (corpus words and deletions): 2151998\n",
    "  edit distance for deletions: 3\n",
    "  length of longest word in corpus: 18\n",
    "CPU times: user 108 ms, sys: 26.9 ms, total: 135 ms\n",
    "Wall time: 4min 23s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
