{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "Creating dictionary...\n",
      "total words processed: 1105285\n",
      "total unique words in corpus: 29157\n",
      "total items in dictionary (corpus words and deletions): 2151998\n",
      "  edit distance for deletions: 3\n",
      "  length of longest word in corpus: 18\n",
      " \n",
      "Word correction\n",
      "---------------\n",
      "Enter your input (or enter to exit): hello\n",
      "('hello', (1, 0))\n",
      " \n",
      "Enter your input (or enter to exit): there\n",
      "('there', (2972, 0))\n",
      " \n",
      "Enter your input (or enter to exit): thinkl\n",
      "('think', (557, 1))\n",
      " \n",
      "Enter your input (or enter to exit): prest\n",
      "('rest', (209, 1))\n",
      " \n",
      "Enter your input (or enter to exit): \n",
      "goodbye\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "serial_single.py\n",
    "\n",
    "v 1.1 last revised 28 Nov 2015\n",
    "\n",
    "This program is a Python version of a spellchecker based on SymSpell, \n",
    "a Symmetric Delete spelling correction algorithm developed by Wolf Garbe \n",
    "and originally written in C#.\n",
    "\n",
    "From the original SymSpell documentation:\n",
    "\n",
    "\"The Symmetric Delete spelling correction algorithm reduces the complexity \n",
    " of edit candidate generation and dictionary lookup for a given Damerau-\n",
    " Levenshtein distance. It is six orders of magnitude faster and language \n",
    " independent. Opposite to other algorithms only deletes are required, \n",
    " no transposes + replaces + inserts. Transposes + replaces + inserts of the \n",
    " input term are transformed into deletes of the dictionary term.\n",
    " Replaces and inserts are expensive and language dependent: \n",
    " e.g. Chinese has 70,000 Unicode Han characters!\"\n",
    "\n",
    "For further information on SymSpell, please consult the original documentation:\n",
    "  URL: blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "  Description: blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "\n",
    "The current version of this program will output all possible suggestions for\n",
    "corrections up to an edit distance (configurable) of max_edit_distance = 3. \n",
    "\n",
    "Changes in this version (1.1):\n",
    "We implement allowing for less verbose options: e.g. when only a single recommended\n",
    "correction is required, the search may terminate early, thereby enhancing performance. \n",
    "\n",
    "To execute program:\n",
    "1. Ensure \"big.txt\" is in the current working directory. This is the corpus\n",
    "   from which the dictionary for the spellchecker will be built.\n",
    "2. When prompted, enter word to be corrected (enter will exit).\n",
    "'''\n",
    "\n",
    "import re\n",
    "\n",
    "max_edit_distance = 3 \n",
    "verbose = 0\n",
    "# 0: top suggestion\n",
    "# 1: all suggestions of smallest edit distance\n",
    "# 2: all suggestions <= max_edit_distance (slower, no early termination)\n",
    "\n",
    "dictionary = {}\n",
    "longest_word_length = 0\n",
    "\n",
    "def get_deletes_list(w):\n",
    "    '''given a word, derive strings with up to max_edit_distance characters deleted'''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(max_edit_distance):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes\n",
    "\n",
    "def create_dictionary_entry(w):\n",
    "    '''add word and its derived deletions to dictionary'''\n",
    "    # check if word is already in dictionary\n",
    "    # dictionary entries are in the form: (list of suggested corrections, frequency of word in corpus)\n",
    "    global longest_word_length\n",
    "    new_real_word_added = False\n",
    "    if w in dictionary:\n",
    "        dictionary[w] = (dictionary[w][0], dictionary[w][1] + 1)  # increment count of word in corpus\n",
    "    else:\n",
    "        dictionary[w] = ([], 1)  \n",
    "        longest_word_length = max(longest_word_length, len(w))\n",
    "        \n",
    "    if dictionary[w][1]==1:\n",
    "        # first appearance of word in corpus\n",
    "        # n.b. word may already be in dictionary as a derived word (deleting character from a real word)\n",
    "        # but counter of frequency of word in corpus is not incremented in those cases)\n",
    "        new_real_word_added = True\n",
    "        deletes = get_deletes_list(w)\n",
    "        for item in deletes:\n",
    "            if item in dictionary:\n",
    "                # add (correct) word to delete's suggested correction list if not already there\n",
    "                if item not in dictionary[item][0]:\n",
    "                    dictionary[item][0].append(w)\n",
    "            else:\n",
    "                dictionary[item] = ([w], 0)  # note frequency of word in corpus is not incremented\n",
    "        \n",
    "    return new_real_word_added\n",
    "\n",
    "def create_dictionary(fname):\n",
    "\n",
    "    total_word_count = 0\n",
    "    unique_word_count = 0\n",
    "    print \"Creating dictionary...\" \n",
    "    \n",
    "    with open(fname) as file:    \n",
    "        for line in file:\n",
    "            words = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for word in words:\n",
    "                total_word_count += 1\n",
    "                if create_dictionary_entry(word):\n",
    "                    unique_word_count += 1\n",
    "    \n",
    "    print \"total words processed: %i\" % total_word_count\n",
    "    print \"total unique words in corpus: %i\" % unique_word_count\n",
    "    print \"total items in dictionary (corpus words and deletions): %i\" % len(dictionary)\n",
    "    print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    print \"  length of longest word in corpus: %i\" % longest_word_length\n",
    "        \n",
    "    return dictionary\n",
    "\n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    This method has not been modified from the original.\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    \n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    for x in xrange(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, silent=False):\n",
    "    '''return list of suggested corrections for potentially incorrectly spelled word'''\n",
    "    if (len(string) - longest_word_length) > max_edit_distance:\n",
    "        if not silent:\n",
    "            print \"no items in dictionary within maximum edit distance\"\n",
    "        return []\n",
    "    \n",
    "    global verbose\n",
    "    suggest_dict = {}\n",
    "    min_suggest_len = float('inf')\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # early exit\n",
    "        if ((verbose<2) and (len(suggest_dict)>0) and ((len(string)-len(q_item))>min_suggest_len)):\n",
    "            break\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus, and not already in suggestion list\n",
    "            # so add to suggestion dictionary, indexed by the word with value (frequency in corpus, edit distance)\n",
    "            # note q_items that are not the input string are shorter than input string \n",
    "            # since only deletes are added (unless manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = (dictionary[q_item][1], len(string) - len(q_item))\n",
    "                # early exit\n",
    "                if ((verbose<2) and (len(string)==len(q_item))):\n",
    "                    break\n",
    "                elif (len(string) - len(q_item)) < min_suggest_len:\n",
    "                    min_suggest_len = len(string) - len(q_item)\n",
    "            \n",
    "            ## the suggested corrections for q_item as stored in dictionary (whether or not\n",
    "            ## q_item itself is a valid word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using, for example, Damerau-Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    # do not add words with greater edit distance if verbose setting not on\n",
    "                    if ((verbose<2) and (item_dist>min_suggest_len)):\n",
    "                        pass\n",
    "                    elif item_dist<=max_edit_distance:\n",
    "                        assert sc_item in dictionary  # should already be in dictionary if in suggestion list\n",
    "                        suggest_dict[sc_item] = (dictionary[sc_item][1], item_dist)\n",
    "                        if item_dist < min_suggest_len:\n",
    "                            min_suggest_len = item_dist\n",
    "                    \n",
    "                    # depending on order words are processed, some words with different edit distances\n",
    "                    # may be entered into suggestions; trim suggestion dictionary if verbose setting not on\n",
    "                    if verbose<2:\n",
    "                        suggest_dict = {k:v for k, v in suggest_dict.items() if v[1]<=min_suggest_len}\n",
    "                \n",
    "        # now generate deletes (e.g. a substring of string or of a delete) from the queue item\n",
    "        # as additional items to check -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "                    \n",
    "        # do not add words with greater edit distance if verbose setting not on\n",
    "        if ((verbose<2) and ((len(string)-len(q_item))>min_suggest_len)):\n",
    "            pass\n",
    "        elif (len(string)-len(q_item))<max_edit_distance and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
    "                     \n",
    "    # queue is now empty: convert suggestions in dictionary to list for output\n",
    "    if not silent and verbose!=0:\n",
    "        print \"number of possible corrections: %i\" %len(suggest_dict)\n",
    "        print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    \n",
    "    # output option 1\n",
    "    # sort results by ascending order of edit distance and descending order of frequency\n",
    "    #     and return list of suggested word corrections only:\n",
    "    # return sorted(suggest_dict, key = lambda x: (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "    # output option 2\n",
    "    # return list of suggestions with (correction, (frequency in corpus, edit distance)):\n",
    "    as_list = suggest_dict.items()\n",
    "    outlist = sorted(as_list, key = lambda (term, (freq, dist)): (dist, -freq))\n",
    "    \n",
    "    if verbose==0:\n",
    "        return outlist[0]\n",
    "    else:\n",
    "        return outlist\n",
    "\n",
    "    '''\n",
    "    Option 1:\n",
    "    get_suggestions(\"file\")\n",
    "    ['file', 'five', 'fire', 'fine', ...]\n",
    "    \n",
    "    Option 2:\n",
    "    get_suggestions(\"file\")\n",
    "    [('file', (5, 0)),\n",
    "     ('five', (67, 1)),\n",
    "     ('fire', (54, 1)),\n",
    "     ('fine', (17, 1))...]  \n",
    "    '''\n",
    "\n",
    "def best_word(s, silent=False):\n",
    "    try:\n",
    "        return get_suggestions(s, silent)[0]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def correct_document(fname, printlist=True):\n",
    "    # correct an entire document\n",
    "    with open(fname) as file:\n",
    "        doc_word_count = 0\n",
    "        corrected_word_count = 0\n",
    "        unknown_word_count = 0\n",
    "        print \"Finding misspelled words in your document...\" \n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            doc_words = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for doc_word in doc_words:\n",
    "                doc_word_count += 1\n",
    "                suggestion = best_word(doc_word, silent=True)\n",
    "                if suggestion is None:\n",
    "                    if printlist:\n",
    "                        print \"In line %i, the word < %s > was not found (no suggested correction)\" % (i, doc_word)\n",
    "                    unknown_word_count += 1\n",
    "                elif suggestion[0]!=doc_word:\n",
    "                    if printlist:\n",
    "                        print \"In line %i, %s: suggested correction is < %s >\" % (i, doc_word, suggestion[0])\n",
    "                    corrected_word_count += 1\n",
    "        \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % doc_word_count\n",
    "    print \"total unknown words: %i\" % unknown_word_count\n",
    "    print \"total potential errors found: %i\" % corrected_word_count\n",
    "\n",
    "    return\n",
    "\n",
    "## main\n",
    "\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print \"Please wait...\"\n",
    "    time.sleep(2)\n",
    "    create_dictionary(\"/Users/K-Lo/Desktop/big.txt\")\n",
    "\n",
    "    print \" \"\n",
    "    print \"Word correction\"\n",
    "    print \"---------------\"\n",
    "    \n",
    "    while True:\n",
    "        word_in = raw_input('Enter your input (or enter to exit): ')\n",
    "        if len(word_in)==0:\n",
    "            print \"goodbye\"\n",
    "            break\n",
    "        print get_suggestions(word_in)\n",
    "        print \" \""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
