{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import scipy as sp\n",
    "#import matplotlib as mpl\n",
    "#import matplotlib.cm as cm\n",
    "#import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "#import time\n",
    "#pd.set_option('display.width', 500)\n",
    "#pd.set_option('display.max_columns', 100)\n",
    "#pd.set_option('display.notebook_repr_html', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Based on SymSpell:\n",
    "\n",
    "Originally written in C#:\n",
    "\n",
    "// SymSpell: 1 million times faster through Symmetric Delete spelling correction algorithm\n",
    "//\n",
    "// The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup \n",
    "// for a given Damerau-Levenshtein distance. It is six orders of magnitude faster and language independent.\n",
    "// Opposite to other algorithms only deletes are required, no transposes + replaces + inserts.\n",
    "// Transposes + replaces + inserts of the input term are transformed into deletes of the dictionary term.\n",
    "// Replaces and inserts are expensive and language dependent: e.g. Chinese has 70,000 Unicode Han characters!\n",
    "//\n",
    "// Copyright (C) 2015 Wolf Garbe\n",
    "// Version: 3.0\n",
    "// Author: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// Maintainer: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// URL: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "// Description: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "//\n",
    "// License:\n",
    "// This program is free software; you can redistribute it and/or modify\n",
    "// it under the terms of the GNU Lesser General Public License, \n",
    "// version 3.0 (LGPL-3.0) as published by the Free Software Foundation.\n",
    "// http://www.opensource.org/licenses/LGPL-3.0\n",
    "//\n",
    "// Usage: single word + Enter:  Display spelling suggestions\n",
    "//        Enter without input:  Terminate the program\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import re, collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_edit_distance = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deletes_list(w):\n",
    "    '''given a word, derive strings with up to max_edit_distance characters deleted'''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(max_edit_distance):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_deletes_list(\"tomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary_entry(w):\n",
    "    '''add word and its derived deletions to dictionary'''\n",
    "    # check if word is already in dictionary\n",
    "    # dictionary entries are in the form: (list of suggested corrections, frequency of word in corpus)\n",
    "    global longest_word_length\n",
    "    new_real_word_added = False\n",
    "    if w in dictionary:\n",
    "        dictionary[w] = (dictionary[w][0], dictionary[w][1] + 1)  # increment count of word in corpus\n",
    "    else:\n",
    "        dictionary[w] = ([], 1)  \n",
    "        longest_word_length = max(longest_word_length, len(w))\n",
    "        \n",
    "    if dictionary[w][1]==1:\n",
    "        # first appearance of word in corpus\n",
    "        # n.b. word may already be in dictionary as a derived word (deleting character from a real word)\n",
    "        # but counter of frequency of word in corpus is not incremented in those cases)\n",
    "        new_real_word_added = True\n",
    "        deletes = get_deletes_list(w)\n",
    "        for item in deletes:\n",
    "            if item in dictionary:\n",
    "                # add (correct) word to delete's suggested correction list if not already there\n",
    "                if item not in dictionary[item][0]:\n",
    "                    dictionary[item][0].append(w)\n",
    "            else:\n",
    "                dictionary[item] = ([w], 0)  # note frequency of word in corpus is not incremented\n",
    "        \n",
    "    return new_real_word_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dictionary(fname):\n",
    "    \n",
    "    word_count = 0\n",
    "    print \"Creating dictionary...\" \n",
    "    \n",
    "    with open(fname) as file:    \n",
    "        for line in file:\n",
    "            words = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for word in words:\n",
    "                assert type(word) != 'str'\n",
    "                if create_dictionary_entry(word):\n",
    "                    word_count += 1\n",
    "                    \n",
    "    print \"total unique words in corpus: %i\" % word_count\n",
    "    print \"total items in dictionary (corpus words and deletions): %i\" % len(dictionary)\n",
    "    print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    print \"  length of longest word in corpus: %i\" % longest_word_length\n",
    "        \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "total unique words in corpus: 29157\n",
      "total items in dictionary (corpus words and deletions): 2151998\n",
      "  edit distance for deletions: 3\n",
      "  length of longest word in corpus: 18\n",
      "CPU times: user 32.7 s, sys: 1.62 s, total: 34.3 s\n",
      "Wall time: 35.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary = {}\n",
    "longest_word_length = 0\n",
    "create_dictionary(\"testdata/big.txt\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b> <p>\n",
    "Can look up a specific entry in the dictionary below. <br>\n",
    "shows (possible corrections, and frequency that entry itself is in corpus - 0 if not a real word) <br>\n",
    "Note: will return key error if there are no corrections (because we are accessing dictionary directly here)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['essentially', 'essentials'], 92)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"essential\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['wrack'], 0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[\"wack\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    \n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    for x in xrange(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suggestions(string, silent=False):\n",
    "    '''return list of suggested corrections for potentially incorrectly spelled word'''\n",
    "    if (len(string) - longest_word_length) > max_edit_distance:\n",
    "        if not silent:\n",
    "            print \"no items in dictionary within maximum edit distance\"\n",
    "        return []\n",
    "    \n",
    "    # suggestions = []\n",
    "    # s_dictionary = {}\n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        # print \"processing '%s'\" % q_item\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus, and not already in suggestion list\n",
    "            # so add to suggestion dictionary, indexed by the word with value (frequency in corpus, edit distance)\n",
    "            # note q_items that are not the input string are shorter than input string \n",
    "            # since only deletes are added (unless manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            ## the suggested corrections for q_item as stored in dictionary (whether or not\n",
    "            ## q_item itself is a valid word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    # compute edit distance\n",
    "                    # if len(sc_item)==len(q_item):\n",
    "                    #    item_dist = len(string) - len(q_item)\n",
    "                    # suggested items should always be longer (unless manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "                    #elif len(q_item)==len(string):\n",
    "                        # a suggestion could be longer or shorter than original string (bug in original FAROO?)\n",
    "                        # if suggestion is from string's suggestion list, sc_item will be longer\n",
    "                        # if suggestion is from a delete's suggestion list, sc_item may be shorter\n",
    "                    #   item_dist = abs(len(sc_item) - len(q_item))\n",
    "                    #else:\n",
    "                    # check in original code, but probably not necessary because string has already checked\n",
    "                    assert sc_item!=string\n",
    "                    \n",
    "                    # calculate edit distance using, for example, Damerau-Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=max_edit_distance:\n",
    "                        assert sc_item in dictionary  # should already be in dictionary if in suggestion list\n",
    "                        suggest_dict[sc_item] = (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a delete) from the queue item\n",
    "        # as additional items to check -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<max_edit_distance and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
    "             \n",
    "    # queue is now empty: convert suggestions in dictionary to list for output\n",
    "    \n",
    "    if not silent:\n",
    "        print \"number of possible corrections: %i\" %len(suggest_dict)\n",
    "        print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    \n",
    "    # output option 1\n",
    "    # sort results by ascending order of edit distance and descending order of frequency\n",
    "    #     and return list of suggested corrections only:\n",
    "    # return sorted(suggest_dict, key = lambda x: (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "    # output option 2\n",
    "    # return list of suggestions with (correction, (frequency in corpus, edit distance)):\n",
    "    as_list = suggest_dict.items()\n",
    "    return sorted(as_list, key = lambda (term, (freq, dist)): (dist, -freq))\n",
    "\n",
    "    '''\n",
    "    Option 1:\n",
    "    get_suggestions(\"file\")\n",
    "    ['file', 'five', 'fire', 'fine', ...]\n",
    "    \n",
    "    Option 2:\n",
    "    get_suggestions(\"file\")\n",
    "    [('file', (5, 0)),\n",
    "     ('five', (67, 1)),\n",
    "     ('fire', (54, 1)),\n",
    "     ('fine', (17, 1))...]  \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Type in word to correct below, to test and get whole list of possible suggestions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible corrections: 103\n",
      "  edit distance for deletions: 3\n",
      "CPU times: user 14 ms, sys: 3.44 ms, total: 17.4 ms\n",
      "Wall time: 14.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('matters', (136, 2)),\n",
       " ('bitten', (13, 2)),\n",
       " ('kitten', (7, 2)),\n",
       " ('listens', (2, 2)),\n",
       " ('battens', (1, 2)),\n",
       " ('smitten', (1, 2)),\n",
       " ('matter', (365, 3)),\n",
       " ('sitting', (269, 3)),\n",
       " ('minutes', (146, 3)),\n",
       " ('written', (117, 3)),\n",
       " ('miles', (110, 3)),\n",
       " ('citizens', (109, 3)),\n",
       " ('letters', (108, 3)),\n",
       " ('listen', (100, 3)),\n",
       " ('cities', (77, 3)),\n",
       " ('bitter', (47, 3)),\n",
       " ('masters', (37, 3)),\n",
       " ('intense', (34, 3)),\n",
       " ('witness', (33, 3)),\n",
       " ('attend', (29, 3)),\n",
       " ('mistress', (24, 3)),\n",
       " ('fitted', (23, 3)),\n",
       " ('mines', (22, 3)),\n",
       " ('fitting', (21, 3)),\n",
       " ('miners', (19, 3)),\n",
       " ('mitenka', (16, 3)),\n",
       " ('tens', (16, 3)),\n",
       " ('sisters', (16, 3)),\n",
       " ('intent', (13, 3)),\n",
       " ('mothers', (12, 3)),\n",
       " ('mutton', (11, 3)),\n",
       " ('bites', (10, 3)),\n",
       " ('sites', (9, 3)),\n",
       " ('omitted', (9, 3)),\n",
       " ('buttons', (8, 3)),\n",
       " ('fitness', (8, 3)),\n",
       " ('rotten', (8, 3)),\n",
       " ('attends', (8, 3)),\n",
       " ('matrena', (8, 3)),\n",
       " ('intend', (8, 3)),\n",
       " ('titles', (7, 3)),\n",
       " ('pitted', (6, 3)),\n",
       " ('matted', (6, 3)),\n",
       " ('omitting', (5, 3)),\n",
       " ('intends', (5, 3)),\n",
       " ('witted', (5, 3)),\n",
       " ('kitchens', (5, 3)),\n",
       " ('hitting', (5, 3)),\n",
       " ('emitted', (4, 3)),\n",
       " ('items', (4, 3)),\n",
       " ('mutter', (4, 3)),\n",
       " ('pitting', (4, 3)),\n",
       " ('mates', (4, 3)),\n",
       " ('emitting', (3, 3)),\n",
       " ('litter', (3, 3)),\n",
       " ('dites', (3, 3)),\n",
       " ('mister', (2, 3)),\n",
       " ('distend', (2, 3)),\n",
       " ('softens', (2, 3)),\n",
       " ('withers', (2, 3)),\n",
       " ('motions', (2, 3)),\n",
       " ('cutters', (2, 3)),\n",
       " ('mien', (2, 3)),\n",
       " ('linens', (2, 3)),\n",
       " ('mints', (2, 3)),\n",
       " ('hastens', (2, 3)),\n",
       " ('gotten', (2, 3)),\n",
       " ('misses', (2, 3)),\n",
       " ('dickens', (2, 3)),\n",
       " ('viens', (2, 3)),\n",
       " ('winters', (2, 3)),\n",
       " ('matins', (2, 3)),\n",
       " ('potters', (2, 3)),\n",
       " ('amiens', (1, 3)),\n",
       " ('metes', (1, 3)),\n",
       " ('filters', (1, 3)),\n",
       " ('pickens', (1, 3)),\n",
       " ('distends', (1, 3)),\n",
       " ('kittenish', (1, 3)),\n",
       " ('gutters', (1, 3)),\n",
       " ('rioters', (1, 3)),\n",
       " ('hatters', (1, 3)),\n",
       " ('sittings', (1, 3)),\n",
       " ('moyens', (1, 3)),\n",
       " ('fetters', (1, 3)),\n",
       " ('fatten', (1, 3)),\n",
       " ('remittent', (1, 3)),\n",
       " ('imitates', (1, 3)),\n",
       " ('mattress', (1, 3)),\n",
       " ('pities', (1, 3)),\n",
       " ('utters', (1, 3)),\n",
       " ('athens', (1, 3)),\n",
       " ('maidens', (1, 3)),\n",
       " ('tiens', (1, 3)),\n",
       " ('matting', (1, 3)),\n",
       " ('milton', (1, 3)),\n",
       " ('mists', (1, 3)),\n",
       " ('hittel', (1, 3)),\n",
       " ('cottons', (1, 3)),\n",
       " ('intents', (1, 3)),\n",
       " ('midges', (1, 3)),\n",
       " ('wotten', (1, 3)),\n",
       " ('patterns', (1, 3))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_suggestions(\"mittens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.39 s, sys: 81.9 ms, total: 2.47 s\n",
      "Wall time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"acamodation\", silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.63 s, sys: 116 ms, total: 2.75 s\n",
      "Wall time: 3.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"acomodation\", silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.9 s, sys: 347 ms, total: 43.3 s\n",
      "Wall time: 44.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"hous\", silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get best word\n",
    "def best_word(s, silent=False):\n",
    "    try:\n",
    "        return get_suggestions(s, silent)[0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Type in word to correct below, to test and get most suggested word.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible corrections: 337\n",
      "  edit distance for deletions: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('hello', (1, 0))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_word(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_document(fname):\n",
    "    with open(fname) as file:\n",
    "        doc_word_count = 0\n",
    "        corrected_word_count = 0\n",
    "        unknown_word_count = 0\n",
    "        print \"Finding misspelled words in your document...\" \n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            doc_words = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for doc_word in doc_words:\n",
    "                doc_word_count += 1\n",
    "                suggestion = best_word(doc_word, silent=True)\n",
    "                if suggestion is None:\n",
    "                    print \"In line %i, the word < %s > was not found (no suggested correction)\" % (i, doc_word)\n",
    "                    unknown_word_count += 1\n",
    "                elif suggestion[0]!=doc_word:\n",
    "                    print \"In line %i, %s: suggested correction is < %s >\" % (i, doc_word, suggestion[0])\n",
    "                    corrected_word_count += 1\n",
    "        \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % doc_word_count\n",
    "    print \"total unknown words: %i\" % unknown_word_count\n",
    "    print \"total potential errors found: %i\" % corrected_word_count\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Provide text file to correct, and give all best word suggestions (word level only) for errors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "In line 4, za: suggested correction is < a >\n",
      "In line 6, tee: suggested correction is < the >\n",
      "-----\n",
      "total words checked: 27\n",
      "total unknown words: 0\n",
      "total potential errors found: 2\n"
     ]
    }
   ],
   "source": [
    "correct_document(\"testdata/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "In line 3, taiths: suggested correction is < faith >\n",
      "In line 11, the word < oonipiittee > was not found (no suggested correction)\n",
      "In line 13, tj: suggested correction is < to >\n",
      "In line 13, mnnff: suggested correction is < snuff >\n",
      "In line 13, gjpt: suggested correction is < get >\n",
      "In line 15, bh: suggested correction is < by >\n",
      "In line 15, snc: suggested correction is < sac >\n",
      "In line 15, uth: suggested correction is < th >\n",
      "In line 15, unuer: suggested correction is < under >\n",
      "In line 20, mthiitt: suggested correction is < thirty >\n",
      "In line 21, cas: suggested correction is < was >\n",
      "In line 22, pythian: suggested correction is < scythian >\n",
      "In line 26, brainin: suggested correction is < brain >\n",
      "In line 27, jfl: suggested correction is < of >\n",
      "In line 28, ji: suggested correction is < i >\n",
      "In line 28, stice: suggested correction is < stick >\n",
      "In line 28, blaci: suggested correction is < black >\n",
      "In line 28, eug: suggested correction is < dug >\n",
      "In line 28, debbs: suggested correction is < debts >\n",
      "In line 29, nericans: suggested correction is < americans >\n",
      "In line 30, ainin: suggested correction is < again >\n",
      "In line 30, ergs: suggested correction is < eggs >\n",
      "In line 31, trumped: suggested correction is < trumpet >\n",
      "In line 32, erican: suggested correction is < american >\n",
      "In line 33, unorthodox: suggested correction is < orthodox >\n",
      "In line 33, nenance: suggested correction is < penance >\n",
      "In line 33, thg: suggested correction is < the >\n",
      "In line 34, sln: suggested correction is < son >\n",
      "In line 34, rgs: suggested correction is < rags >\n",
      "In line 38, williaij: suggested correction is < william >\n",
      "In line 38, eu: suggested correction is < e >\n",
      "In line 40, fcsf: suggested correction is < ff >\n",
      "In line 40, ber: suggested correction is < be >\n",
      "In line 42, unorthodoxy: suggested correction is < orthodox >\n",
      "In line 42, thpt: suggested correction is < that >\n",
      "In line 42, the word < senbrnrgs > was not found (no suggested correction)\n",
      "In line 44, fascism: suggested correction is < fascia >\n",
      "In line 62, loo: suggested correction is < look >\n",
      "In line 65, ththn: suggested correction is < then >\n",
      "In line 65, scbell: suggested correction is < bell >\n",
      "In line 65, ife: suggested correction is < if >\n",
      "In line 65, yktcn: suggested correction is < skin >\n",
      "In line 65, thl: suggested correction is < the >\n",
      "In line 66, thi: suggested correction is < the >\n",
      "In line 68, saij: suggested correction is < said >\n",
      "In line 69, defendants: suggested correction is < defendant >\n",
      "In line 69, cornr: suggested correction is < corner >\n",
      "In line 69, nists: suggested correction is < fists >\n",
      "In line 72, ro: suggested correction is < to >\n",
      "In line 74, ath: suggested correction is < at >\n",
      "In line 75, tti: suggested correction is < ti >\n",
      "In line 75, rg: suggested correction is < re >\n",
      "In line 75, acrific: suggested correction is < pacific >\n",
      "In line 77, korea: suggested correction is < more >\n",
      "In line 78, ro: suggested correction is < to >\n",
      "In line 78, doatli: suggested correction is < death >\n",
      "In line 81, ith: suggested correction is < it >\n",
      "In line 81, ry: suggested correction is < by >\n",
      "In line 81, kl: suggested correction is < ll >\n",
      "In line 81, ech: suggested correction is < each >\n",
      "In line 82, rb: suggested correction is < re >\n",
      "In line 82, the word < ghmhvestigat > was not found (no suggested correction)\n",
      "In line 82, nb: suggested correction is < no >\n",
      "In line 82, rg: suggested correction is < re >\n",
      "In line 83, rosenbt: suggested correction is < rodent >\n",
      "In line 83, rgs: suggested correction is < rags >\n",
      "In line 84, coriritted: suggested correction is < committed >\n",
      "In line 86, fighti: suggested correction is < fight >\n",
      "In line 88, bths: suggested correction is < baths >\n",
      "In line 88, tchf: suggested correction is < the >\n",
      "In line 91, ro: suggested correction is < to >\n",
      "In line 91, ijb: suggested correction is < in >\n",
      "In line 92, telegrnm: suggested correction is < telegram >\n",
      "In line 92, jillia: suggested correction is < william >\n",
      "In line 92, patt: suggested correction is < part >\n",
      "In line 92, rson: suggested correction is < son >\n",
      "In line 93, ecretdry: suggested correction is < secretary >\n",
      "In line 95, purview: suggested correction is < purves >\n",
      "In line 95, rder: suggested correction is < order >\n",
      "In line 99, gor: suggested correction is < for >\n",
      "In line 99, dthethg: suggested correction is < teeth >\n",
      "In line 99, ared: suggested correction is < are >\n",
      "In line 99, ro: suggested correction is < to >\n",
      "In line 99, enb: suggested correction is < end >\n",
      "In line 99, rg: suggested correction is < re >\n",
      "In line 100, sacc: suggested correction is < sac >\n",
      "In line 100, vthnz: suggested correction is < the >\n",
      "In line 100, dri: suggested correction is < dry >\n",
      "In line 100, yfu: suggested correction is < you >\n",
      "In line 101, ile: suggested correction is < ill >\n",
      "In line 101, rosi: suggested correction is < rose >\n",
      "In line 101, rg: suggested correction is < re >\n",
      "In line 102, fnir: suggested correction is < fair >\n",
      "In line 102, jhy: suggested correction is < why >\n",
      "In line 102, azi: suggested correction is < ami >\n",
      "In line 103, fascist: suggested correction is < fascia >\n",
      "In line 104, nb: suggested correction is < no >\n",
      "-----\n",
      "total words checked: 700\n",
      "total unknown words: 3\n",
      "total potential errors found: 94\n"
     ]
    }
   ],
   "source": [
    "# from http://www.columbia.edu/acis/cria/rosenberg/sample/\n",
    "correct_document(\"testdata/OCRsample.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Serial code for context-based improvement <p>\n",
    "Using same corpus to generate, but could consider pre-manufactured lists<br>\n",
    "Could integrate with code above when generating main dictionary\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_of_two = {}\n",
    "first_counts = {}\n",
    "last_of_two = {}\n",
    "last_2counts = {}\n",
    "last_of_three = {}\n",
    "last_3counts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_trigram_dict(fname):\n",
    "    '''creates bigram dictionaries, and 1 trigram dictionary for (__, __, word)'''\n",
    "    word_count = 0\n",
    "    print \"Creating trigram database...\" \n",
    "    \n",
    "    word1 = \"\"\n",
    "    word2 = \"\"\n",
    "    \n",
    "    with open(fname) as file:    \n",
    "        for line in file:\n",
    "            words = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for word in words:\n",
    "                word_count += 1\n",
    "                if len(word1)==0:\n",
    "                    word1 = word\n",
    "                elif len(word2)==0:\n",
    "                    word2 = word\n",
    "                    first_of_two[word1] = [word2]\n",
    "                    first_counts[word1 + \" \" + word2] = 1\n",
    "                    last_of_two[word2] = [word1]\n",
    "                    last_2counts[word1 + \" \" + word2] = 1\n",
    "                else:\n",
    "                    #third word onwards\n",
    "                    if word2 in first_of_two:\n",
    "                        if word in first_of_two[word2]:\n",
    "                            first_counts[word2 + \" \" + word] += 1\n",
    "                        else:\n",
    "                            first_of_two[word2].append(word)\n",
    "                            first_counts[word2 + \" \" + word] = 1\n",
    "                    else:\n",
    "                        first_of_two[word2] = [word]\n",
    "                        first_counts[word2 + \" \" + word] = 1\n",
    "                        \n",
    "                    if word in last_of_two:\n",
    "                        if word2 in last_of_two[word]:\n",
    "                            last_2counts[word2 + \" \" + word] += 1\n",
    "                        else:\n",
    "                            last_of_two[word].append(word2)\n",
    "                            last_2counts[word2 + \" \" + word] = 1\n",
    "                    else:\n",
    "                        last_of_two[word] = [word2]\n",
    "                        last_2counts[word2 + \" \" + word] = 1\n",
    "                        \n",
    "                    back_two = word1 + \" \" + word2\n",
    "                    if word in last_of_three:\n",
    "                        if back_two in last_of_three[word]:\n",
    "                            last_3counts[back_two + \" \" + word] += 1\n",
    "                        else:\n",
    "                            last_of_three[word].append(back_two)\n",
    "                            last_3counts[back_two + \" \" + word] = 1\n",
    "                    else:\n",
    "                        last_of_three[word] = [back_two]\n",
    "                        last_3counts[back_two + \" \" + word] = 1\n",
    "                    \n",
    "                    #increment words\n",
    "                    word1 = word2\n",
    "                    word2 = word\n",
    "    \n",
    "    print \"total unique words in corpus: %i\" % word_count\n",
    "    print \"total number of unique first words of a pair: %i\" % len(first_counts)\n",
    "    print \"total number of unique second words of a pair: %i\" % len(last_2counts)\n",
    "    print \"total number of unique last words of a trigram: %i\" % len(last_3counts)\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trigram database...\n",
      "total unique words in corpus: 1105285\n",
      "total number of unique first words of a pair: 378951\n",
      "total number of unique second words of a pair: 378951\n",
      "total number of unique last words of a trigram: 818180\n",
      "CPU times: user 4min 28s, sys: 2.16 s, total: 4min 30s\n",
      "Wall time: 4min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "create_trigram_dict(\"testdata/big.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first_of_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# last_of_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# last_2counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# last_of_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# last_3counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# last_of_three[\"there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"here and\" in last_of_three[\"there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"here and\" in last_of_three[\"their\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_3counts[\"here and there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"and\" in last_of_two[\"there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"and\" in last_of_two[\"their\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_2counts[\"and there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_2counts[\"and their\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_context(w1, w2, w_list, k):\n",
    "    '''determine most frequent trigram (greater than threshold k) for every w in w_list: (w1, w2, w)'''\n",
    "    # w_list comprises a list of tuples (w, (other info))\n",
    "    leading = w1 + \" \" + w2\n",
    "    \n",
    "    best_word = \"\"\n",
    "    trigram_found = False\n",
    "    best_tri_count = 0\n",
    "    best_bi_count = 0\n",
    "    \n",
    "    for item in w_list:\n",
    "        w = item[0]\n",
    "        if (leading in last_of_three[w]) and (last_3counts[leading + \" \" + w]>max(k, best_tri_count)):\n",
    "            best_tri_count = last_3counts[leading + \" \" + w]\n",
    "            best_word = w\n",
    "            trigram_found = True\n",
    "        elif (not trigram_found) and (w2 in last_of_two[w]) and (last_2counts[w2 + \" \" + w]>max(k, best_bi_count)):\n",
    "            # check bigrams only if no trigram candidates found\n",
    "            best_bi_count = last_2counts[w2 + \" \" + w]\n",
    "            best_word = w\n",
    "    \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correct_document_context(fname, context_threshold=5, num_word_suggestions=5000):\n",
    "    with open(fname) as file:\n",
    "        doc_word_count2 = 0\n",
    "        corrected_word_count = 0\n",
    "        unknown_word_count = 0\n",
    "        mismatches = 0\n",
    "        print \"Finding misspelled words in your document...\" \n",
    "        \n",
    "        word1 = \"\"\n",
    "        word2 = \"\"\n",
    "        first_two = True\n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            doc_words2 = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for doc_word in doc_words2:\n",
    "                suggestion_w = None\n",
    "                suggestion_c = None\n",
    "                doc_word_count2 += 1\n",
    "                \n",
    "                if len(word1)==0:\n",
    "                    word1 = doc_word\n",
    "                elif len(word2)==0:\n",
    "                    word2 = doc_word\n",
    "                else:\n",
    "                    first_two = False\n",
    "                \n",
    "                # character level correction\n",
    "                list_suggestions_byword = get_suggestions(doc_word, silent=True)\n",
    "                if len(list_suggestions_byword)>num_word_suggestions:\n",
    "                    list_suggestions_byword = list_suggestions_byword[0:num_word_suggestions]\n",
    "                if len(list_suggestions_byword)>0:\n",
    "                    suggestion_w = list_suggestions_byword[0][0]  # string of most frequent word tuple\n",
    "                    \n",
    "                    # context correction on all potential suggestions\n",
    "                    # note if there are no suggestions, we do not perform context correction\n",
    "\n",
    "                    # current algorithm - choose the best trigram if count is over threshold\n",
    "                    # if none then choose best bigram (doc_word as last word) if count is over threshold\n",
    "                    # if none then use best character suggestion\n",
    "                    if not first_two:\n",
    "                        suggestion_c = best_context(word1, word2, list_suggestions_byword, context_threshold)\n",
    "                        # print suggestion_c\n",
    "                        if len(suggestion_c)==0:\n",
    "                            suggestion_c = suggestion_w\n",
    "                    else:\n",
    "                        suggestion_c = suggestion_w\n",
    "                \n",
    "                if suggestion_w is None:\n",
    "                    print \"In line %i, the word < %s > was not found (no suggested corrections)\" % (i, doc_word)\n",
    "                    unknown_word_count += 1\n",
    "                elif (suggestion_w!=doc_word) or (suggestion_c!=doc_word):\n",
    "                    print \"In line %i, (%s %s) %s: word level correction is < %s >, context correction is < %s >\" \\\n",
    "                              % (i, word1, word2, doc_word, suggestion_w, suggestion_c)\n",
    "                    corrected_word_count += 1\n",
    "                    if suggestion_w!=suggestion_c:\n",
    "                        mismatches += 1\n",
    "                                        \n",
    "                word1 = word2\n",
    "                word2 = doc_word\n",
    "        \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % doc_word_count2\n",
    "    print \"total unknown words: %i\" % unknown_word_count\n",
    "    print \"total potential errors found: %i\" % corrected_word_count\n",
    "    print \"total mismatches (word level vs. context): %i\" % mismatches\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Provide text file to correct, and give all best word suggestions (word level & context level) for all known words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test.txt\n",
    "\n",
    "#this is a test\n",
    "#this is a test\n",
    "#here is a test\n",
    "#this is ax test\n",
    "#this is za test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "In line 0, (sherlock holmes) she: word level correction is < she >, context correction is < was >\n",
      "In line 0, (holmes she) is: word level correction is < is >, context correction is < was >\n",
      "In line 0, (always the) woman: word level correction is < woman >, context correction is < war >\n",
      "-----\n",
      "total words checked: 8\n",
      "total unknown words: 0\n",
      "total potential errors found: 3\n",
      "total mismatches (word level vs. context): 3\n"
     ]
    }
   ],
   "source": [
    "correct_document_context(\"testdata/verytiny.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_3counts[\"is a very\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'is a test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-a6767cdf5c17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast_3counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is a test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'is a test'"
     ]
    }
   ],
   "source": [
    "last_3counts[\"is a test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_3counts[\"a test this\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "last_3counts[\"a test tube\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct_document_context(\"/Users/K-Lo/Desktop/OCRsample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct_document_context(\"/Users/K-Lo/Desktop/usingengsample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct_document_context(\"/Users/K-Lo/Desktop/usingengsample.txt\", 50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
