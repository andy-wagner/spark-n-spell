{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "  <strong>To run this program, restart notebook, and start executing the cells of this section starting here.</strong> <br><p>\n",
    "  This version parallelizes the word check for all the words in a document, using word-level correction. Since SPARK does not permit RDD manipulation from within an RDD transformation (i.e. no parallelism within a parallel task), we converted the `get_suggestions` function that acts on an individual word to a serial method. This allows us to then parallelize across multiple words in a document. <i>This is a reasonable trade off when the number of words in a document is much larger compared to the number of suggestions that will likely be found for any given word)</i>. <br><p>\n",
    "  Also note the (modified) `no_RDD_get_suggestions` function still returns an entire list of all possible suggestions to the calling function (e.g. for context checking), even if only the top match is used or required. Future improvements may be made to `no_RDD_get_suggestions` to terminate early once a \"top\" match (e.g. minimum edit distance) is found; a speedup in that function will in turn lead to a performance improvement of the document checking function as well.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "v 4.0 last revised 27 Nov 2015\n",
    "\n",
    "This program is a Spark (PySpark) version of a spellchecker based on SymSpell, \n",
    "a Symmetric Delete spelling correction algorithm developed by Wolf Garbe \n",
    "and originally written in C#.\n",
    "\n",
    "'''\n",
    "import re\n",
    "\n",
    "n_partitions = 6  # number of partitions to be used\n",
    "max_edit_distance = 3\n",
    "\n",
    "# helper functions\n",
    "\n",
    "    \n",
    "def copartitioned(RDD1, RDD2):\n",
    "    '''check if two RDDs are copartitioned'''\n",
    "    return RDD1.partitioner == RDD2.partitioner\n",
    "\n",
    "def combine_joined_lists(tup):\n",
    "    '''takes as input a tuple in the form (a, b) where each of a, b may be None (but not both) or a list\n",
    "       and returns a concatenated list of unique elements'''\n",
    "    concat_list = []\n",
    "    if tup[1] is None:\n",
    "        concat_list = tup[0]\n",
    "    elif tup[0] is None:\n",
    "        concat_list = tup[1]\n",
    "    else:\n",
    "        concat_list = tup[0] + tup[1]\n",
    "        \n",
    "    return list(set(concat_list)) \n",
    "\n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance (an integer) between sequences.\n",
    "\n",
    "    This code has not been modified from the original.\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    \n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    for x in xrange(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "    \n",
    "def correct_document(fname, d, lwl=float('inf'), printlist=True):\n",
    "    '''Correct an entire document using word-level correction.\n",
    "    \n",
    "    Note: Uses a serialized version of an individual word checker. \n",
    "    \n",
    "    fname: filename\n",
    "    d: the main dictionary (python dict), which includes deletes\n",
    "             entries, is in the form of: {word: ([suggested corrections], \n",
    "                                                 frequency of word in corpus), ...}\n",
    "    lwl: optional identifier of longest real word in masterdict\n",
    "    printlist: identify unknown words and words with error (default is True)\n",
    "    '''\n",
    "    \n",
    "    # broadcast lookup dictionary to workers\n",
    "    bd = sc.broadcast(d)\n",
    "    \n",
    "    print \"Finding misspelled words in your document...\" \n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "\n",
    "    # convert file into one long sequence of words with the line index for reference\n",
    "    make_all_lower = sc.textFile(fname).map(lambda line: line.lower()).zipWithIndex()\n",
    "    replace_nonalphs = make_all_lower.map(lambda (line, index): (regex.sub(' ', line), index))\n",
    "    flattened = replace_nonalphs.map(lambda (line, index): \n",
    "                                 [(i, index) for i in line.split()]).flatMap(list)\n",
    "    \n",
    "    # create RDD with (each word in document, corresponding line index) \n",
    "    # key value pairs and cache it\n",
    "    all_words = flattened.partitionBy(n_partitions).cache()\n",
    "    \n",
    "    # check all words in parallel --  stores whole list of suggestions for each word\n",
    "    get_corrections = all_words.map(lambda (w, index): \n",
    "                                    (w, (no_RDD_get_suggestions(w, bd.value, lwl, True), index)),\n",
    "                                     preservesPartitioning=True).cache()\n",
    "    \n",
    "    # UNKNOWN words are words where the suggestion list is empty\n",
    "    unknown_words = get_corrections.filter(lambda (w, (sl, index)): len(sl)==0)\n",
    "    if printlist:\n",
    "        print \"    Unknown words (line number, word in text):\"\n",
    "        print unknown_words.map(lambda (w, (sl, index)): (index, str(w))).sortByKey().collect()\n",
    "    \n",
    "    # ERROR words are words where the word does not match the first tuple's word (top match)\n",
    "    error_words = get_corrections.filter(lambda (w, (sl, index)): len(sl)>0 and w!=sl[0][0]) \n",
    "    if printlist:\n",
    "        print \"    Words with suggested corrections (line number, word in text, top match):\"\n",
    "        print error_words.map(lambda (w, (sl, index)): \n",
    "                                 (index, str(w) + \" --> \" +\n",
    "                                         str(sl[0][0]))).sortByKey().collect()\n",
    "    \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % get_corrections.count()\n",
    "    print \"total unknown words: %i\" % unknown_words.count()\n",
    "    print \"total potential errors found: %i\" % error_words.count()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "  <strong>Run the cell below only once to build the dictionary.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "total words processed: 1105285\n",
      "total unique words in corpus: 29157\n",
      "total items in dictionary (corpus words and deletions): 2151998\n",
      "  edit distance for deletions: 3\n",
      "  length of longest word in corpus: 18\n",
      "CPU times: user 11.6 s, sys: 1.19 s, total: 12.8 s\n",
      "Wall time: 47.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d, lwl = parallel_create_dictionary(\"testdata/big.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <strong>Enter word to correct below.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking up suggestions based on input word...\n",
      "number of possible corrections: 604\n",
      "  edit distance for deletions: 3\n",
      "CPU times: user 60.2 ms, sys: 3.25 ms, total: 63.5 ms\n",
      "Wall time: 61.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('there', (2972, 0)),\n",
       " ('these', (1231, 1)),\n",
       " ('where', (977, 1)),\n",
       " ('here', (691, 1)),\n",
       " ('three', (584, 1)),\n",
       " ('thee', (26, 1)),\n",
       " ('chere', (9, 1)),\n",
       " ('theme', (8, 1)),\n",
       " ('the', (80030, 2)),\n",
       " ('her', (5284, 2)),\n",
       " ('were', (4289, 2)),\n",
       " ('they', (3938, 2)),\n",
       " ('their', (2955, 2)),\n",
       " ('them', (2241, 2)),\n",
       " ('then', (1558, 2)),\n",
       " ('other', (1502, 2)),\n",
       " ('those', (1201, 2)),\n",
       " ('others', (410, 2)),\n",
       " ('third', (239, 2)),\n",
       " ('term', (133, 2)),\n",
       " ('threw', (96, 2)),\n",
       " ('mere', (79, 2)),\n",
       " ('theory', (79, 2)),\n",
       " ('share', (69, 2)),\n",
       " ('hero', (55, 2)),\n",
       " ('tree', (42, 2)),\n",
       " ('hare', (36, 2)),\n",
       " (u'thereby', (32, 2)),\n",
       " ('sphere', (31, 2)),\n",
       " ('hers', (30, 2)),\n",
       " (u'thereof', (26, 2)),\n",
       " ('cher', (25, 2)),\n",
       " ('tore', (18, 2)),\n",
       " ('herd', (15, 2)),\n",
       " ('theirs', (14, 2)),\n",
       " ('thiers', (13, 2)),\n",
       " ('shore', (11, 2)),\n",
       " ('thence', (10, 2)),\n",
       " ('tete', (9, 2)),\n",
       " ('ether', (8, 2)),\n",
       " ('adhere', (8, 2)),\n",
       " ('sheer', (8, 2)),\n",
       " ('tver', (7, 2)),\n",
       " (u'therein', (6, 2)),\n",
       " ('herb', (5, 2)),\n",
       " ('cheer', (5, 2)),\n",
       " ('hire', (5, 2)),\n",
       " ('thermo', (5, 2)),\n",
       " ('tier', (5, 2)),\n",
       " ('threes', (4, 2)),\n",
       " (u'theatre', (4, 2)),\n",
       " ('pere', (4, 2)),\n",
       " ('thine', (3, 2)),\n",
       " ('themes', (3, 2)),\n",
       " (u'theresa', (3, 2)),\n",
       " ('herr', (3, 2)),\n",
       " ('thwee', (3, 2)),\n",
       " ('tire', (3, 2)),\n",
       " ('teres', (3, 2)),\n",
       " ('theft', (3, 2)),\n",
       " ('thorn', (2, 2)),\n",
       " ('frere', (2, 2)),\n",
       " ('vere', (2, 2)),\n",
       " (u'thereon', (1, 2)),\n",
       " ('dere', (1, 2)),\n",
       " (u'thereto', (1, 2)),\n",
       " ('ere', (1, 2)),\n",
       " ('thyreo', (1, 2)),\n",
       " ('tyre', (1, 2)),\n",
       " ('theah', (1, 2)),\n",
       " ('tierce', (1, 2)),\n",
       " ('zere', (1, 2)),\n",
       " ('thebes', (1, 2)),\n",
       " ('terse', (1, 2)),\n",
       " ('tiers', (1, 2)),\n",
       " ('that', (12512, 3)),\n",
       " ('he', (12401, 3)),\n",
       " ('this', (4063, 3)),\n",
       " ('she', (3946, 3)),\n",
       " ('are', (3630, 3)),\n",
       " ('have', (3493, 3)),\n",
       " ('when', (2923, 3)),\n",
       " ('more', (1997, 3)),\n",
       " ('pierre', (1964, 3)),\n",
       " ('time', (1529, 3)),\n",
       " ('very', (1340, 3)),\n",
       " ('over', (1282, 3)),\n",
       " ('than', (1206, 3)),\n",
       " ('see', (1101, 3)),\n",
       " ('while', (768, 3)),\n",
       " ('whole', (744, 3)),\n",
       " ('head', (725, 3)),\n",
       " ('every', (650, 3)),\n",
       " ('heard', (636, 3)),\n",
       " ('take', (616, 3)),\n",
       " ('think', (557, 3)),\n",
       " ('father', (533, 3)),\n",
       " ('tell', (492, 3)),\n",
       " ('free', (421, 3)),\n",
       " ('white', (353, 3)),\n",
       " ('horse', (334, 3)),\n",
       " ('nerve', (324, 3)),\n",
       " ('mother', (312, 3)),\n",
       " ('thing', (303, 3)),\n",
       " ('table', (296, 3)),\n",
       " ('home', (295, 3)),\n",
       " ('either', (293, 3)),\n",
       " ('held', (287, 3)),\n",
       " ('fire', (274, 3)),\n",
       " ('ever', (274, 3)),\n",
       " ('heart', (256, 3)),\n",
       " ('short', (236, 3)),\n",
       " ('help', (230, 3)),\n",
       " ('rather', (219, 3)),\n",
       " ('ten', (219, 3)),\n",
       " ('trade', (217, 3)),\n",
       " ('thus', (212, 3)),\n",
       " ('true', (205, 3)),\n",
       " ('re', (189, 3)),\n",
       " ('turn', (188, 3)),\n",
       " ('whose', (188, 3)),\n",
       " ('hear', (183, 3)),\n",
       " ('hard', (180, 3)),\n",
       " ('severe', (173, 3)),\n",
       " ('tears', (172, 3)),\n",
       " ('knee', (171, 3)),\n",
       " ('tone', (166, 3)),\n",
       " ('thin', (166, 3)),\n",
       " ('hope', (149, 3)),\n",
       " ('terms', (148, 3)),\n",
       " ('sure', (123, 3)),\n",
       " ('thirty', (123, 3)),\n",
       " ('eye', (110, 3)),\n",
       " ('tea', (107, 3)),\n",
       " ('care', (106, 3)),\n",
       " ('thank', (105, 3)),\n",
       " ('berg', (98, 3)),\n",
       " ('huge', (89, 3)),\n",
       " ('try', (87, 3)),\n",
       " ('type', (87, 3)),\n",
       " ('per', (86, 3)),\n",
       " ('twice', (84, 3)),\n",
       " ('sharp', (83, 3)),\n",
       " ('heat', (83, 3)),\n",
       " ('rare', (83, 3)),\n",
       " ('chest', (81, 3)),\n",
       " ('sore', (81, 3)),\n",
       " ('agree', (77, 3)),\n",
       " ('thick', (77, 3)),\n",
       " ('teeth', (76, 3)),\n",
       " ('vera', (75, 3)),\n",
       " ('shed', (74, 3)),\n",
       " ('bare', (74, 3)),\n",
       " ('cure', (71, 3)),\n",
       " ('charge', (70, 3)),\n",
       " ('stern', (65, 3)),\n",
       " ('serve', (64, 3)),\n",
       " ('shape', (62, 3)),\n",
       " ('piece', (61, 3)),\n",
       " ('torn', (60, 3)),\n",
       " (u'gathered', (59, 3)),\n",
       " ('uttered', (58, 3)),\n",
       " ('wore', (58, 3)),\n",
       " ('tsar', (56, 3)),\n",
       " ('pure', (54, 3)),\n",
       " ('test', (53, 3)),\n",
       " ('tend', (52, 3)),\n",
       " ('aware', (52, 3)),\n",
       " ('th', (51, 3)),\n",
       " ('henry', (51, 3)),\n",
       " ('thumb', (51, 3)),\n",
       " ('toes', (51, 3)),\n",
       " ('trees', (51, 3)),\n",
       " ('shirt', (50, 3)),\n",
       " ('hart', (49, 3)),\n",
       " ('scene', (49, 3)),\n",
       " ('throw', (48, 3)),\n",
       " ('thy', (47, 3)),\n",
       " ('bore', (46, 3)),\n",
       " ('tube', (45, 3)),\n",
       " ('ahead', (43, 3)),\n",
       " ('twelve', (43, 3)),\n",
       " ('cheek', (43, 3)),\n",
       " ('thirds', (43, 3)),\n",
       " ('dare', (43, 3)),\n",
       " ('thou', (42, 3)),\n",
       " ('harm', (42, 3)),\n",
       " ('shone', (41, 3)),\n",
       " ('shell', (41, 3)),\n",
       " ('utter', (41, 3)),\n",
       " ('heal', (40, 3)),\n",
       " ('tied', (40, 3)),\n",
       " ('hide', (40, 3)),\n",
       " ('tired', (40, 3)),\n",
       " ('title', (39, 3)),\n",
       " ('thigh', (39, 3)),\n",
       " ('wheat', (38, 3)),\n",
       " ('check', (38, 3)),\n",
       " ('toe', (36, 3)),\n",
       " ('trace', (36, 3)),\n",
       " ('shade', (35, 3)),\n",
       " ('hurt', (34, 3)),\n",
       " ('chose', (34, 3)),\n",
       " ('tide', (33, 3)),\n",
       " ('peri', (33, 3)),\n",
       " ('rhode', (33, 3)),\n",
       " ('sire', (32, 3)),\n",
       " ('hence', (32, 3)),\n",
       " ('shame', (32, 3)),\n",
       " ('treat', (31, 3)),\n",
       " ('charm', (31, 3)),\n",
       " ('eve', (30, 3)),\n",
       " ('sheet', (29, 3)),\n",
       " ('fee', (29, 3)),\n",
       " (u'theodore', (28, 3)),\n",
       " ('spare', (27, 3)),\n",
       " ('lee', (27, 3)),\n",
       " ('text', (26, 3)),\n",
       " ('throne', (26, 3)),\n",
       " ('shared', (25, 3)),\n",
       " ('clerk', (25, 3)),\n",
       " ('heroes', (25, 3)),\n",
       " ('scheme', (25, 3)),\n",
       " ('theater', (25, 3)),\n",
       " ('tent', (24, 3)),\n",
       " ('nowhere', (24, 3)),\n",
       " ('tear', (24, 3)),\n",
       " ('ties', (24, 3)),\n",
       " ('sheep', (24, 3)),\n",
       " ('bier', (24, 3)),\n",
       " ('wire', (23, 3)),\n",
       " ('taste', (23, 3)),\n",
       " ('whence', (23, 3)),\n",
       " ('tense', (23, 3)),\n",
       " ('wheel', (21, 3)),\n",
       " ('hive', (21, 3)),\n",
       " (u'theories', (21, 3)),\n",
       " ('tale', (21, 3)),\n",
       " ('phase', (21, 3)),\n",
       " ('heel', (21, 3)),\n",
       " ('tenure', (21, 3)),\n",
       " ('niece', (21, 3)),\n",
       " ('chase', (20, 3)),\n",
       " ('hate', (20, 3)),\n",
       " ('gather', (19, 3)),\n",
       " ('thorax', (19, 3)),\n",
       " ('irene', (18, 3)),\n",
       " ('fathers', (18, 3)),\n",
       " ('horn', (18, 3)),\n",
       " ('hole', (18, 3)),\n",
       " ('geese', (18, 3)),\n",
       " ('heap', (17, 3)),\n",
       " ('score', (17, 3)),\n",
       " ('truce', (17, 3)),\n",
       " ('tens', (16, 3)),\n",
       " ('thread', (16, 3)),\n",
       " ('serf', (16, 3)),\n",
       " ('tune', (15, 3)),\n",
       " ('termed', (15, 3)),\n",
       " ('tie', (15, 3)),\n",
       " ('heed', (15, 3)),\n",
       " ('queer', (15, 3)),\n",
       " ('cheap', (15, 3)),\n",
       " ('pre', (15, 3)),\n",
       " ('tyler', (15, 3)),\n",
       " ('bee', (15, 3)),\n",
       " ('tavern', (14, 3)),\n",
       " ('erie', (14, 3)),\n",
       " ('shake', (14, 3)),\n",
       " ('thirst', (13, 3)),\n",
       " ('pierce', (13, 3)),\n",
       " ('teno', (13, 3)),\n",
       " ('alert', (13, 3)),\n",
       " ('tread', (12, 3)),\n",
       " ('tour', (12, 3)),\n",
       " ('fierce', (12, 3)),\n",
       " ('thief', (12, 3)),\n",
       " ('moore', (12, 3)),\n",
       " ('chart', (12, 3)),\n",
       " ('mothers', (12, 3)),\n",
       " ('rhine', (12, 3)),\n",
       " ('store', (12, 3)),\n",
       " ('hue', (12, 3)),\n",
       " ('opera', (11, 3)),\n",
       " ('hears', (11, 3)),\n",
       " ('flee', (11, 3)),\n",
       " ('fete', (11, 3)),\n",
       " ('hemp', (11, 3)),\n",
       " ('whereas', (11, 3)),\n",
       " ('threat', (11, 3)),\n",
       " ('avert', (11, 3)),\n",
       " ('shoe', (11, 3)),\n",
       " ('cheered', (10, 3)),\n",
       " ('hey', (10, 3)),\n",
       " ('era', (10, 3)),\n",
       " ('acre', (10, 3)),\n",
       " ('heir', (10, 3)),\n",
       " ('thud', (10, 3)),\n",
       " ('sabre', (9, 3)),\n",
       " ('shores', (9, 3)),\n",
       " ('tower', (9, 3)),\n",
       " ('sera', (9, 3)),\n",
       " ('cherry', (9, 3)),\n",
       " ('beer', (9, 3)),\n",
       " ('siege', (8, 3)),\n",
       " ('thieves', (8, 3)),\n",
       " ('whereby', (8, 3)),\n",
       " ('thrice', (8, 3)),\n",
       " ('exert', (8, 3)),\n",
       " ('trend', (8, 3)),\n",
       " ('team', (8, 3)),\n",
       " ('chyle', (8, 3)),\n",
       " ('ushered', (8, 3)),\n",
       " ('user', (8, 3)),\n",
       " ('faire', (8, 3)),\n",
       " ('verge', (8, 3)),\n",
       " ('heirs', (8, 3)),\n",
       " ('fiery', (8, 3)),\n",
       " ('harp', (8, 3)),\n",
       " ('emerge', (8, 3)),\n",
       " ('weren', (8, 3)),\n",
       " ('shelf', (8, 3)),\n",
       " ('lure', (8, 3)),\n",
       " ('verse', (8, 3)),\n",
       " ('fare', (7, 3)),\n",
       " ('tar', (7, 3)),\n",
       " ('scherer', (7, 3)),\n",
       " ('averse', (7, 3)),\n",
       " ('swore', (7, 3)),\n",
       " ('hired', (7, 3)),\n",
       " ('germ', (7, 3)),\n",
       " ('core', (7, 3)),\n",
       " ('sneer', (6, 3)),\n",
       " ('whew', (6, 3)),\n",
       " ('whirl', (6, 3)),\n",
       " ('helen', (6, 3)),\n",
       " ('spheres', (6, 3)),\n",
       " ('sero', (6, 3)),\n",
       " ('merge', (6, 3)),\n",
       " (u'withered', (6, 3)),\n",
       " ('sherren', (6, 3)),\n",
       " ('shove', (6, 3)),\n",
       " ('tinge', (6, 3)),\n",
       " ('chord', (6, 3)),\n",
       " ('jerk', (6, 3)),\n",
       " ('hen', (6, 3)),\n",
       " ('tweed', (6, 3)),\n",
       " ('howe', (6, 3)),\n",
       " ('cheers', (6, 3)),\n",
       " ('sheds', (5, 3)),\n",
       " ('zero', (5, 3)),\n",
       " ('hither', (5, 3)),\n",
       " ('der', (5, 3)),\n",
       " ('taper', (5, 3)),\n",
       " ('chess', (5, 3)),\n",
       " ('mare', (5, 3)),\n",
       " ('deer', (5, 3)),\n",
       " ('peered', (5, 3)),\n",
       " ('ordre', (5, 3)),\n",
       " ('er', (5, 3)),\n",
       " ('adhered', (5, 3)),\n",
       " ('tory', (5, 3)),\n",
       " ('cheese', (5, 3)),\n",
       " ('wharf', (5, 3)),\n",
       " ('erb', (5, 3)),\n",
       " ('stare', (5, 3)),\n",
       " ('glare', (5, 3)),\n",
       " ('bother', (4, 3)),\n",
       " ('hem', (4, 3)),\n",
       " ('attire', (4, 3)),\n",
       " ('hell', (4, 3)),\n",
       " ('heave', (4, 3)),\n",
       " ('zheg', (4, 3)),\n",
       " ('tres', (4, 3)),\n",
       " ('haze', (4, 3)),\n",
       " ('peru', (4, 3)),\n",
       " ('wherein', (4, 3)),\n",
       " ('cheat', (4, 3)),\n",
       " ('etre', (4, 3)),\n",
       " ('hedge', (4, 3)),\n",
       " ('inert', (4, 3)),\n",
       " ('tapers', (4, 3)),\n",
       " ('peer', (4, 3)),\n",
       " ('tribe', (4, 3)),\n",
       " ('rete', (4, 3)),\n",
       " ('overt', (4, 3)),\n",
       " ('steer', (4, 3)),\n",
       " ('thorns', (4, 3)),\n",
       " ('hebrew', (4, 3)),\n",
       " ('users', (4, 3)),\n",
       " ('adore', (4, 3)),\n",
       " ('shine', (4, 3)),\n",
       " ('whereof', (4, 3)),\n",
       " ('herein', (4, 3)),\n",
       " ('spore', (3, 3)),\n",
       " ('hors', (3, 3)),\n",
       " ('tilde', (3, 3)),\n",
       " ('shares', (3, 3)),\n",
       " ('ashore', (3, 3)),\n",
       " ('tiger', (3, 3)),\n",
       " ('oder', (3, 3)),\n",
       " ('turk', (3, 3)),\n",
       " ('barre', (3, 3)),\n",
       " ('herds', (3, 3)),\n",
       " ('notre', (3, 3)),\n",
       " ('glee', (3, 3)),\n",
       " ('wee', (3, 3)),\n",
       " ('ware', (3, 3)),\n",
       " ('tease', (3, 3)),\n",
       " ('thecal', (3, 3)),\n",
       " ('fera', (3, 3)),\n",
       " ('chale', (3, 3)),\n",
       " ('dire', (3, 3)),\n",
       " ('query', (3, 3)),\n",
       " ('cheery', (3, 3)),\n",
       " ('tenn', (3, 3)),\n",
       " ('towers', (3, 3)),\n",
       " ('scare', (3, 3)),\n",
       " ('shave', (3, 3)),\n",
       " ('thierry', (3, 3)),\n",
       " ('pier', (3, 3)),\n",
       " ('whale', (3, 3)),\n",
       " ('tsars', (3, 3)),\n",
       " ('ore', (3, 3)),\n",
       " (u'tethered', (3, 3)),\n",
       " ('turf', (3, 3)),\n",
       " ('seers', (2, 3)),\n",
       " ('neve', (2, 3)),\n",
       " ('whirr', (2, 3)),\n",
       " ('choke', (2, 3)),\n",
       " ('azure', (2, 3)),\n",
       " ('sacre', (2, 3)),\n",
       " ('ver', (2, 3)),\n",
       " ('thresh', (2, 3)),\n",
       " ('garre', (2, 3)),\n",
       " ('hearer', (2, 3)),\n",
       " ('hewn', (2, 3)),\n",
       " ('andre', (2, 3)),\n",
       " ('adheres', (2, 3)),\n",
       " ('whine', (2, 3)),\n",
       " ('beri', (2, 3)),\n",
       " ('ted', (2, 3)),\n",
       " ('err', (2, 3)),\n",
       " ('herpes', (2, 3)),\n",
       " ('shred', (2, 3)),\n",
       " ('tape', (2, 3)),\n",
       " ('tra', (2, 3)),\n",
       " ('piers', (2, 3)),\n",
       " ('thames', (2, 3)),\n",
       " ('coerce', (2, 3)),\n",
       " ('hume', (2, 3)),\n",
       " ('shorn', (2, 3)),\n",
       " ('sherry', (2, 3)),\n",
       " ('jeered', (2, 3)),\n",
       " ('peers', (2, 3)),\n",
       " ('horde', (2, 3)),\n",
       " ('hark', (2, 3)),\n",
       " ('thrive', (2, 3)),\n",
       " ('heh', (2, 3)),\n",
       " ('thesis', (2, 3)),\n",
       " ('phone', (2, 3)),\n",
       " ('veered', (2, 3)),\n",
       " ('athlete', (2, 3)),\n",
       " ('lather', (2, 3)),\n",
       " ('rhyme', (2, 3)),\n",
       " ('verb', (2, 3)),\n",
       " ('votre', (2, 3)),\n",
       " ('hares', (2, 3)),\n",
       " ('gathers', (2, 3)),\n",
       " ('withers', (2, 3)),\n",
       " ('throb', (2, 3)),\n",
       " ('guerre', (2, 3)),\n",
       " ('chef', (2, 3)),\n",
       " ('treble', (2, 3)),\n",
       " ('doers', (2, 3)),\n",
       " ('thaw', (2, 3)),\n",
       " ('outre', (2, 3)),\n",
       " ('spire', (2, 3)),\n",
       " ('tendre', (2, 3)),\n",
       " ('tache', (2, 3)),\n",
       " ('spree', (2, 3)),\n",
       " ('heresy', (2, 3)),\n",
       " ('hur', (2, 3)),\n",
       " ('cheque', (2, 3)),\n",
       " ('hereby', (2, 3)),\n",
       " ('fibre', (2, 3)),\n",
       " ('utters', (1, 3)),\n",
       " ('flare', (1, 3)),\n",
       " ('yore', (1, 3)),\n",
       " ('padre', (1, 3)),\n",
       " (u'theorise', (1, 3)),\n",
       " ('snare', (1, 3)),\n",
       " ('avare', (1, 3)),\n",
       " ('yer', (1, 3)),\n",
       " ('chile', (1, 3)),\n",
       " ('wheal', (1, 3)),\n",
       " ('sheen', (1, 3)),\n",
       " ('shale', (1, 3)),\n",
       " ('chime', (1, 3)),\n",
       " ('tigers', (1, 3)),\n",
       " ('clare', (1, 3)),\n",
       " ('gare', (1, 3)),\n",
       " ('thefts', (1, 3)),\n",
       " ('usher', (1, 3)),\n",
       " ('hewed', (1, 3)),\n",
       " ('luther', (1, 3)),\n",
       " ('tarred', (1, 3)),\n",
       " ('tr', (1, 3)),\n",
       " ('lore', (1, 3)),\n",
       " ('galere', (1, 3)),\n",
       " ('chafe', (1, 3)),\n",
       " ('hesse', (1, 3)),\n",
       " ('ghent', (1, 3)),\n",
       " ('charme', (1, 3)),\n",
       " ('hereof', (1, 3)),\n",
       " ('sexe', (1, 3)),\n",
       " ('mather', (1, 3)),\n",
       " ('meme', (1, 3)),\n",
       " ('snore', (1, 3)),\n",
       " ('tame', (1, 3)),\n",
       " ('vert', (1, 3)),\n",
       " ('autre', (1, 3)),\n",
       " ('ire', (1, 3)),\n",
       " ('berne', (1, 3)),\n",
       " ('wert', (1, 3)),\n",
       " ('mete', (1, 3)),\n",
       " ('herder', (1, 3)),\n",
       " ('twue', (1, 3)),\n",
       " ('teemed', (1, 3)),\n",
       " ('steve', (1, 3)),\n",
       " ('luthers', (1, 3)),\n",
       " ('heah', (1, 3)),\n",
       " ('fore', (1, 3)),\n",
       " ('tiens', (1, 3)),\n",
       " ('chew', (1, 3)),\n",
       " ('lyre', (1, 3)),\n",
       " ('te', (1, 3)),\n",
       " ('litre', (1, 3)),\n",
       " ('thoreau', (1, 3)),\n",
       " ('ober', (1, 3)),\n",
       " ('crewe', (1, 3)),\n",
       " ('boer', (1, 3)),\n",
       " ('helm', (1, 3)),\n",
       " ('nee', (1, 3)),\n",
       " ('tires', (1, 3)),\n",
       " ('hew', (1, 3)),\n",
       " ('trove', (1, 3)),\n",
       " ('chewed', (1, 3)),\n",
       " ('thermal', (1, 3)),\n",
       " ('boire', (1, 3)),\n",
       " ('rire', (1, 3)),\n",
       " ('metre', (1, 3)),\n",
       " ('toured', (1, 3)),\n",
       " ('swede', (1, 3)),\n",
       " ('thins', (1, 3)),\n",
       " ('obese', (1, 3)),\n",
       " ('mele', (1, 3)),\n",
       " ('afore', (1, 3)),\n",
       " ('nether', (1, 3)),\n",
       " ('quire', (1, 3)),\n",
       " ('thwow', (1, 3)),\n",
       " ('amene', (1, 3)),\n",
       " ('euer', (1, 3)),\n",
       " ('ethel', (1, 3)),\n",
       " ('vierge', (1, 3)),\n",
       " ('tarry', (1, 3)),\n",
       " ('tiara', (1, 3)),\n",
       " ('tate', (1, 3)),\n",
       " ('adele', (1, 3)),\n",
       " ('queue', (1, 3)),\n",
       " ('tile', (1, 3)),\n",
       " ('shew', (1, 3)),\n",
       " ('steered', (1, 3)),\n",
       " (u'etherege', (1, 3)),\n",
       " ('tyne', (1, 3)),\n",
       " ('truer', (1, 3)),\n",
       " ('madere', (1, 3)),\n",
       " ('thwart', (1, 3)),\n",
       " ('tante', (1, 3)),\n",
       " ('tonne', (1, 3)),\n",
       " ('chorea', (1, 3)),\n",
       " ('thaler', (1, 3)),\n",
       " ('eerie', (1, 3)),\n",
       " ('athens', (1, 3)),\n",
       " ('sheaf', (1, 3)),\n",
       " ('diese', (1, 3)),\n",
       " ('esther', (1, 3)),\n",
       " ('trite', (1, 3)),\n",
       " ('hyde', (1, 3)),\n",
       " ('utero', (1, 3)),\n",
       " ('chiene', (1, 3)),\n",
       " ('towered', (1, 3)),\n",
       " ('phoebe', (1, 3)),\n",
       " ('chary', (1, 3)),\n",
       " ('gene', (1, 3)),\n",
       " ('henri', (1, 3)),\n",
       " ('gee', (1, 3)),\n",
       " ('hale', (1, 3)),\n",
       " ('inheres', (1, 3)),\n",
       " ('sterne', (1, 3)),\n",
       " ('therapy', (1, 3)),\n",
       " ('hise', (1, 3)),\n",
       " ('freer', (1, 3)),\n",
       " ('trent', (1, 3))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "no_RDD_get_suggestions(\"there\", d, lwl)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "looking up suggestions based on input word...\n",
    "number of possible corrections: 604\n",
    "  edit distance for deletions: 3\n",
    "CPU times: user 56.3 ms, sys: 4.17 ms, total: 60.5 ms\n",
    "Wall time: 58.2 ms\n",
    "Out[3]:\n",
    "[('there', (2972, 0)),\n",
    " ('these', (1231, 1)),\n",
    " ('where', (977, 1)),\n",
    " ('here', (691, 1)),\n",
    " ('three', (584, 1)),\n",
    " ('thee', (26, 1)),\n",
    " ('chere', (9, 1)),\n",
    " ('theme', (8, 1)),\n",
    " ('the', (80030, 2)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking up suggestions based on input word...\n",
      "number of possible corrections: 0\n",
      "  edit distance for deletions: 3\n",
      "CPU times: user 272 µs, sys: 97 µs, total: 369 µs\n",
      "Wall time: 290 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "no_RDD_get_suggestions(\"zzffttt\", d, lwl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <strong>Enter file name of document to correct below.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "    Unknown words (line number, word in text):\n",
      "[(11, 'oonipiittee'), (42, 'senbrnrgs'), (82, 'ghmhvestigat')]\n",
      "    Words with suggested corrections (line number, word in text, top match):\n",
      "[(3, 'taiths --> faith'), (13, 'gjpt --> get'), (13, 'tj --> to'), (13, 'mnnff --> snuff'), (15, 'bh --> by'), (15, 'uth --> th'), (15, 'unuer --> under'), (15, 'snc --> sac'), (20, 'mthiitt --> thirty'), (21, 'cas --> was'), (22, 'pythian --> scythian'), (26, 'brainin --> brain'), (27, 'jfl --> of'), (28, 'eug --> dug'), (28, 'stice --> stick'), (28, 'blaci --> black'), (28, 'ji --> i'), (28, 'debbs --> debts'), (29, 'nericans --> americans'), (30, 'ergs --> eggs'), (30, 'ainin --> again'), (31, 'trumped --> trumpet'), (32, 'erican --> american'), (33, 'thg --> the'), (33, 'nenance --> penance'), (33, 'unorthodox --> orthodox'), (34, 'rgs --> rags'), (34, 'sln --> son'), (38, 'eu --> e'), (38, 'williaij --> william'), (40, 'fcsf --> ff'), (40, 'ber --> be'), (42, 'thpt --> that'), (42, 'unorthodoxy --> orthodox'), (44, 'fascism --> fascia'), (62, 'loo --> look'), (65, 'ththn --> then'), (65, 'thl --> the'), (65, 'yktcn --> skin'), (65, 'scbell --> bell'), (65, 'ife --> if'), (66, 'thi --> the'), (68, 'saij --> said'), (69, 'cornr --> corner'), (69, 'defendants --> defendant'), (69, 'nists --> lists'), (72, 'ro --> to'), (74, 'ath --> at'), (75, 'rg --> re'), (75, 'acrific --> pacific'), (75, 'tti --> tit'), (77, 'korea --> more'), (78, 'doatli --> death'), (78, 'ro --> to'), (81, 'ry --> by'), (81, 'ith --> it'), (81, 'kl --> ll'), (81, 'ech --> each'), (82, 'rg --> re'), (82, 'rb --> re'), (82, 'nb --> no'), (83, 'rosenbt --> rodent'), (83, 'rgs --> rags'), (84, 'coriritted --> committed'), (86, 'fighti --> fight'), (88, 'bths --> baths'), (88, 'tchf --> the'), (91, 'ro --> to'), (91, 'ijb --> in'), (92, 'telegrnm --> telegram'), (92, 'rson --> son'), (92, 'jillia --> william'), (92, 'patt --> part'), (93, 'ecretdry --> secretary'), (95, 'purview --> purves'), (95, 'rder --> order'), (99, 'gor --> for'), (99, 'rg --> re'), (99, 'enb --> end'), (99, 'dthethg --> teeth'), (99, 'ro --> to'), (99, 'ared --> are'), (100, 'dri --> dry'), (100, 'yfu --> you'), (100, 'vthnz --> the'), (100, 'sacc --> sac'), (101, 'rosi --> rose'), (101, 'rg --> re'), (101, 'ile --> ill'), (102, 'jhy --> why'), (102, 'fnir --> fair'), (102, 'azi --> ai'), (103, 'fascist --> fascia'), (104, 'nb --> no')]\n",
      "-----\n",
      "total words checked: 700\n",
      "total unknown words: 3\n",
      "total potential errors found: 94\n",
      "CPU times: user 9.82 s, sys: 607 ms, total: 10.4 s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document(\"testdata/OCRsample.txt\", d, lwl)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finding misspelled words in your document...\n",
    "    Unknown words (line number, word in text):\n",
    "[(11, 'oonipiittee'), (42, 'senbrnrgs'), (82, 'ghmhvestigat')]\n",
    "    Words with suggested corrections (line number, word in text, top match):\n",
    "[(3, 'taiths --> faith'), (13, 'gjpt --> get'), (13, 'tj --> to'), (13, 'mnnff --> snuff'), (15, 'bh --> by'), (15, 'uth --> th'), (15, 'unuer --> under'), (15, 'snc --> sac'), (20, 'mthiitt --> thirty'), (21, 'cas --> was'), (22, 'pythian --> scythian'), (26, 'brainin --> brain'), (27, 'jfl --> of'), (28, 'eug --> dug'), (28, 'stice --> stick'), (28, 'blaci --> black'), (28, 'ji --> i'), (28, 'debbs --> debts'), (29, 'nericans --> americans'), (30, 'ergs --> eggs'), (30, 'ainin --> again'), (31, 'trumped --> trumpet'), (32, 'erican --> american'), (33, 'thg --> the'), (33, 'nenance --> penance'), (33, 'unorthodox --> orthodox'), (34, 'rgs --> rags'), (34, 'sln --> son'), (38, 'eu --> e'), (38, 'williaij --> william'), (40, 'fcsf --> ff'), (40, 'ber --> be'), (42, 'thpt --> that'), (42, 'unorthodoxy --> orthodox'), (44, 'fascism --> fascia'), (62, 'loo --> look'), (65, 'ththn --> then'), (65, 'thl --> the'), (65, 'yktcn --> skin'), (65, 'scbell --> bell'), (65, 'ife --> if'), (66, 'thi --> the'), (68, 'saij --> said'), (69, 'cornr --> corner'), (69, 'defendants --> defendant'), (69, 'nists --> lists'), (72, 'ro --> to'), (74, 'ath --> at'), (75, 'rg --> re'), (75, 'acrific --> pacific'), (75, 'tti --> tit'), (77, 'korea --> more'), (78, 'doatli --> death'), (78, 'ro --> to'), (81, 'ry --> by'), (81, 'ith --> it'), (81, 'kl --> ll'), (81, 'ech --> each'), (82, 'rg --> re'), (82, 'rb --> re'), (82, 'nb --> no'), (83, 'rosenbt --> rodent'), (83, 'rgs --> rags'), (84, 'coriritted --> committed'), (86, 'fighti --> fight'), (88, 'bths --> baths'), (88, 'tchf --> the'), (91, 'ro --> to'), (91, 'ijb --> in'), (92, 'telegrnm --> telegram'), (92, 'rson --> son'), (92, 'jillia --> william'), (92, 'patt --> part'), (93, 'ecretdry --> secretary'), (95, 'purview --> purves'), (95, 'rder --> order'), (99, 'gor --> for'), (99, 'rg --> re'), (99, 'enb --> end'), (99, 'dthethg --> teeth'), (99, 'ro --> to'), (99, 'ared --> are'), (100, 'dri --> dry'), (100, 'yfu --> you'), (100, 'vthnz --> the'), (100, 'sacc --> sac'), (101, 'rosi --> rose'), (101, 'rg --> re'), (101, 'ile --> ill'), (102, 'jhy --> why'), (102, 'fnir --> fair'), (102, 'azi --> ai'), (103, 'fascist --> fascia'), (104, 'nb --> no')]\n",
    "-----\n",
    "total words checked: 700\n",
    "total unknown words: 3\n",
    "total potential errors found: 94\n",
    "CPU times: user 9.9 s, sys: 619 ms, total: 10.5 s\n",
    "Wall time: 1min 3s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>START RUNNING CODE HERE</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_partitions = 6  # number of partitions to be used\n",
    "MAX_EDIT_DISTANCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_n_deletes_list(w, n):\n",
    "    '''given a word, derive list of strings with up to n characters deleted'''\n",
    "    # since this list is generally of the same magnitude as the number of \n",
    "    # characters in a word, it may not make sense to parallelize this\n",
    "    # so we use python to create the list\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(n):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "# load file & initial processing\n",
    "#\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"testdata/big.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-z ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_all_lower = sc.textFile(fname).map(lambda line: line.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print make_all_lower\n",
    "# print make_all_lower.getNumPartitions()\n",
    "# print make_all_lower.count()\n",
    "# print make_all_lower.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_sentence = make_all_lower.flatMap(lambda line: line.split('.')).map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print split_sentence\n",
    "# print split_sentence.getNumPartitions()\n",
    "# print split_sentence.count()\n",
    "# print split_sentence.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "# generate start probabilities\n",
    "#\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_words = split_sentence.map(lambda sentence: sentence[0] if len(sentence)>0 else None) \\\n",
    "    .filter(lambda word: word!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print start_words\n",
    "# print start_words.getNumPartitions()\n",
    "# print start_words.count()\n",
    "# print start_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accum_total_start_words = sc.accumulator(0)\n",
    "count_start_words_once = start_words.map(lambda word: (word, 1))\n",
    "count_total_start_words = count_start_words_once.foreach(lambda x: accum_total_start_words.add(1))\n",
    "total_start_words = float(accum_total_start_words.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total start words: 137073.0\n"
     ]
    }
   ],
   "source": [
    "# print count_start_words_once\n",
    "# print count_start_words_once.getNumPartitions()\n",
    "# print count_start_words_once.count()\n",
    "# print count_start_words_once.take(5)\n",
    "\n",
    "print 'Total start words:', total_start_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_start_words = count_start_words_once.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print unique_start_words\n",
    "# print unique_start_words.getNumPartitions()\n",
    "# print unique_start_words.count()\n",
    "# print unique_start_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_prob_calc = unique_start_words.map(lambda (k,v): (k, math.log(v/total_start_words)))\n",
    "default_start_prob = math.log(1/total_start_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default start probability: -11.8282689096\n"
     ]
    }
   ],
   "source": [
    "# print start_prob_calc\n",
    "# print start_prob_calc.getNumPartitions()\n",
    "# print start_prob_calc.count()\n",
    "# print start_prob_calc.take(5)\n",
    "\n",
    "print 'Default start probability:', default_start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_prob = start_prob_calc.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "# generate transition probabilities\n",
    "#\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_transitions(sentence):\n",
    "    result = []\n",
    "    if len(sentence)<2:\n",
    "        return None\n",
    "    else:\n",
    "        for i in range(len(sentence)-1):\n",
    "            result.append(((sentence[i], sentence[i+1]), 1))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accum_total_other_words = sc.accumulator(0)\n",
    "other_words = split_sentence.map(lambda sentence: get_transitions(sentence)).filter(lambda x: x!=None). \\\n",
    "                flatMap(lambda x: x)\n",
    "count_total_other_words = other_words.foreach(lambda x: accum_total_other_words.add(1))\n",
    "total_other_words = float(accum_total_other_words.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total other words 968212.0\n"
     ]
    }
   ],
   "source": [
    "# print other_words\n",
    "# print other_words.getNumPartitions()\n",
    "# print other_words.count()\n",
    "# print other_words.take(5)\n",
    "\n",
    "print 'Total other words', total_other_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_other_words = other_words.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print unique_other_words\n",
    "# print unique_other_words.getNumPartitions()\n",
    "# print unique_other_words.count()\n",
    "# print unique_other_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "other_words_collapsed = unique_other_words.map(lambda x: (x[0][0], (x[0][1], x[1]))).groupByKey().mapValues(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print other_words_collapsed\n",
    "# print other_words_collapsed.getNumPartitions()\n",
    "# print other_words_collapsed.count()\n",
    "# print other_words_collapsed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_transition_prob(x):\n",
    "    vals = x[1]\n",
    "    total = float(sum(vals.values()))\n",
    "    probs = {k: math.log(v/total) for k, v in vals.items()}\n",
    "    return (x[0], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_prob_calc = other_words_collapsed.map(lambda x: map_transition_prob(x))\n",
    "default_transition_prob = math.log(1/total_other_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default transition probability: -13.7832063505\n"
     ]
    }
   ],
   "source": [
    "# print transition_prob_calc\n",
    "# print transition_prob_calc.getNumPartitions()\n",
    "# print transition_prob_calc.count()\n",
    "# print transition_prob_calc.take(5)\n",
    "\n",
    "print 'Default transition probability:', default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_prob = transition_prob_calc.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "# generate dictionary\n",
    "#\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = make_all_lower.map(lambda line: regex.sub(' ', line)).flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print all_words\n",
    "# print all_words.getNumPartitions()\n",
    "# print all_words.count()\n",
    "# print all_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_once = all_words.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print count_once\n",
    "# print count_once.getNumPartitions()\n",
    "# print count_once.count()\n",
    "# print count_once.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_words_with_count = count_once.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print unique_words_with_count\n",
    "# print unique_words_with_count.getNumPartitions()\n",
    "# print unique_words_with_count.count()\n",
    "# print unique_words_with_count.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert MAX_EDIT_DISTANCE>0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_deletes = unique_words_with_count.map(lambda (parent, count): \n",
    "                                                   (parent, get_n_deletes_list(parent, MAX_EDIT_DISTANCE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print generate_deletes\n",
    "# print generate_deletes.getNumPartitions()\n",
    "# print generate_deletes.count()\n",
    "# print generate_deletes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expand_deletes = generate_deletes.flatMapValues(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print expand_deletes\n",
    "# print expand_deletes.getNumPartitions()\n",
    "# print expand_deletes.count()\n",
    "# print expand_deletes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swap = expand_deletes.map(lambda (orig, delete): (delete, ([orig], 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print swap\n",
    "# print swap.getNumPartitions()\n",
    "# print swap.count()\n",
    "# print swap.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = unique_words_with_count.mapValues(lambda count: ([], count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print corpus\n",
    "# print corpus.getNumPartitions()\n",
    "# print corpus.count()\n",
    "# print corpus.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combine = swap.union(corpus)  # combine deletes with main dictionary, eliminate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print combine\n",
    "# print combine.getNumPartitions()\n",
    "# print combine.count()\n",
    "# print combine.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = combine.reduceByKeyLocally(lambda a, b: (a[0]+b[0], a[1]+b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "longest_word_length = unique_words_with_count.map(lambda (k, v): len(k)).reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# Sentence-level parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(word typed|word intended)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suggestions(string, dictionary, \n",
    "                    longest_word_length, min_count):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        return []\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>=min_count):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>=min_count): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions:\n",
    "    # (correction, (frequency in corpus, edit distance)):\n",
    "    as_list = suggest_dict.items()\n",
    "    return sorted(as_list, key = \\\n",
    "                  lambda (term, (freq, dist)): (dist, -freq))\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file')\n",
    "    [('file', (5, 0)),\n",
    "     ('five', (67, 1)),\n",
    "     ('fire', (54, 1)),\n",
    "     ('fine', (17, 1))...]  \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(words, dictionary, longest_word_length,\n",
    "            start_prob, default_start_prob, \n",
    "            transition_prob, default_transition_prob,\n",
    "            min_count=1,num_word_suggestions=5000):\n",
    "    \n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    path_context = []\n",
    "    \n",
    "    # character level correction - used to determine state space\n",
    "    corrections = get_suggestions(\n",
    "        words[0], dictionary, longest_word_length, min_count)\n",
    "\n",
    "    # to ensure Viterbi can keep running\n",
    "    # -- use the word itself if no corrections are found\n",
    "    if len(corrections) == 0:\n",
    "        corrections = [(words[0], (1, 0))]\n",
    "    else:    \n",
    "        if len(corrections) > num_word_suggestions:\n",
    "            corrections = corrections[0:num_word_suggestions]\n",
    "        \n",
    "    # Initialize base cases (t == 0)\n",
    "    for sug_word in corrections:\n",
    "        \n",
    "        # compute the value for all possible starting states\n",
    "        V[0][sug_word[0]] = math.exp(\n",
    "            get_start_prob(sug_word[0], start_prob, \n",
    "                           default_start_prob)\n",
    "            + get_emission_prob(sug_word[1][1]))\n",
    "        \n",
    "        # remember all the different paths (only one state so far)\n",
    "        path[sug_word[0]] = [sug_word[0]]\n",
    " \n",
    "    # normalize for numerical stability\n",
    "    path_temp_sum = sum(V[0].values())\n",
    "    V[0].update({k: math.log(v/path_temp_sum) \n",
    "                 for k, v in V[0].items()})\n",
    "    \n",
    "    # keep track of previous state space\n",
    "    prev_corrections = [i[0] for i in corrections]\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        path_context = [max(V[0], key=lambda i: V[0][i])]\n",
    "        return path_context\n",
    "\n",
    "    # run Viterbi for t > 0\n",
    "    for t in range(1, len(words)):\n",
    "\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        \n",
    "        # character level correction\n",
    "        corrections = get_suggestions(\n",
    "            words[t], dictionary, longest_word_length, min_count)\n",
    "        \n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        if len(corrections) == 0:\n",
    "            corrections = [(words[t], (1, 0))]\n",
    "        else:\n",
    "            if len(corrections) > num_word_suggestions:\n",
    "                corrections = corrections[0:num_word_suggestions]\n",
    " \n",
    "        for sug_word in corrections:\n",
    "        \n",
    "            sug_word_emission_prob = get_emission_prob(sug_word[1][1])\n",
    "            \n",
    "            # compute the values coming from all possible previous\n",
    "            # states, only keep the maximum\n",
    "            (prob, word) = max(\n",
    "                (get_belief(prev_word, V[t-1]) \n",
    "                + get_transition_prob(sug_word[0], prev_word, \n",
    "                    transition_prob, default_transition_prob)\n",
    "                + sug_word_emission_prob, prev_word) \n",
    "                               for prev_word in prev_corrections)\n",
    "\n",
    "            # save the maximum value for each state\n",
    "            V[t][sug_word[0]] = math.exp(prob)\n",
    "            \n",
    "            # remember the path we came from to get this maximum value\n",
    "            new_path[sug_word[0]] = path[word] + [sug_word[0]]\n",
    "            \n",
    "        # normalize for numerical stability\n",
    "        path_temp_sum = sum(V[t].values())\n",
    "        V[t].update({k: math.log(v/path_temp_sum) \n",
    "                     for k, v in V[t].items()})\n",
    "        \n",
    "        # keep track of previous state space\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "        # don't need to remember the old paths\n",
    "        path = new_path\n",
    "     \n",
    "    (prob, word) = max((V[t][sug_word[0]], sug_word[0]) \n",
    "                       for sug_word in corrections)\n",
    "    path_context = path[word]\n",
    "    \n",
    "    return path_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "# load file & initial processing\n",
    "#\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"testdata/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# broadcast Python dictionaries to workers\n",
    "bc_dictionary = sc.broadcast(dictionary)\n",
    "bc_start_prob = sc.broadcast(start_prob)\n",
    "bc_transition_prob = sc.broadcast(transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_all_lower = sc.textFile(fname).map(lambda line: line.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print make_all_lower\n",
    "# print make_all_lower.getNumPartitions()\n",
    "# print make_all_lower.count()\n",
    "# print make_all_lower.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_sentence = make_all_lower.flatMap(lambda line: line.split('.')).map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'], [u'here', u'is', u'a', u'test'], [u'this', u'is', u'ax', u'tesst'], [u'this', u'is', u'za', u'test'], [u'thee', u'is', u'a', u'test'], [u'her', u'tee', u'set']]\n"
     ]
    }
   ],
   "source": [
    "# print split_sentence\n",
    "# print split_sentence.getNumPartitions()\n",
    "# print split_sentence.count()\n",
    "# print split_sentence.take(5)\n",
    "print split_sentence.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words checked:  27\n"
     ]
    }
   ],
   "source": [
    "# use accumulator to count the number of words checked\n",
    "accum_total_words = sc.accumulator(0)\n",
    "split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "print 'Words checked: ', accum_total_words.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [u'this', u'is', u'a', u'test']), (1, [u'this', u'is', u'a', u'test']), (2, [u'here', u'is', u'a', u'test']), (3, [u'this', u'is', u'ax', u'tesst']), (4, [u'this', u'is', u'za', u'test']), (5, [u'thee', u'is', u'a', u'test']), (6, [u'her', u'tee', u'set'])]\n"
     ]
    }
   ],
   "source": [
    "# print sentence_id\n",
    "# print sentence_id.getNumPartitions()\n",
    "# print sentence_id.count()\n",
    "# print sentence_id.take(5)\n",
    "print sentence_id.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_correction = sentence_id.map(lambda (k, v): (k, (v, viterbi(\n",
    "                v, bc_dictionary.value, longest_word_length, bc_start_prob.value, \n",
    "                default_start_prob, bc_transition_prob.value, default_transition_prob))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'])), (1, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'])), (2, ([u'here', u'is', u'a', u'test'], [u'here', u'is', u'a', u'test'])), (3, ([u'this', u'is', u'ax', u'tesst'], [u'this', u'is', u'a', u'test'])), (4, ([u'this', u'is', u'za', u'test'], [u'this', u'is', u'a', u'test'])), (5, ([u'thee', u'is', u'a', u'test'], [u'there', u'is', u'a', u'test'])), (6, ([u'her', u'tee', u'set'], [u'her', u'to', u'set']))]\n"
     ]
    }
   ],
   "source": [
    "# print sentence_correction\n",
    "# print sentence_correction.getNumPartitions()\n",
    "# print sentence_correction.count()\n",
    "# print sentence_correction.take(5)\n",
    "print sentence_correction.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_mismatches(sentences):\n",
    "    orig_sentence, sug_sentence = sentences\n",
    "    mismatches = [(orig_sentence[i], sug_sentence[i]) for i in range(len(orig_sentence)) \n",
    "            if orig_sentence[i]!=sug_sentence[i]]\n",
    "    if len(mismatches)==0:\n",
    "        return None\n",
    "    else:\n",
    "        return mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_mismatch = sentence_correction.map(lambda (k, v): (k, get_sentence_mismatches(v))) \\\n",
    "                .filter(lambda (k,v): v!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, [(u'ax', u'a'), (u'tesst', u'test')]),\n",
       " (4, [(u'za', u'a')]),\n",
       " (5, [(u'thee', u'there')]),\n",
       " (6, [(u'tee', u'to')])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print sentence_mismatch\n",
    "# print sentence_mismatch.getNumPartitions()\n",
    "# print sentence_mismatch.count()\n",
    "# print sentence_mismatch.take(5)\n",
    "sentence_mismatch.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_mismatches(mismatches):\n",
    "    sent_id, word_list = mismatches\n",
    "    result = []\n",
    "    for word in word_list:\n",
    "        result.append([sent_id, word[0], word[1]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_mismatch = sentence_mismatch.flatMap(lambda x: split_mismatches(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[[3, u'ax', u'a'], [3, u'tesst', u'test'], [4, u'za', u'a'], [5, u'thee', u'there'], [6, u'tee', u'to']]\n"
     ]
    }
   ],
   "source": [
    "# print word_mismatch\n",
    "# print word_mismatch.getNumPartitions()\n",
    "print word_mismatch.count()\n",
    "# print word_mismatch.take(5)\n",
    "print word_mismatch.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential mismatches:  5\n"
     ]
    }
   ],
   "source": [
    "# use accumulator to count the number of mismatches\n",
    "accum_total_mismatches = sc.accumulator(0)\n",
    "count_mismatches = word_mismatch.foreach(lambda x: accum_total_mismatches.add(1))\n",
    "print 'Potential mismatches: ', accum_total_mismatches.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printlist=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Words with suggested corrections (line number, word in text, top match):\n",
      "[(3, 'ax --> a'), (3, 'tesst --> test'), (4, 'za --> a'), (5, 'thee --> there'), (6, 'tee --> to')]\n"
     ]
    }
   ],
   "source": [
    "# ERROR words are words where the word does not match the first tuple's word (top match)\n",
    "if printlist:\n",
    "    print '    Words with suggested corrections (line number, word in text, top match):'\n",
    "    print word_mismatch.map(lambda x: (x[0], str(x[1]) + \" --> \" + str(x[2]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
