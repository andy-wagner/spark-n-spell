{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>START RUNNING CODE HERE</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_partitions = 6  # number of partitions to be used\n",
    "MAX_EDIT_DISTANCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_n_deletes_list(w, n):\n",
    "    '''given a word, derive list of strings with up to n characters deleted'''\n",
    "    # since this list is generally of the same magnitude as the number of \n",
    "    # characters in a word, it may not make sense to parallelize this\n",
    "    # so we use python to create the list\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(n):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "# load file & initial processing\n",
    "#\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"testdata/big.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-z ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_all_lower = sc.textFile(fname).map(lambda line: line.lower()).filter(lambda x: x!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[2] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "103600\n",
      "[u'the project gutenberg ebook of the adventures of sherlock holmes', u'by sir arthur conan doyle', u'(#15 in our series by sir arthur conan doyle)', u'copyright laws are changing all over the world. be sure to check the', u'copyright laws for your country before downloading or redistributing']\n"
     ]
    }
   ],
   "source": [
    "print make_all_lower\n",
    "print make_all_lower.getNumPartitions()\n",
    "print make_all_lower.count()\n",
    "print make_all_lower.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[5] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "162272\n",
      "[[u'the', u'project', u'gutenberg', u'ebook', u'of', u'the', u'adventures', u'of', u'sherlock', u'holmes'], [u'by', u'sir', u'arthur', u'conan', u'doyle'], [u'in', u'our', u'series', u'by', u'sir', u'arthur', u'conan', u'doyle'], [u'copyright', u'laws', u'are', u'changing', u'all', u'over', u'the', u'world'], [u'be', u'sure', u'to', u'check', u'the']]\n"
     ]
    }
   ],
   "source": [
    "print split_sentence\n",
    "print split_sentence.getNumPartitions()\n",
    "print split_sentence.count()\n",
    "print split_sentence.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "# generate start probabilities\n",
    "#\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_words = split_sentence.map(lambda sentence: sentence[0] if len(sentence)>0 else None) \\\n",
    "    .filter(lambda word: word!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[8] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "137073\n",
      "[u'the', u'by', u'in', u'copyright', u'be']\n"
     ]
    }
   ],
   "source": [
    "print start_words\n",
    "print start_words.getNumPartitions()\n",
    "print start_words.count()\n",
    "print start_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accum_total_start_words = sc.accumulator(0)\n",
    "count_start_words_once = start_words.map(lambda word: (word, 1))\n",
    "count_total_start_words = count_start_words_once.foreach(lambda x: accum_total_start_words.add(1))\n",
    "total_start_words = float(accum_total_start_words.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[12] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "137073\n",
      "[(u'the', 1), (u'by', 1), (u'in', 1), (u'copyright', 1), (u'be', 1)]\n",
      "Total start words: 137073.0\n"
     ]
    }
   ],
   "source": [
    "print count_start_words_once\n",
    "print count_start_words_once.getNumPartitions()\n",
    "print count_start_words_once.count()\n",
    "print count_start_words_once.take(5)\n",
    "\n",
    "print 'Total start words:', total_start_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_start_words = count_start_words_once.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[19] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "15297\n",
      "[(u'aided', 3), (u'suicidal', 1), (u'desirable', 4), (u'all', 562), (u'yellow', 4)]\n"
     ]
    }
   ],
   "source": [
    "print unique_start_words\n",
    "print unique_start_words.getNumPartitions()\n",
    "print unique_start_words.count()\n",
    "print unique_start_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_prob_calc = unique_start_words.map(lambda (k,v): (k, math.log(v/total_start_words)))\n",
    "default_start_prob = math.log(1/total_start_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[22] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "15297\n",
      "[(u'aided', -10.729656620945079), (u'suicidal', -11.82826890961319), (u'desirable', -10.441974548493299), (u'all', -5.496767059719498), (u'yellow', -10.441974548493299)]\n",
      "Default start probability: -11.8282689096\n"
     ]
    }
   ],
   "source": [
    "print start_prob_calc\n",
    "print start_prob_calc.getNumPartitions()\n",
    "print start_prob_calc.count()\n",
    "print start_prob_calc.take(5)\n",
    "\n",
    "print 'Default start probability:', default_start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_prob = start_prob_calc.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "# generate transition probabilities\n",
    "#\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_transitions(sentence):\n",
    "#     result = []\n",
    "    if len(sentence)<2:\n",
    "        return None\n",
    "    else:\n",
    "        return [((sentence[i], sentence[i+1]), 1) for i in range(len(sentence)-1)]\n",
    "#         for i in range(len(sentence)-1):\n",
    "#             result.append(((sentence[i], sentence[i+1]), 1))\n",
    "#         return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accum_total_other_words = sc.accumulator(0)\n",
    "other_words = split_sentence.map(lambda sentence: get_transitions(sentence)).filter(lambda x: x!=None). \\\n",
    "                flatMap(lambda x: x)\n",
    "count_total_other_words = other_words.foreach(lambda x: accum_total_other_words.add(1))\n",
    "total_other_words = float(accum_total_other_words.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[26] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "968212\n",
      "[((u'the', u'project'), 1), ((u'project', u'gutenberg'), 1), ((u'gutenberg', u'ebook'), 1), ((u'ebook', u'of'), 1), ((u'of', u'the'), 1)]\n",
      "Total other words 968212.0\n"
     ]
    }
   ],
   "source": [
    "print other_words\n",
    "print other_words.getNumPartitions()\n",
    "print other_words.count()\n",
    "print other_words.take(5)\n",
    "\n",
    "print 'Total other words', total_other_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_other_words = other_words.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[33] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "319665\n",
      "[((u'so', u'was'), 5), ((u'mischievous', u'pang'), 1), ((u'gave', u'confused'), 1), ((u'getting', u'stouter'), 1), ((u'long', u'frock'), 1)]\n"
     ]
    }
   ],
   "source": [
    "print unique_other_words\n",
    "print unique_other_words.getNumPartitions()\n",
    "print unique_other_words.count()\n",
    "print unique_other_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "other_words_collapsed = unique_other_words.map(lambda x: (x[0][0], (x[0][1], x[1]))).groupByKey().mapValues(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[40] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "27224\n",
      "[(u'bennigsens', {u'and': 1}), (u'aided', {u'the': 3, u'by': 12, u'augustus': 1}), (u'suicidal', {u'and': 2, u'cut': 1, u'or': 1, u'commented': 1}), (u'linsey', {u'woolseys': 1}), (u'unheeded', {u'to': 1, u'upon': 1})]\n"
     ]
    }
   ],
   "source": [
    "print other_words_collapsed\n",
    "print other_words_collapsed.getNumPartitions()\n",
    "print other_words_collapsed.count()\n",
    "print other_words_collapsed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_transition_prob(x):\n",
    "    vals = x[1]\n",
    "    total = float(sum(vals.values()))\n",
    "    probs = {k: math.log(v/total) for k, v in vals.items()}\n",
    "    return (x[0], probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_prob_calc = other_words_collapsed.map(lambda x: map_transition_prob(x))\n",
    "default_transition_prob = math.log(1/total_other_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[43] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "27224\n",
      "[(u'bennigsens', {u'and': 0.0}), (u'aided', {u'the': -1.6739764335716716, u'by': -0.2876820724517809, u'augustus': -2.772588722239781}), (u'suicidal', {u'and': -0.916290731874155, u'cut': -1.6094379124341003, u'or': -1.6094379124341003, u'commented': -1.6094379124341003}), (u'linsey', {u'woolseys': 0.0}), (u'unheeded', {u'to': -0.6931471805599453, u'upon': -0.6931471805599453})]\n",
      "Default transition probability: -13.7832063505\n"
     ]
    }
   ],
   "source": [
    "print transition_prob_calc\n",
    "print transition_prob_calc.getNumPartitions()\n",
    "print transition_prob_calc.count()\n",
    "print transition_prob_calc.take(5)\n",
    "\n",
    "print 'Default transition probability:', default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transition_prob = transition_prob_calc.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "# generate dictionary\n",
    "#\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = make_all_lower.map(lambda line: regex.sub(' ', line)).flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[46] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "1105285\n",
      "[u'the', u'project', u'gutenberg', u'ebook', u'of']\n"
     ]
    }
   ],
   "source": [
    "print all_words\n",
    "print all_words.getNumPartitions()\n",
    "print all_words.count()\n",
    "print all_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_once = all_words.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[49] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "1105285\n",
      "[(u'the', 1), (u'project', 1), (u'gutenberg', 1), (u'ebook', 1), (u'of', 1)]\n"
     ]
    }
   ],
   "source": [
    "print count_once\n",
    "print count_once.getNumPartitions()\n",
    "print count_once.count()\n",
    "print count_once.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_words_with_count = count_once.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[56] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "29157\n",
      "[(u'aided', 17), (u'bennigsens', 1), (u'suicidal', 5), (u'linsey', 1), (u'worshiped', 1)]\n"
     ]
    }
   ],
   "source": [
    "print unique_words_with_count\n",
    "print unique_words_with_count.getNumPartitions()\n",
    "print unique_words_with_count.count()\n",
    "print unique_words_with_count.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert MAX_EDIT_DISTANCE>0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_deletes = unique_words_with_count.map(lambda (parent, count): \n",
    "                                                   (parent, get_n_deletes_list(parent, MAX_EDIT_DISTANCE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[59] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "29157\n",
      "[(u'aided', [u'ided', u'aded', u'aied', u'aidd', u'aide', u'ded', u'ied', u'idd', u'ide', u'aed', u'add', u'ade', u'aid', u'aie', u'ed', u'dd', u'de', u'id', u'ie', u'ad', u'ae', u'ai']), (u'bennigsens', [u'ennigsens', u'bnnigsens', u'benigsens', u'benngsens', u'bennisens', u'bennigens', u'bennigsns', u'bennigses', u'bennigsen', u'nnigsens', u'enigsens', u'enngsens', u'ennisens', u'ennigens', u'ennigsns', u'ennigses', u'ennigsen', u'bnigsens', u'bnngsens', u'bnnisens', u'bnnigens', u'bnnigsns', u'bnnigses', u'bnnigsen', u'beigsens', u'bengsens', u'benisens', u'benigens', u'benigsns', u'benigses', u'benigsen', u'bennsens', u'benngens', u'benngsns', u'benngses', u'benngsen', u'benniens', u'bennisns', u'bennises', u'bennisen', u'bennigns', u'benniges', u'bennigen', u'bennigss', u'bennigsn', u'bennigse', u'nigsens', u'nngsens', u'nnisens', u'nnigens', u'nnigsns', u'nnigses', u'nnigsen', u'eigsens', u'engsens', u'enisens', u'enigens', u'enigsns', u'enigses', u'enigsen', u'ennsens', u'enngens', u'enngsns', u'enngses', u'enngsen', u'enniens', u'ennisns', u'ennises', u'ennisen', u'ennigns', u'enniges', u'ennigen', u'ennigss', u'ennigsn', u'ennigse', u'bigsens', u'bngsens', u'bnisens', u'bnigens', u'bnigsns', u'bnigses', u'bnigsen', u'bnnsens', u'bnngens', u'bnngsns', u'bnngses', u'bnngsen', u'bnniens', u'bnnisns', u'bnnises', u'bnnisen', u'bnnigns', u'bnniges', u'bnnigen', u'bnnigss', u'bnnigsn', u'bnnigse', u'begsens', u'beisens', u'beigens', u'beigsns', u'beigses', u'beigsen', u'bensens', u'bengens', u'bengsns', u'bengses', u'bengsen', u'beniens', u'benisns', u'benises', u'benisen', u'benigns', u'beniges', u'benigen', u'benigss', u'benigsn', u'benigse', u'bennens', u'bennsns', u'bennses', u'bennsen', u'benngns', u'bennges', u'benngen', u'benngss', u'benngsn', u'benngse', u'bennins', u'bennies', u'bennien', u'benniss', u'bennisn', u'bennise', u'bennigs', u'bennign', u'bennige']), (u'suicidal', [u'uicidal', u'sicidal', u'sucidal', u'suiidal', u'suicdal', u'suicial', u'suicidl', u'suicida', u'icidal', u'ucidal', u'uiidal', u'uicdal', u'uicial', u'uicidl', u'uicida', u'scidal', u'siidal', u'sicdal', u'sicial', u'sicidl', u'sicida', u'suidal', u'sucdal', u'sucial', u'sucidl', u'sucida', u'suiial', u'suiidl', u'suiida', u'suical', u'suicdl', u'suicda', u'suicil', u'suicia', u'suicid', u'cidal', u'iidal', u'icdal', u'icial', u'icidl', u'icida', u'uidal', u'ucdal', u'ucial', u'ucidl', u'ucida', u'uiial', u'uiidl', u'uiida', u'uical', u'uicdl', u'uicda', u'uicil', u'uicia', u'uicid', u'sidal', u'scdal', u'scial', u'scidl', u'scida', u'siial', u'siidl', u'siida', u'sical', u'sicdl', u'sicda', u'sicil', u'sicia', u'sicid', u'sudal', u'suial', u'suidl', u'suida', u'sucal', u'sucdl', u'sucda', u'sucil', u'sucia', u'sucid', u'suiil', u'suiia', u'suiid', u'suicl', u'suica', u'suicd', u'suici']), (u'linsey', [u'insey', u'lnsey', u'lisey', u'liney', u'linsy', u'linse', u'nsey', u'isey', u'iney', u'insy', u'inse', u'lsey', u'lney', u'lnsy', u'lnse', u'liey', u'lisy', u'lise', u'liny', u'line', u'lins', u'sey', u'ney', u'nsy', u'nse', u'iey', u'isy', u'ise', u'iny', u'ine', u'ins', u'ley', u'lsy', u'lse', u'lny', u'lne', u'lns', u'liy', u'lie', u'lis', u'lin']), (u'worshiped', [u'orshiped', u'wrshiped', u'woshiped', u'worhiped', u'worsiped', u'worshped', u'worshied', u'worshipd', u'worshipe', u'rshiped', u'oshiped', u'orhiped', u'orsiped', u'orshped', u'orshied', u'orshipd', u'orshipe', u'wshiped', u'wrhiped', u'wrsiped', u'wrshped', u'wrshied', u'wrshipd', u'wrshipe', u'wohiped', u'wosiped', u'woshped', u'woshied', u'woshipd', u'woshipe', u'woriped', u'worhped', u'worhied', u'worhipd', u'worhipe', u'worsped', u'worsied', u'worsipd', u'worsipe', u'worshed', u'worshpd', u'worshpe', u'worshid', u'worshie', u'worship', u'shiped', u'rhiped', u'rsiped', u'rshped', u'rshied', u'rshipd', u'rshipe', u'ohiped', u'osiped', u'oshped', u'oshied', u'oshipd', u'oshipe', u'oriped', u'orhped', u'orhied', u'orhipd', u'orhipe', u'orsped', u'orsied', u'orsipd', u'orsipe', u'orshed', u'orshpd', u'orshpe', u'orshid', u'orshie', u'orship', u'whiped', u'wsiped', u'wshped', u'wshied', u'wshipd', u'wshipe', u'wriped', u'wrhped', u'wrhied', u'wrhipd', u'wrhipe', u'wrsped', u'wrsied', u'wrsipd', u'wrsipe', u'wrshed', u'wrshpd', u'wrshpe', u'wrshid', u'wrshie', u'wrship', u'woiped', u'wohped', u'wohied', u'wohipd', u'wohipe', u'wosped', u'wosied', u'wosipd', u'wosipe', u'woshed', u'woshpd', u'woshpe', u'woshid', u'woshie', u'woship', u'worped', u'woried', u'woripd', u'woripe', u'worhed', u'worhpd', u'worhpe', u'worhid', u'worhie', u'worhip', u'worsed', u'worspd', u'worspe', u'worsid', u'worsie', u'worsip', u'worshd', u'worshe', u'worshp', u'worshi'])]\n"
     ]
    }
   ],
   "source": [
    "print generate_deletes\n",
    "print generate_deletes.getNumPartitions()\n",
    "print generate_deletes.count()\n",
    "print generate_deletes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expand_deletes = generate_deletes.flatMapValues(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[62] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "2863776\n",
      "[(u'aided', u'ided'), (u'aided', u'aded'), (u'aided', u'aied'), (u'aided', u'aidd'), (u'aided', u'aide')]\n"
     ]
    }
   ],
   "source": [
    "print expand_deletes\n",
    "print expand_deletes.getNumPartitions()\n",
    "print expand_deletes.count()\n",
    "print expand_deletes.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "swap = expand_deletes.map(lambda (orig, delete): (delete, ([orig], 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[65] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "2863776\n",
      "[(u'ided', ([u'aided'], 0)), (u'aded', ([u'aided'], 0)), (u'aied', ([u'aided'], 0)), (u'aidd', ([u'aided'], 0)), (u'aide', ([u'aided'], 0))]\n"
     ]
    }
   ],
   "source": [
    "print swap\n",
    "print swap.getNumPartitions()\n",
    "print swap.count()\n",
    "print swap.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = unique_words_with_count.mapValues(lambda count: ([], count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[68] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "29157\n",
      "[(u'aided', ([], 17)), (u'bennigsens', ([], 1)), (u'suicidal', ([], 5)), (u'linsey', ([], 1)), (u'worshiped', ([], 1))]\n"
     ]
    }
   ],
   "source": [
    "print corpus\n",
    "print corpus.getNumPartitions()\n",
    "print corpus.count()\n",
    "print corpus.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combine = swap.union(corpus)  # combine deletes with main dictionary, eliminate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnionRDD[71] at union at NativeMethodAccessorImpl.java:-2\n",
      "12\n",
      "2892933\n",
      "[(u'ided', ([u'aided'], 0)), (u'aded', ([u'aided'], 0)), (u'aied', ([u'aided'], 0)), (u'aidd', ([u'aided'], 0)), (u'aide', ([u'aided'], 0))]\n"
     ]
    }
   ],
   "source": [
    "print combine\n",
    "print combine.getNumPartitions()\n",
    "print combine.count()\n",
    "print combine.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = combine.reduceByKeyLocally(lambda a, b: (a[0]+b[0], a[1]+b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "longest_word_length = unique_words_with_count.map(lambda (k, v): len(k)).reduce(max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# Sentence-level parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(word typed|word intended)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suggestions(string, dictionary, \n",
    "                    longest_word_length, min_count=1):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        return []\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>=min_count):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>=min_count): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions:\n",
    "    # (correction, (frequency in corpus, edit distance)):\n",
    "    as_list = suggest_dict.items()\n",
    "    return sorted(as_list, key = \\\n",
    "                  lambda (term, (freq, dist)): (dist, -freq))\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file')\n",
    "    [('file', (5, 0)),\n",
    "     ('five', (67, 1)),\n",
    "     ('fire', (54, 1)),\n",
    "     ('fine', (17, 1))...]  \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(words, dictionary, longest_word_length,\n",
    "            start_prob, default_start_prob, \n",
    "            transition_prob, default_transition_prob,\n",
    "            num_word_suggestions=5000):\n",
    "    \n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    path_context = []\n",
    "    \n",
    "    # character level correction - used to determine state space\n",
    "    corrections = get_suggestions(\n",
    "        words[0], dictionary, longest_word_length)\n",
    "\n",
    "    # to ensure Viterbi can keep running\n",
    "    # -- use the word itself if no corrections are found\n",
    "    if len(corrections) == 0:\n",
    "        corrections = [(words[0], (1, 0))]\n",
    "    else:    \n",
    "        if len(corrections) > num_word_suggestions:\n",
    "            corrections = corrections[0:num_word_suggestions]\n",
    "        \n",
    "    # Initialize base cases (t == 0)\n",
    "    for sug_word in corrections:\n",
    "        \n",
    "        # compute the value for all possible starting states\n",
    "        V[0][sug_word[0]] = math.exp(\n",
    "            get_start_prob(sug_word[0], start_prob, \n",
    "                           default_start_prob)\n",
    "            + get_emission_prob(sug_word[1][1]))\n",
    "        \n",
    "        # remember all the different paths (only one state so far)\n",
    "        path[sug_word[0]] = [sug_word[0]]\n",
    " \n",
    "    # normalize for numerical stability\n",
    "    path_temp_sum = sum(V[0].values())\n",
    "    V[0].update({k: math.log(v/path_temp_sum) \n",
    "                 for k, v in V[0].items()})\n",
    "    \n",
    "    # keep track of previous state space\n",
    "    prev_corrections = [i[0] for i in corrections]\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        path_context = [max(V[0], key=lambda i: V[0][i])]\n",
    "        return path_context\n",
    "\n",
    "    # run Viterbi for t > 0\n",
    "    for t in range(1, len(words)):\n",
    "\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        \n",
    "        # character level correction\n",
    "        corrections = get_suggestions(\n",
    "            words[t], dictionary, longest_word_length)\n",
    "        \n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        if len(corrections) == 0:\n",
    "            corrections = [(words[t], (1, 0))]\n",
    "        else:\n",
    "            if len(corrections) > num_word_suggestions:\n",
    "                corrections = corrections[0:num_word_suggestions]\n",
    " \n",
    "        for sug_word in corrections:\n",
    "        \n",
    "            sug_word_emission_prob = get_emission_prob(sug_word[1][1])\n",
    "            \n",
    "            # compute the values coming from all possible previous\n",
    "            # states, only keep the maximum\n",
    "            (prob, word) = max(\n",
    "                (get_belief(prev_word, V[t-1]) \n",
    "                + get_transition_prob(sug_word[0], prev_word, \n",
    "                    transition_prob, default_transition_prob)\n",
    "                + sug_word_emission_prob, prev_word) \n",
    "                               for prev_word in prev_corrections)\n",
    "\n",
    "            # save the maximum value for each state\n",
    "            V[t][sug_word[0]] = math.exp(prob)\n",
    "            \n",
    "            # remember the path we came from to get this maximum value\n",
    "            new_path[sug_word[0]] = path[word] + [sug_word[0]]\n",
    "            \n",
    "        # normalize for numerical stability\n",
    "        path_temp_sum = sum(V[t].values())\n",
    "        V[t].update({k: math.log(v/path_temp_sum) \n",
    "                     for k, v in V[t].items()})\n",
    "        \n",
    "        # keep track of previous state space\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "        # don't need to remember the old paths\n",
    "        path = new_path\n",
    "     \n",
    "    (prob, word) = max((V[t][sug_word[0]], sug_word[0]) \n",
    "                       for sug_word in corrections)\n",
    "    path_context = path[word]\n",
    "    \n",
    "    return path_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############\n",
    "#\n",
    "# load file & initial processing\n",
    "#\n",
    "############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"testdata/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# broadcast Python dictionaries to workers\n",
    "bc_dictionary = sc.broadcast(dictionary)\n",
    "bc_start_prob = sc.broadcast(start_prob)\n",
    "bc_transition_prob = sc.broadcast(transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_all_lower = sc.textFile(fname).map(lambda line: line.lower()).filter(lambda x: x!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[562] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "7\n",
      "[u'this is a test', u'this is a test', u'here is54a test', u'this is ax test', u'this is za test']\n"
     ]
    }
   ],
   "source": [
    "print make_all_lower\n",
    "print make_all_lower.getNumPartitions()\n",
    "print make_all_lower.count()\n",
    "print make_all_lower.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_sentence = make_all_lower.flatMap(lambda line: line.split('.')).map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[566] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "7\n",
      "[[u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'], [u'here', u'is', u'a', u'test'], [u'this', u'is', u'ax', u'test'], [u'this', u'is', u'za', u'test']]\n"
     ]
    }
   ],
   "source": [
    "print split_sentence\n",
    "print split_sentence.getNumPartitions()\n",
    "print split_sentence.count()\n",
    "print split_sentence.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words checked:  27\n"
     ]
    }
   ],
   "source": [
    "# use accumulator to count the number of words checked\n",
    "accum_total_words = sc.accumulator(0)\n",
    "split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "print 'Words checked: ', accum_total_words.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[572] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "7\n",
      "[(0, [u'this', u'is', u'a', u'test']), (1, [u'this', u'is', u'a', u'test']), (2, [u'here', u'is', u'a', u'test']), (3, [u'this', u'is', u'ax', u'test']), (4, [u'this', u'is', u'za', u'test'])]\n"
     ]
    }
   ],
   "source": [
    "print sentence_id\n",
    "print sentence_id.getNumPartitions()\n",
    "print sentence_id.count()\n",
    "print sentence_id.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_correction = sentence_id.map(lambda (k, v): (k, (v, viterbi(\n",
    "                v, bc_dictionary.value, longest_word_length, bc_start_prob.value, \n",
    "                default_start_prob, bc_transition_prob.value, default_transition_prob))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[576] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "7\n",
      "[(0, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'])), (1, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'])), (2, ([u'here', u'is', u'a', u'test'], [u'here', u'is', u'a', u'test'])), (3, ([u'this', u'is', u'ax', u'test'], [u'this', u'is', u'ax', u'test'])), (4, ([u'this', u'is', u'za', u'test'], [u'this', u'is', u'za', u'test']))]\n"
     ]
    }
   ],
   "source": [
    "print sentence_correction\n",
    "print sentence_correction.getNumPartitions()\n",
    "print sentence_correction.count()\n",
    "print sentence_correction.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_mismatches(sentences):\n",
    "    orig_sentence, sug_sentence = sentences\n",
    "    mismatches = [(orig_sentence[i], sug_sentence[i]) for i in range(len(orig_sentence)) \n",
    "            if orig_sentence[i]!=sug_sentence[i]]\n",
    "    if len(mismatches)==0:\n",
    "        return None\n",
    "    else:\n",
    "        return mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_mismatch = sentence_correction.map(lambda (k, v): (k, get_sentence_mismatches(v))) \\\n",
    "                .filter(lambda (k,v): v!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[580] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print sentence_mismatch\n",
    "print sentence_mismatch.getNumPartitions()\n",
    "print sentence_mismatch.count()\n",
    "print sentence_mismatch.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_mismatches(mismatches):\n",
    "    sent_id, word_list = mismatches\n",
    "#     result = []\n",
    "#     for word in word_list:\n",
    "#         result.append([sent_id, word[0], word[1]])\n",
    "#     return result\n",
    "    return [[sent_id, word[0], word[1]] for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_mismatch = sentence_mismatch.flatMap(lambda x: split_mismatches(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[584] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "0\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print word_mismatch\n",
    "print word_mismatch.getNumPartitions()\n",
    "print word_mismatch.count()\n",
    "print word_mismatch.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential mismatches:  0\n"
     ]
    }
   ],
   "source": [
    "# use accumulator to count the number of mismatches\n",
    "accum_total_mismatches = sc.accumulator(0)\n",
    "count_mismatches = word_mismatch.foreach(lambda x: accum_total_mismatches.add(1))\n",
    "print 'Potential mismatches: ', accum_total_mismatches.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "printlist=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Words with suggested corrections (line number, word in text, top match):\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# ERROR words are words where the word does not match the first tuple's word (top match)\n",
    "if printlist:\n",
    "    print '    Words with suggested corrections (line number, word in text, top match):'\n",
    "    print word_mismatch.map(lambda x: (x[0], str(x[1]) + \" --> \" + str(x[2]))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# Word-level parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"testdata/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# broadcast Python dictionaries to workers\n",
    "bc_dictionary = sc.broadcast(dictionary)\n",
    "bc_start_prob = sc.broadcast(start_prob)\n",
    "bc_transition_prob = sc.broadcast(transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_all_lower = sc.textFile(fname).map(lambda line: line.lower()).filter(lambda x: x!='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[592] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "7\n",
      "[u'this is a test', u'this is a test', u'here is54a test', u'this is ax test', u'this is za test']\n"
     ]
    }
   ],
   "source": [
    "print make_all_lower\n",
    "print make_all_lower.getNumPartitions()\n",
    "print make_all_lower.count()\n",
    "print make_all_lower.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_sentence = make_all_lower.flatMap(lambda line: line.split('.')).map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[596] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "7\n",
      "[[u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'], [u'here', u'is', u'a', u'test'], [u'this', u'is', u'ax', u'test'], [u'this', u'is', u'za', u'test']]\n"
     ]
    }
   ],
   "source": [
    "print split_sentence\n",
    "print split_sentence.getNumPartitions()\n",
    "print split_sentence.count()\n",
    "print split_sentence.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words checked:  27\n"
     ]
    }
   ],
   "source": [
    "# use accumulator to count the number of words checked\n",
    "accum_total_words = sc.accumulator(0)\n",
    "split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "print 'Words checked: ', accum_total_words.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[602] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "7\n",
      "[(0, [u'this', u'is', u'a', u'test']), (1, [u'this', u'is', u'a', u'test']), (2, [u'here', u'is', u'a', u'test']), (3, [u'this', u'is', u'ax', u'test']), (4, [u'this', u'is', u'za', u'test'])]\n"
     ]
    }
   ],
   "source": [
    "print sentence_id\n",
    "print sentence_id.getNumPartitions()\n",
    "print sentence_id.count()\n",
    "print sentence_id.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_sentence_words(sentence, tmp_dict, longest_word_length):\n",
    "    return [[word, get_suggestions(word, tmp_dict, longest_word_length)] \n",
    "            for i, word in enumerate(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_words = sentence_id.map(lambda (k,v): (k, \n",
    "                                map_sentence_words(v, bc_dictionary.value, longest_word_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[641] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "7\n",
      "[(0, [[u'this', [(u'this', (4, 0)), (u'is', (6, 2))]], [u'is', [(u'is', (6, 0)), (u'this', (4, 2))]], [u'a', [(u'a', (4, 0)), (u'ax', (1, 1)), (u'za', (1, 1))]], [u'test', [(u'test', (6, 0))]]]), (1, [[u'this', [(u'this', (4, 0)), (u'is', (6, 2))]], [u'is', [(u'is', (6, 0)), (u'this', (4, 2))]], [u'a', [(u'a', (4, 0)), (u'ax', (1, 1)), (u'za', (1, 1))]], [u'test', [(u'test', (6, 0))]]]), (2, [[u'here', [(u'here', (1, 0)), (u'her', (1, 1))]], [u'is', [(u'is', (6, 0)), (u'this', (4, 2))]], [u'a', [(u'a', (4, 0)), (u'ax', (1, 1)), (u'za', (1, 1))]], [u'test', [(u'test', (6, 0))]]]), (3, [[u'this', [(u'this', (4, 0)), (u'is', (6, 2))]], [u'is', [(u'is', (6, 0)), (u'this', (4, 2))]], [u'ax', [(u'ax', (1, 0)), (u'a', (4, 1)), (u'za', (1, 2))]], [u'test', [(u'test', (6, 0))]]]), (4, [[u'this', [(u'this', (4, 0)), (u'is', (6, 2))]], [u'is', [(u'is', (6, 0)), (u'this', (4, 2))]], [u'za', [(u'za', (1, 0)), (u'a', (4, 1)), (u'ax', (1, 2))]], [u'test', [(u'test', (6, 0))]]])]\n"
     ]
    }
   ],
   "source": [
    "print sentence_words\n",
    "print sentence_words.getNumPartitions()\n",
    "print sentence_words.count()\n",
    "print sentence_words.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# testing itertools\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# suggestions\n",
    "# 'this': [('this', (4, 0)), ('is', (6, 2))]\t\t\t\t2\n",
    "# 'is': [('is', (6, 0)), (u'this', (4, 2))]\t\t\t\t\t2\n",
    "# 'a': [('a', (4, 0)), (u'ax', (1, 1)), (u'za', (1, 1))]\t3\n",
    "# 'test': [('test', (6, 0))]\t\t\t\t\t\t\t\t1\n",
    "# combinations: 2*2*3*1=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_this = [('this', (4, 0)), ('is', (6, 2))]\n",
    "w_is = [('is', (6, 0)), (u'this', (4, 2))]\n",
    "w_a = [('a', (4, 0)), (u'ax', (1, 1)), (u'za', (1, 1))]\n",
    "w_test = [('test', (6, 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('this', (4, 0)), ('is', (6, 0)), ('a', (4, 0)), ('test', (6, 0))),\n",
       " (('this', (4, 0)), ('is', (6, 0)), (u'ax', (1, 1)), ('test', (6, 0))),\n",
       " (('this', (4, 0)), ('is', (6, 0)), (u'za', (1, 1)), ('test', (6, 0))),\n",
       " (('this', (4, 0)), (u'this', (4, 2)), ('a', (4, 0)), ('test', (6, 0))),\n",
       " (('this', (4, 0)), (u'this', (4, 2)), (u'ax', (1, 1)), ('test', (6, 0))),\n",
       " (('this', (4, 0)), (u'this', (4, 2)), (u'za', (1, 1)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), ('is', (6, 0)), ('a', (4, 0)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), ('is', (6, 0)), (u'ax', (1, 1)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), ('is', (6, 0)), (u'za', (1, 1)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), (u'this', (4, 2)), ('a', (4, 0)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), (u'this', (4, 2)), (u'ax', (1, 1)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), (u'this', (4, 2)), (u'za', (1, 1)), ('test', (6, 0)))]"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.product(w_this, w_is, w_a, w_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_list = [w_this, w_is, w_a, w_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('this', (4, 0)), ('is', (6, 0)), ('a', (4, 0)), ('test', (6, 0))),\n",
       " (('this', (4, 0)), ('is', (6, 0)), (u'ax', (1, 1)), ('test', (6, 0))),\n",
       " (('this', (4, 0)), ('is', (6, 0)), (u'za', (1, 1)), ('test', (6, 0))),\n",
       " (('this', (4, 0)), (u'this', (4, 2)), ('a', (4, 0)), ('test', (6, 0))),\n",
       " (('this', (4, 0)), (u'this', (4, 2)), (u'ax', (1, 1)), ('test', (6, 0))),\n",
       " (('this', (4, 0)), (u'this', (4, 2)), (u'za', (1, 1)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), ('is', (6, 0)), ('a', (4, 0)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), ('is', (6, 0)), (u'ax', (1, 1)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), ('is', (6, 0)), (u'za', (1, 1)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), (u'this', (4, 2)), ('a', (4, 0)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), (u'this', (4, 2)), (u'ax', (1, 1)), ('test', (6, 0))),\n",
       " (('is', (6, 2)), (u'this', (4, 2)), (u'za', (1, 1)), ('test', (6, 0)))]"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.product(*big_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################\n",
    "# back to live code\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_suggestions(sentence):\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        w = word[0]\n",
    "        sug = [(s[0],s[1][1]) for s in word[1]]\n",
    "        result.append([(w, s[0], get_emission_prob(s[1])) for s in sug])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_word_sug = sentence_words.map(lambda (k,v): (k, split_suggestions(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[647] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "7\n",
      "[(0, [[(u'this', u'this', -0.009999999999999946), (u'this', u'is', -9.913487552536127)], [(u'is', u'is', -0.009999999999999946), (u'is', u'this', -9.913487552536127)], [(u'a', u'a', -0.009999999999999946), (u'a', u'ax', -4.615170185988091), (u'a', u'za', -4.615170185988091)], [(u'test', u'test', -0.009999999999999946)]])]\n"
     ]
    }
   ],
   "source": [
    "print sentence_word_sug\n",
    "print sentence_word_sug.getNumPartitions()\n",
    "print sentence_word_sug.count()\n",
    "print sentence_word_sug.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_combos(sug_lists):\n",
    "    return list(itertools.product(*sug_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_word_combos = sentence_word_sug.map(lambda (k, v): (k, get_word_combos(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[656] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "7\n",
      "[(0, [((u'this', u'this', -0.009999999999999946), (u'is', u'is', -0.009999999999999946), (u'a', u'a', -0.009999999999999946), (u'test', u'test', -0.009999999999999946)), ((u'this', u'this', -0.009999999999999946), (u'is', u'is', -0.009999999999999946), (u'a', u'ax', -4.615170185988091), (u'test', u'test', -0.009999999999999946)), ((u'this', u'this', -0.009999999999999946), (u'is', u'is', -0.009999999999999946), (u'a', u'za', -4.615170185988091), (u'test', u'test', -0.009999999999999946)), ((u'this', u'this', -0.009999999999999946), (u'is', u'this', -9.913487552536127), (u'a', u'a', -0.009999999999999946), (u'test', u'test', -0.009999999999999946)), ((u'this', u'this', -0.009999999999999946), (u'is', u'this', -9.913487552536127), (u'a', u'ax', -4.615170185988091), (u'test', u'test', -0.009999999999999946)), ((u'this', u'this', -0.009999999999999946), (u'is', u'this', -9.913487552536127), (u'a', u'za', -4.615170185988091), (u'test', u'test', -0.009999999999999946)), ((u'this', u'is', -9.913487552536127), (u'is', u'is', -0.009999999999999946), (u'a', u'a', -0.009999999999999946), (u'test', u'test', -0.009999999999999946)), ((u'this', u'is', -9.913487552536127), (u'is', u'is', -0.009999999999999946), (u'a', u'ax', -4.615170185988091), (u'test', u'test', -0.009999999999999946)), ((u'this', u'is', -9.913487552536127), (u'is', u'is', -0.009999999999999946), (u'a', u'za', -4.615170185988091), (u'test', u'test', -0.009999999999999946)), ((u'this', u'is', -9.913487552536127), (u'is', u'this', -9.913487552536127), (u'a', u'a', -0.009999999999999946), (u'test', u'test', -0.009999999999999946)), ((u'this', u'is', -9.913487552536127), (u'is', u'this', -9.913487552536127), (u'a', u'ax', -4.615170185988091), (u'test', u'test', -0.009999999999999946)), ((u'this', u'is', -9.913487552536127), (u'is', u'this', -9.913487552536127), (u'a', u'za', -4.615170185988091), (u'test', u'test', -0.009999999999999946))])]\n"
     ]
    }
   ],
   "source": [
    "print sentence_word_combos\n",
    "print sentence_word_combos.getNumPartitions()\n",
    "print sentence_word_combos.count()\n",
    "print sentence_word_combos.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_combos(combos):\n",
    "    sent_id, combo_list = combos\n",
    "    return [[sent_id, c] for c in combo_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_word_combos_split = sentence_word_combos.flatMap(lambda x: split_combos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[665] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "76\n",
      "[[0, ((u'this', u'this', -0.009999999999999946), (u'is', u'is', -0.009999999999999946), (u'a', u'a', -0.009999999999999946), (u'test', u'test', -0.009999999999999946))], [0, ((u'this', u'this', -0.009999999999999946), (u'is', u'is', -0.009999999999999946), (u'a', u'ax', -4.615170185988091), (u'test', u'test', -0.009999999999999946))], [0, ((u'this', u'this', -0.009999999999999946), (u'is', u'is', -0.009999999999999946), (u'a', u'za', -4.615170185988091), (u'test', u'test', -0.009999999999999946))], [0, ((u'this', u'this', -0.009999999999999946), (u'is', u'this', -9.913487552536127), (u'a', u'a', -0.009999999999999946), (u'test', u'test', -0.009999999999999946))], [0, ((u'this', u'this', -0.009999999999999946), (u'is', u'this', -9.913487552536127), (u'a', u'ax', -4.615170185988091), (u'test', u'test', -0.009999999999999946))]]\n"
     ]
    }
   ],
   "source": [
    "print sentence_word_combos_split\n",
    "print sentence_word_combos_split.getNumPartitions()\n",
    "print sentence_word_combos_split.count()\n",
    "print sentence_word_combos_split.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_combo_prob(combo, tmp_sp, d_sp, tmp_tp, d_tp):\n",
    "    \n",
    "    # first word in sentence\n",
    "    # emission prob * start prob\n",
    "    orig_path = [combo[0][0]]\n",
    "    sug_path = [combo[0][1]]\n",
    "    prob = combo[0][2] + get_start_prob(combo[0][1], tmp_sp, d_sp)\n",
    "    \n",
    "    # subsequent words\n",
    "    for i, w in enumerate(combo[1:]):\n",
    "        orig_path.append(w[0])\n",
    "        sug_path.append(w[1])\n",
    "        prob += w[2] + get_transition_prob(w[1], combo[i-1][1], tmp_tp, d_tp)\n",
    "    \n",
    "    return orig_path, sug_path, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_word_combos_prob = sentence_word_combos_split.map(lambda (k,v): (k, \n",
    "                                get_combo_prob(v, bc_start_prob.value, default_start_prob, \n",
    "                                               bc_transition_prob.value, default_transition_prob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[703] at RDD at PythonRDD.scala:43\n",
      "2\n",
      "76\n",
      "[(0, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'], -9.586812608597395)), (0, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'ax', u'test'], -14.191982794585487)), (0, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'za', u'test'], -14.191982794585487)), (0, ([u'this', u'is', u'a', u'test'], [u'this', u'this', u'a', u'test'], -19.490300161133522)), (0, ([u'this', u'is', u'a', u'test'], [u'this', u'this', u'ax', u'test'], -24.095470347121616)), (0, ([u'this', u'is', u'a', u'test'], [u'this', u'this', u'za', u'test'], -24.095470347121616)), (0, ([u'this', u'is', u'a', u'test'], [u'is', u'is', u'a', u'test'], -18.286327356807586)), (0, ([u'this', u'is', u'a', u'test'], [u'is', u'is', u'ax', u'test'], -24.277791903915567)), (0, ([u'this', u'is', u'a', u'test'], [u'is', u'is', u'za', u'test'], -24.277791903915567)), (0, ([u'this', u'is', u'a', u'test'], [u'is', u'this', u'a', u'test'], -28.189814909343713)), (0, ([u'this', u'is', u'a', u'test'], [u'is', u'this', u'ax', u'test'], -34.18127945645169)), (0, ([u'this', u'is', u'a', u'test'], [u'is', u'this', u'za', u'test'], -34.18127945645169))]\n"
     ]
    }
   ],
   "source": [
    "print sentence_word_combos_prob\n",
    "print sentence_word_combos_prob.getNumPartitions()\n",
    "print sentence_word_combos_prob.count()\n",
    "print sentence_word_combos_prob.take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_max_prob = sentence_word_combos_prob.reduceByKey(lambda a,b: a if a[2] > b[2] else b, \n",
    "                                                          numPartitions = n_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[794] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "7\n",
      "[(0, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'], -9.586812608597395)), (6, ([u'her', u'tee', u'set'], [u'her', u'tee', u'set'], -7.967374696163295)), (1, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'], -9.586812608597395)), (2, ([u'here', u'is', u'a', u'test'], [u'here', u'is', u'a', u'test'], -10.973106969717286)), (3, ([u'this', u'is', u'ax', u'test'], [u'this', u'is', u'ax', u'test'], -9.586812608597395))]\n"
     ]
    }
   ],
   "source": [
    "print sentence_max_prob\n",
    "print sentence_max_prob.getNumPartitions()\n",
    "print sentence_max_prob.count()\n",
    "print sentence_max_prob.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_mismatch = sentence_max_prob.map(lambda (k,v): (k, (v[0], v[1]))) \\\n",
    "     .map(lambda (k, v): (k, get_sentence_mismatches(v))) \\\n",
    "     .filter(lambda (k,v): v!=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[823] at RDD at PythonRDD.scala:43\n",
      "6\n",
      "7\n",
      "[(0, None), (6, None), (1, None), (2, None), (3, None)]\n"
     ]
    }
   ],
   "source": [
    "print sentence_mismatch\n",
    "print sentence_mismatch.getNumPartitions()\n",
    "print sentence_mismatch.count()\n",
    "print sentence_mismatch.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
