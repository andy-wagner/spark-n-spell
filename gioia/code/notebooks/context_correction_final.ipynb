{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Level Correction -  Serial Version and Spark Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context_correction.ipynb\n",
    "\n",
    "######################\n",
    "#\n",
    "# Submission by Gioia Dominedo (Harvard ID: 40966234) for\n",
    "# CS 205 - Computing Foundations for Computational Science\n",
    "# \n",
    "# This is part of a joint project with Kendrick Lo that includes a\n",
    "# separate component for word-level checking. This notebook outlines\n",
    "# algorithms for context-level correction, and includes a serial\n",
    "# Python algorithm adapted from third party algorithms (Symspell and\n",
    "# Viterbi algorithms), as well as a Spark/Python algorithm. \n",
    "#\n",
    "# The following were also used as references:\n",
    "# Peter Norvig, How to Write a Spelling Corrector\n",
    "#\t(http://norvig.com/spell-correct.html)\n",
    "# Peter Norvig, Natural Language Corpus Data: Beautiful Data\n",
    "#\t(http://norvig.com/ngrams/ch14.pdf)\n",
    "#\n",
    "# Two main approaches to parallelization were attempted: sentence-\n",
    "# level and word-level. Both attempts are documented in this notebook.\n",
    "#\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# CONTEXT-LEVEL CORRECTION LOGIC\n",
    "#\n",
    "# Each sentence is modeled as a hidden Markov model. Prior\n",
    "# probabilities (for first words in the sentences) and transition\n",
    "# probabilities (for all subsequent words) are calculated when\n",
    "# generating the main dictionary, using the same corpus. Emission\n",
    "# probabilities are generated on the fly by parameterizing a Poisson \n",
    "# distribution with the edit distance. The state space of possible\n",
    "# corrections is based on the suggested words from the word-level\n",
    "# correction. Words must (a) be 'real' word (i.e. appear at least\n",
    "# once in the corpus) used to generate the dictionary in order to be\n",
    "# considered valid suggestions; this ensures that the state space\n",
    "# remains manageable by ignoring words that have not been seen\n",
    "# previously.\n",
    "#\n",
    "# All probabilities are stored in log-space to avoid underflow. Pre-\n",
    "# defined minimum values are used for words that are not present in\n",
    "# the dictionary and/or probability tables.\n",
    "#\n",
    "# More detail is included at each step below.\n",
    "#\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Serial Code Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>PRE-PROCESSING CODE</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# v 1.0 last revised 27 Nov 2015\n",
    "#\n",
    "# The pre-processing steps have been adapted from the dictionary\n",
    "# creation of the word-level spellchecker, which in turn was based on\n",
    "# SymSpell, a Symmetric Delete spelling correction algorithm\n",
    "# developed by Wolf Garbe and originally written in C#. More detail\n",
    "# on SymSpell is included in the word-level spell-check documentation.\n",
    "#\n",
    "# The main modifications to the word-level spellchecker pre-\n",
    "# processing stages are to create the additional outputs that are\n",
    "# required for the context-level checking, and to eliminate redundant\n",
    "# outputs that are not necessary.\n",
    "#\n",
    "# The outputs of the pre-processing stage are:\n",
    "#\n",
    "# - dictionary: A dictionary that combines both words present in the\n",
    "# corpus and other words that are within a given 'delete distance'. \n",
    "# The format of the dictionary is:\n",
    "# {word: ([list of words within the given 'delete distance'], \n",
    "# word count in corpus)}\n",
    "#\n",
    "# - start_prob: A dictionary with key, value pairs that correspond to\n",
    "# (word, probability of the word being the first word in a sentence)\n",
    "#\n",
    "# - transition_prob: A dictionary of dictionaries that stores the\n",
    "# probability of a given word following another. The format of the\n",
    "# dictionary is:\n",
    "# {previous word: {word1 : P(word1|prevous word), word2 : \n",
    "# P(word2|prevous word), ...}}\n",
    "#\n",
    "# - longest_word_length: The length of the longest word that is\n",
    "# encountered in the corpus.\n",
    "#\n",
    "# - default_start_prob: A benchmark probability of a word being at\n",
    "# the start of a sentence, set to 1 / # of unique words at the\n",
    "# beginning of sentences. This ensures that all previously unseen\n",
    "# words at the beginning of sentences are not corrected unnecessarily.\n",
    "#\n",
    "# - default_transition_prob: A benchmark probability of a word being\n",
    "# seen, given the previous word in the sentence, also set to 1 / # of\n",
    "# words in corpus. This ensures that all previously unseen\n",
    "# transitions are not corrected unnecessarily.\n",
    "#\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_EDIT_DISTANCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deletes_list(w):\n",
    "    '''\n",
    "    Given a word, derive strings with up to max_edit_distance\n",
    "    characters deleted.\n",
    "    '''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(MAX_EDIT_DISTANCE):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary_entry(w, dictionary, longest_word_length):\n",
    "    '''\n",
    "    Adds a word and its derived deletions to the dictionary.\n",
    "    Dictionary entries are of the form:\n",
    "    (list of suggested corrections, frequency of word in corpus)\n",
    "    '''\n",
    "\n",
    "    new_real_word_added = False\n",
    "    \n",
    "    # check if word is already in dictionary\n",
    "    if w in dictionary:\n",
    "        # increment count of word in corpus\n",
    "        dictionary[w] = (dictionary[w][0], dictionary[w][1] + 1)\n",
    "    else:\n",
    "        # create new entry in dictionary\n",
    "        dictionary[w] = ([], 1)  \n",
    "        longest_word_length = max(longest_word_length, len(w))\n",
    "        \n",
    "    if dictionary[w][1]==1:\n",
    "        \n",
    "        # first appearance of word in corpus\n",
    "        # n.b. word may already be in dictionary as a derived word\n",
    "        # (deleting character from a real word)\n",
    "        # but counter of frequency of word in corpus is not\n",
    "        # incremented in those cases)\n",
    "        \n",
    "        new_real_word_added = True\n",
    "        deletes = get_deletes_list(w)\n",
    "        \n",
    "        for item in deletes:\n",
    "            if item in dictionary:\n",
    "                # add (correct) word to delete's suggested correction\n",
    "                # list if not already there\n",
    "                if item not in dictionary[item][0]:\n",
    "                    dictionary[item][0].append(w)\n",
    "            else:\n",
    "                # note: frequency of word in corpus is not incremented\n",
    "                dictionary[item] = ([w], 0)  \n",
    "        \n",
    "    return new_real_word_added, longest_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary(fname):\n",
    "    '''\n",
    "    Loads a text file and uses it to create a dictionary and\n",
    "    to calculate start probabilities and transition probabilities. \n",
    "    Please refer to the text above for a full description.\n",
    "    '''\n",
    "    \n",
    "    print 'Creating dictionary...'\n",
    "\n",
    "    dictionary = dict()\n",
    "    longest_word_length = 0\n",
    "    start_prob = dict()\n",
    "    transition_prob = dict()\n",
    "    word_count = 0\n",
    "    \n",
    "    with open(fname) as file:    \n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            # process each sentence separately\n",
    "            for sentence in line.split('.'):\n",
    "                \n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', sentence.lower())      \n",
    "                \n",
    "                for w, word in enumerate(words):\n",
    "                    \n",
    "                    new_word, longest_word_length = \\\n",
    "                        create_dictionary_entry(word, dictionary,\n",
    "                                                longest_word_length)\n",
    "                    \n",
    "                    if new_word:\n",
    "                        word_count += 1\n",
    "                        \n",
    "                    # update probabilities for Hidden Markov Model\n",
    "                    if w == 0:\n",
    "\n",
    "                        # probability of a word being at the\n",
    "                        # beginning of a sentence\n",
    "                        if word in start_prob:\n",
    "                            start_prob[word] += 1\n",
    "                        else:\n",
    "                            start_prob[word] = 1\n",
    "                    else:\n",
    "                        \n",
    "                        # probability of transitionining from one\n",
    "                        # word to another\n",
    "                        # dictionary format:\n",
    "                        # {previous word: {word1 : P(word1|prevous\n",
    "                        # word), word2 : P(word2|prevous word)}}\n",
    "                        \n",
    "                        # check that prior word is present\n",
    "                        # - create if not\n",
    "                        if words[w - 1] not in transition_prob:\n",
    "                            transition_prob[words[w - 1]] = dict()\n",
    "                            \n",
    "                        # check that current word is present\n",
    "                        # - create if not\n",
    "                        if word not in transition_prob[words[w - 1]]:\n",
    "                            transition_prob[words[w - 1]][word] = 0\n",
    "                            \n",
    "                        # update value\n",
    "                        transition_prob[words[w - 1]][word] += 1\n",
    "                    \n",
    "    # convert counts to log-probabilities, to avoid underflow in\n",
    "    # later calculations (note: natural logarithm, not base-10)\n",
    "    \n",
    "    total_start_words = float(sum(start_prob.values()))\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    start_prob.update( \n",
    "        {k: math.log(v/total_start_words)\n",
    "         for k, v in start_prob.items()})\n",
    "    \n",
    "    default_transition_prob = math.log(1./word_count)    \n",
    "    transition_prob.update(\n",
    "        {k: {k1: math.log(float(v1)/sum(v.values()))\n",
    "             for k1, v1 in v.items()} \n",
    "         for k, v in transition_prob.items()})\n",
    "\n",
    "    print 'Total unique words in corpus: %i' % word_count\n",
    "    print 'Total items in dictionary: %i' \\\n",
    "        % len(dictionary)\n",
    "    print '  Edit distance for deletions: %i' % MAX_EDIT_DISTANCE\n",
    "    print '  Length of longest word: %i' % longest_word_length\n",
    "    print 'Total unique words at the start of a sentence: %i' \\\n",
    "        % len(start_prob)\n",
    "    print 'Total unique word transitions: %i' % len(transition_prob)\n",
    "        \n",
    "    return dictionary, longest_word_length, start_prob, \\\n",
    "        default_start_prob, transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dictionary, longest_word_length, \\\n",
    "# start_prob, default_start_prob, \\\n",
    "# transition_prob, default_transition_prob \\\n",
    "# = create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Creating dictionary...  \n",
    "Total unique words in corpus: 29157  \n",
    "Total items in dictionary (corpus words and deletions): 2151998  \n",
    "  Edit distance for deletions: 3  \n",
    "  Length of longest word in corpus: 18  \n",
    "Total unique words appearing at the start of a sentence: 15297  \n",
    "Total unique word transitions: 27224  \n",
    "CPU times: user 34.7 s, sys: 849 ms, total: 35.5 s  \n",
    "Wall time: 36.2 s \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>SPELL-CHECKING CODE</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# v 1.0 last revised 27 Nov 2015\n",
    "#\n",
    "# Reads in a text file, breaks it down into individual sentences (by\n",
    "# splitting on periods), and then carries out context-based spell-\n",
    "# checking on each sentence in turn. In cases where the 'suggested'\n",
    "# word does not match the actual word in the text, both the original\n",
    "# and the suggested sentences are printed.\n",
    "#\n",
    "# Probabilistic model:\n",
    "#\n",
    "# Each sentence is modeled as a hidden Markov model, where the\n",
    "# hidden states are the words that the user intended to type, and\n",
    "# the emissions are the words\n",
    "# that were actually typed.\n",
    "#\n",
    "# For each word in a sentence, we can define:\n",
    "# - emission probabilities: P(word typed|word intended)\n",
    "# - prior probabilities (for first words in sentences only):\n",
    "# P(being the first word in a sentence)\n",
    "# - transition probabilities (for all subsequent words):\n",
    "# P(word intended|previous word intended)\n",
    "#\n",
    "# Prior and transition probabilities were calculated in the pre-\n",
    "# processing step, using the same corpus as the dictionary.\n",
    "# \n",
    "# Emission probabilities are calculated on the fly using a Poisson\n",
    "# distribution, where P(word typed|word intended) = PMF of \n",
    "# Poisson(k, l), where k = edit distance between word type and word\n",
    "# intended, and l=0.01. This approach was taken from the 2015\n",
    "# lecture notes of AM207 Stochastic Optimization, as was the\n",
    "# parameter l=0.01. Various parameters between 0 and 1 were tested,\n",
    "# confirming that 0.01 yields the most accurate word suggestions.\n",
    "# The shape of the PMF is shown in the cell below.\n",
    "#\n",
    "# All probabilities are stored in log-space to avoid underflow. Pre-\n",
    "# defined minimum values (also defined at the pre-processing stage)\n",
    "# are used for words that are not present in the dictionary and/or\n",
    "# probability tables.\n",
    "#\n",
    "# The spell-checking itself is carried out using a modified version\n",
    "# of the Viterbi algorithm, which yields the most likely sequence of\n",
    "# hidden states, i.e. the most likely sequence of words that the\n",
    "# user intended to type. The main difference to the 'standard'\n",
    "# Viterbi algorithm is that the state space (i.e. list of possible\n",
    "# corrections) is generated (and therefore varies) for each word,\n",
    "# instead of considering the state space of all possible words in\n",
    "# the dictionary for every word that is checked. This ensures that\n",
    "# the problem remains tractable.\n",
    "#\n",
    "# The algorithm is best illustrated by way of an example.\n",
    "# e.g. suppose that we are checking the sentence 'This is ax test.'\n",
    "# The emissions are 'This is ax test.' and the hidden states are\n",
    "# 'This is a test.'\n",
    "#\n",
    "# As a pre-processing step, we convert everything to lowercase,\n",
    "# eliminate punctuation, and break the sentence up into a list of\n",
    "# words: ['this', 'is', 'ax', 'text']\n",
    "# This list is passed as a parameter to the viterbi function.\n",
    "#\n",
    "# The algorithm tackles each word in turn, starting with 'this'.\n",
    "#\n",
    "# We first use get_suggestions to obtain a list of all words that\n",
    "# may have been intended, i.e. all possible hidden states (intended\n",
    "# words) for the emission (word typed).\n",
    "# get_suggestions returns 1,004 possible corrections, including:\n",
    "# - 1 word with an edit distance of 0 ['this']\n",
    "# - 2 words with an edit distance of 1 ['his', 'thus', 'thin',\n",
    "# 'tis', 'thins']\n",
    "# - 109 words with an edit distance of 2 ['the', 'that', 'is',\n",
    "# 'him', 'they', ...]\n",
    "# - 889 words with an edit distance of 3 ['to', 'in', 'he', 'was',\n",
    "# 'it', 'as', ...]\n",
    "# Note: get_suggestions is capped at an edit distance of 3.\n",
    "# \n",
    "# These 1,004 words represent our state space, i.e. all possible\n",
    "# words that may have been intended. They each have an emission\n",
    "# probability = PMF of Poisson(edit distance, 0.01)\n",
    "# We refer to this below as the list of possible corrections.\n",
    "#\n",
    "# For each word in the list of possible corrections, we calculate:\n",
    "# P(word starting a sentence) * P(typing 'this'|meaning to type word)\n",
    "# This is a simple application of Bayes' rule: by normalizing the\n",
    "# probabilities we obtain P(meaning to type word|typing 'this') for\n",
    "# each of the 1,004 words.\n",
    "# These probabilities are referred to as the belief state, and they\n",
    "# are stored for\n",
    "# future use. We also store the possible paths, which at this stage\n",
    "# are only one word long.\n",
    "# \n",
    "# We now move on to the next word. After the first word, all\n",
    "# subsequent words are\n",
    "# treated as follows.\n",
    "#\n",
    "# The second word in our test sentence is 'is'. Once again, we use\n",
    "# get_suggestions to obtain a list of all words that may have been\n",
    "# intended. get_suggestions returns 1,124 possible corrections,\n",
    "# including:\n",
    "# - 1 word with an edit distance of 0 ['is']\n",
    "# - 31 words with an edit distance of 1 ['in', 'it', 'his', 'as',\n",
    "# 'i', ...]\n",
    "# - 213 words with an edit distance of 2 ['was', 'him', 'this',\n",
    "# 'so', 'did', ...]\n",
    "# - 879 words with an edit distance of 3 ['with', 'she', 'said',\n",
    "# 'into', ...]\n",
    "# These 1,124 words represent our state space for the second word.\n",
    "#\n",
    "# For each word in the list of possible corrections, we loop through\n",
    "# all the words in the previous list of possible corrections, and\n",
    "# calculate:\n",
    "# belief state(previous word) * P(current word|previous word)\n",
    "#    * P(typing 'is'|meaning to type current word)\n",
    "# We store the previous word that maximizes this calculation, as\n",
    "# well as the probability that it results in.\n",
    "#\n",
    "# For example, suppose that we are considering the possibility that\n",
    "# 'is' was indeed intended to be 'is'. We then calculate: \n",
    "# belief state(previous word) * P('is'|previous word) * P('is'|'is')\n",
    "# for all possible previous words, and discover that the previous\n",
    "# word 'this' maximizes the above calculation. We therefore store\n",
    "# 'this is' as the optimal path for the suggested correction 'is'\n",
    "# (more specifically, path['is'] = 'this is'), and the above\n",
    "# (normalized) probability as the belief state for 'is' (more\n",
    "# specifically, belief['is'] = prob('this is')).\n",
    "#\n",
    "# If the sentence had been only 2 words long, then at this point we\n",
    "# would return the path that maximizes the most recent belief state.\n",
    "# As it is not, we repeat the previous steps for 'ax' and 'test',\n",
    "# and then return the path that is associated with the most likely\n",
    "# belief state at the last step.\n",
    "#\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEZCAYAAACaWyIJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQuCcc23c/wezQJZirVoSswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107465210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# emission probabilities\n",
    "l = 0.01\n",
    "plt.plot(range(10), [poisson.pmf(k, l) for k in range(10)], \n",
    "         color='r', linewidth=2, label='Lambda='+str(l))\n",
    "plt.ylabel('P(word typed|word intended)')\n",
    "plt.xlabel('Edit distance between word typed and word intended')\n",
    "plt.title('Emission probabilities: P(word typed|word intended)')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_suggestions(string, dictionary, \n",
    "                    longest_word_length, min_count):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        return []\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>=min_count):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>=min_count): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions:\n",
    "    # (correction, (frequency in corpus, edit distance)):\n",
    "    as_list = suggest_dict.items()\n",
    "    return sorted(as_list, key = \\\n",
    "                  lambda (term, (freq, dist)): (dist, -freq))\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file')\n",
    "    [('file', (5, 0)),\n",
    "     ('five', (67, 1)),\n",
    "     ('fire', (54, 1)),\n",
    "     ('fine', (17, 1))...]  \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(word typed|word intended)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(words, dictionary, longest_word_length,\n",
    "            start_prob, default_start_prob, \n",
    "            transition_prob, default_transition_prob,\n",
    "            min_count=1,num_word_suggestions=5000):\n",
    "    \n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    path_context = []\n",
    "    \n",
    "    # character level correction - used to determine state space\n",
    "    corrections = get_suggestions(\n",
    "        words[0], dictionary, longest_word_length, min_count)\n",
    "\n",
    "    # to ensure Viterbi can keep running\n",
    "    # -- use the word itself if no corrections are found\n",
    "    if len(corrections) == 0:\n",
    "        corrections = [(words[0], (1, 0))]\n",
    "    else:    \n",
    "        if len(corrections) > num_word_suggestions:\n",
    "            corrections = corrections[0:num_word_suggestions]\n",
    "        \n",
    "    # Initialize base cases (t == 0)\n",
    "    for sug_word in corrections:\n",
    "        \n",
    "        # compute the value for all possible starting states\n",
    "        V[0][sug_word[0]] = math.exp(\n",
    "            get_start_prob(sug_word[0], start_prob, \n",
    "                           default_start_prob)\n",
    "            + get_emission_prob(sug_word[1][1]))\n",
    "        \n",
    "        # remember all the different paths (only one state so far)\n",
    "        path[sug_word[0]] = [sug_word[0]]\n",
    " \n",
    "    # normalize for numerical stability\n",
    "    path_temp_sum = sum(V[0].values())\n",
    "    V[0].update({k: math.log(v/path_temp_sum) \n",
    "                 for k, v in V[0].items()})\n",
    "    \n",
    "    # keep track of previous state space\n",
    "    prev_corrections = [i[0] for i in corrections]\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        path_context = [max(V[0], key=lambda i: V[0][i])]\n",
    "        return path_context\n",
    "\n",
    "    # run Viterbi for t > 0\n",
    "    for t in range(1, len(words)):\n",
    "\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        \n",
    "        # character level correction\n",
    "        corrections = get_suggestions(\n",
    "            words[t], dictionary, longest_word_length, min_count)\n",
    "        \n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        if len(corrections) == 0:\n",
    "            corrections = [(words[t], (1, 0))]\n",
    "        else:\n",
    "            if len(corrections) > num_word_suggestions:\n",
    "                corrections = corrections[0:num_word_suggestions]\n",
    " \n",
    "        for sug_word in corrections:\n",
    "        \n",
    "            sug_word_emission_prob = get_emission_prob(sug_word[1][1])\n",
    "            \n",
    "            # compute the values coming from all possible previous\n",
    "            # states, only keep the maximum\n",
    "            (prob, word) = max(\n",
    "                (get_belief(prev_word, V[t-1]) \n",
    "                + get_transition_prob(sug_word[0], prev_word, \n",
    "                    transition_prob, default_transition_prob)\n",
    "                + sug_word_emission_prob, prev_word) \n",
    "                               for prev_word in prev_corrections)\n",
    "\n",
    "            # save the maximum value for each state\n",
    "            V[t][sug_word[0]] = math.exp(prob)\n",
    "            \n",
    "            # remember the path we came from to get this maximum value\n",
    "            new_path[sug_word[0]] = path[word] + [sug_word[0]]\n",
    "            \n",
    "        # normalize for numerical stability\n",
    "        path_temp_sum = sum(V[t].values())\n",
    "        V[t].update({k: math.log(v/path_temp_sum) \n",
    "                     for k, v in V[t].items()})\n",
    "        \n",
    "        # keep track of previous state space\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "        # don't need to remember the old paths\n",
    "        path = new_path\n",
    "     \n",
    "    (prob, word) = max((V[t][sug_word[0]], sug_word[0]) \n",
    "                       for sug_word in corrections)\n",
    "    path_context = path[word]\n",
    "    \n",
    "    return path_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correct_document_context(fname, dictionary, longest_word_length,\n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob,\n",
    "                             num_word_suggestions=5000):\n",
    "    \n",
    "    doc_word_count = 0\n",
    "    corrected_word_count = 0\n",
    "    \n",
    "    with open(fname) as file:\n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            \n",
    "            for sentence in line.split('.'):\n",
    "                \n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', sentence.lower())  \n",
    "                doc_word_count += len(words)\n",
    "                \n",
    "                if len(words) > 0:\n",
    "                \n",
    "                    suggestion = viterbi(words, dictionary,\n",
    "                                longest_word_length, start_prob, \n",
    "                                default_start_prob, transition_prob,\n",
    "                                default_transition_prob)\n",
    "\n",
    "                    # display sentences with corrections\n",
    "                    if words != suggestion:\n",
    "                        \n",
    "                        # most users will expect to see 1-indexing.\n",
    "                        print '\\nErrors found in line %i.' % (i+1)\n",
    "                        print 'Original sentence: %s' \\\n",
    "                            % (' '.join(words))\n",
    "                        print 'Suggested correction: %s' \\\n",
    "                            % (' '.join(suggestion))\n",
    "                        \n",
    "                        # update count of corrected words\n",
    "                        corrected_word_count += \\\n",
    "                        sum([words[j]!=suggestion[j] \n",
    "                             for j in range(len(words))])\n",
    "  \n",
    "    print '-----'\n",
    "    print 'Total words checked: %i' % doc_word_count\n",
    "    print 'Total potential errors found: %i' % corrected_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# correct_document_context('testdata/test.txt', dictionary,\n",
    "#                          longest_word_length, start_prob,\n",
    "#                          default_start_prob, transition_prob,\n",
    "#                          default_transition_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "min_count = 1  \n",
    "\n",
    "Errors found in line 4.   \n",
    "Original sentence: this is ax test  \n",
    "Suggested correction: this is a test  \n",
    "  \n",
    "Errors found in line 5.   \n",
    "Original sentence: this is za test  \n",
    "Suggested correction: this is a test  \n",
    "  \n",
    "Errors found in line 6.   \n",
    "Original sentence: thee is a test  \n",
    "Suggested correction: there is a test  \n",
    "  \n",
    "Errors found in line 7.   \n",
    "Original sentence: her tee set  \n",
    "Suggested correction: her the set  \n",
    "----- \n",
    "Total words checked: 27  \n",
    "Total potential errors found: 4  \n",
    "CPU times: user 47.5 s, sys: 269 ms, total: 47.8 s  \n",
    "Wall time: 48.7 s  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>SAMPLE USAGE</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "  <strong>Run the cell below only once to build the dictionary.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary: 2151998\n",
      "  Edit distance for deletions: 3\n",
      "  Length of longest word: 18\n",
      "Total unique words at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n"
     ]
    }
   ],
   "source": [
    "dictionary, longest_word_length, \\\n",
    "start_prob, default_start_prob, \\\n",
    "transition_prob, default_transition_prob \\\n",
    "= create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "  <strong>Enter file name of document to correct below.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Errors found in line 4.\n",
      "Original sentence: this is ax test\n",
      "Suggested correction: this is a test\n",
      "\n",
      "Errors found in line 5.\n",
      "Original sentence: this is za test\n",
      "Suggested correction: this is a test\n",
      "\n",
      "Errors found in line 6.\n",
      "Original sentence: thee is a test\n",
      "Suggested correction: there is a test\n",
      "\n",
      "Errors found in line 7.\n",
      "Original sentence: her tee set\n",
      "Suggested correction: her the set\n",
      "-----\n",
      "Total words checked: 27\n",
      "Total potential errors found: 4\n"
     ]
    }
   ],
   "source": [
    "correct_document_context('testdata/test.txt', dictionary,\n",
    "                         longest_word_length, start_prob,\n",
    "                         default_start_prob, transition_prob,\n",
    "                         default_transition_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. SPARK Code Performance - Parallelized Across Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# CONVERT FILE INTO:\n",
    "# [(sentence #, sentence)]\n",
    "#\n",
    "# MAP:\n",
    "# [(sentence #, sentence, viterbi(sentence))]\n",
    "#\n",
    "# MAP:\n",
    "# [(sentence #, sentence, viterbi(sentence), # of corrected words)]\n",
    "#\n",
    "# FILTER: # of corrected words > 0\n",
    "# [(sentence #, sentence, viterbi(sentence), # of corrected words)]\n",
    "#\n",
    "# COLLECT/OUTPUT:\n",
    "# Summary stats on corrections\n",
    "#\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. SPARK Code Performance - Parallelized Across Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# CONVERT FILE INTO:\n",
    "# [(sentence #, word #,), (word, previous word)]\n",
    "# or None if it is the first word in the sentence\n",
    "#\n",
    "# SPLIT INTO 2 RDDS\n",
    "# rdd1 = word # is 1\n",
    "# rdd2 = word # is >1\n",
    "#\n",
    "# rdd1 - first words in the sentence\n",
    "#\n",
    "# MAP:\n",
    "# [(sentence #, word #), (original word, [all word suggestions])]\n",
    "#\n",
    "# MAP:\n",
    "# [(sentence #, word #), (original word, [tuples for all word sug of (word, emission prob)])]\n",
    "#\n",
    "# MAP:\n",
    "# [(sentence #, word #), (original word, suggested word, emission prob * suggested word prob)],\n",
    "# i.e. a list of tuples for all words in list of word suggestions\n",
    "#\n",
    "# FLATMAP\n",
    "# [(sentence #, word #), (original word, suggested word, emission prob * suggested word prob)]\n",
    "#\n",
    "# REDUCE\n",
    "# [(sentence #, word #), original word, suggested word]\n",
    "# where suggested word is the word with maximum emission prob * suggested word prob\n",
    "#\n",
    "# rdd2 - all other words:\n",
    "#\n",
    "# MAP:\n",
    "# [(sentence #, word #), (original word, [all word suggestions], [all previous word suggestions])]\n",
    "#\n",
    "# MAP:\n",
    "# [(sentence #, word #), (original word, [tuples for all word sug of (word, emission prob)], [all previous word sug])]\n",
    "#\n",
    "# MAP:\n",
    "# [(sentence #, word #), (original word, [tuples for all word, previous word combinations with (word, prev word, emission)])]\n",
    "#\n",
    "# MAP:\n",
    "# [(sentence #, word #), (original word, prev word, emission prob * transition prob)],\n",
    "# i.e. a list of tuples for all word, previous word combinations\n",
    "#\n",
    "# FIND MAX VALUE PER SENTENCE/WORD PAIR\n",
    "#\n",
    "# >>> COMBINE RESULTS OF RDD1 AND RDD2\n",
    "# >>> CHECK HOW MANY SUGGESTIONS != ORIGINAL WORD\n",
    "#\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
