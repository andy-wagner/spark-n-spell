{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Level Correction -  Serial Version and SPARK Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context_correction.ipynb\n",
    "\n",
    "######################\n",
    "#\n",
    "# Submission by Gioia Dominedo (Harvard ID: 40966234) for\n",
    "# CS 205 - Computing Foundations for Computational Science\n",
    "# \n",
    "# This is part of a joint project with Kendrick Lo that includes a\n",
    "# separate component for word-level checking. This notebook outlines\n",
    "# algorithms for context-level correction, and includes a serial\n",
    "# Python algorithm adapted from third party algorithms (Symspell and\n",
    "# Viterbi algorithms), as well as a Spark/Python algorithm. \n",
    "#\n",
    "# The following were also used as references:\n",
    "# Peter Norvig, How to Write a Spelling Corrector\n",
    "#\t(http://norvig.com/spell-correct.html)\n",
    "# Peter Norvig, Natural Language Corpus Data: Beautiful Data\n",
    "#\t(http://norvig.com/ngrams/ch14.pdf)\n",
    "#\n",
    "# Two main approaches to parallelization were attempted: sentence-\n",
    "# level and word-level. Both attempts are documented in this notebook.\n",
    "#\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# CONTEXT-LEVEL CORRECTION LOGIC - VITERBI ALGORITHM\n",
    "#\n",
    "# Each sentence is modeled as a hidden Markov model. Prior\n",
    "# probabilities (for first words in the sentences) and transition\n",
    "# probabilities (for all subsequent words) are calculated when\n",
    "# generating the main dictionary, using the same corpus. Emission\n",
    "# probabilities are generated on the fly by parameterizing a Poisson \n",
    "# distribution with the edit distance. The state space of possible\n",
    "# corrections is based on the suggested words from the word-level\n",
    "# correction. Words must (a) be 'real' word (i.e. appear at least\n",
    "# once in the corpus) used to generate the dictionary in order to be\n",
    "# considered valid suggestions; this ensures that the state space\n",
    "# remains manageable by ignoring words that have not been seen\n",
    "# previously.\n",
    "#\n",
    "# All probabilities are stored in log-space to avoid underflow. Pre-\n",
    "# defined minimum values are used for words that are not present in\n",
    "# the dictionary and/or probability tables.\n",
    "#\n",
    "# More detail is included at each step below.\n",
    "#\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>To run the serial version, restart notebook, and start executing the cells of this section starting here.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Serial Code Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>PRE-PROCESSING CODE</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# v 1.0 last revised 27 Nov 2015\n",
    "#\n",
    "# The pre-processing steps have been adapted from the dictionary\n",
    "# creation of the word-level spellchecker, which in turn was based on\n",
    "# SymSpell, a Symmetric Delete spelling correction algorithm\n",
    "# developed by Wolf Garbe and originally written in C#. More detail\n",
    "# on SymSpell is included in the word-level spell-check documentation.\n",
    "#\n",
    "# The main modifications to the word-level spellchecker pre-\n",
    "# processing stages are to create the additional outputs that are\n",
    "# required for the context-level checking, and to eliminate redundant\n",
    "# outputs that are not necessary.\n",
    "#\n",
    "# The outputs of the pre-processing stage are:\n",
    "#\n",
    "# - dictionary: A dictionary that combines both words present in the\n",
    "# corpus and other words that are within a given 'delete distance'. \n",
    "# The format of the dictionary is:\n",
    "# {word: ([list of words within the given 'delete distance'], \n",
    "# word count in corpus)}\n",
    "#\n",
    "# - start_prob: A dictionary with key, value pairs that correspond to\n",
    "# (word, probability of the word being the first word in a sentence)\n",
    "#\n",
    "# - transition_prob: A dictionary of dictionaries that stores the\n",
    "# probability of a given word following another. The format of the\n",
    "# dictionary is:\n",
    "# {previous word: {word1 : P(word1|prevous word), word2 : \n",
    "# P(word2|prevous word), ...}}\n",
    "#\n",
    "# - default_start_prob: A benchmark probability of a word being at\n",
    "# the start of a sentence, set to 1 / # of words at the beginning of\n",
    "# sentences. This ensures that all previously unseen words at the\n",
    "# beginning of sentences are not corrected unnecessarily.\n",
    "#\n",
    "# - default_transition_prob: A benchmark probability of a word being\n",
    "# seen, given the previous word in the sentence, also set to 1 / # of\n",
    "# transitions in corpus. This ensures that all previously unseen\n",
    "# transitions are not corrected unnecessarily.\n",
    "#\n",
    "######################\n",
    "\n",
    "MAX_EDIT_DISTANCE = 3\n",
    "\n",
    "def get_deletes_list(w):\n",
    "    '''\n",
    "    Given a word, derive strings with up to max_edit_distance\n",
    "    characters deleted.\n",
    "    '''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(MAX_EDIT_DISTANCE):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes\n",
    "\n",
    "def create_dictionary_entry(w, dictionary):\n",
    "    '''\n",
    "    Adds a word and its derived deletions to the dictionary.\n",
    "    Dictionary entries are of the form:\n",
    "    (list of suggested corrections, frequency of word in corpus)\n",
    "    '''\n",
    "\n",
    "    new_real_word_added = False\n",
    "    \n",
    "    # check if word is already in dictionary\n",
    "    if w in dictionary:\n",
    "        # increment count of word in corpus\n",
    "        dictionary[w] = (dictionary[w][0], dictionary[w][1] + 1)\n",
    "    else:\n",
    "        # create new entry in dictionary\n",
    "        dictionary[w] = ([], 1)  \n",
    "        \n",
    "    if dictionary[w][1]==1:\n",
    "        \n",
    "        # first appearance of word in corpus\n",
    "        # n.b. word may already be in dictionary as a derived word\n",
    "        # (deleting character from a real word)\n",
    "        # but counter of frequency of word in corpus is not\n",
    "        # incremented in those cases)\n",
    "        \n",
    "        new_real_word_added = True\n",
    "        deletes = get_deletes_list(w)\n",
    "        \n",
    "        for item in deletes:\n",
    "            if item in dictionary:\n",
    "                # add (correct) word to delete's suggested correction\n",
    "                # list if not already there\n",
    "                if item not in dictionary[item][0]:\n",
    "                    dictionary[item][0].append(w)\n",
    "            else:\n",
    "                # note: frequency of word in corpus is not incremented\n",
    "                dictionary[item] = ([w], 0)  \n",
    "        \n",
    "    return new_real_word_added\n",
    "\n",
    "def create_dictionary(fname):\n",
    "    '''\n",
    "    Loads a text file and uses it to create a dictionary and\n",
    "    to calculate start probabilities and transition probabilities. \n",
    "    Please refer to the text above for a full description.\n",
    "    '''\n",
    "    \n",
    "    print 'Creating dictionary...'\n",
    "\n",
    "    dictionary = dict()\n",
    "    start_prob = dict()\n",
    "    transition_prob = dict()\n",
    "    word_count = 0\n",
    "    transitions = 0\n",
    "    \n",
    "    with open(fname) as file:    \n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            # process each sentence separately\n",
    "            for sentence in line.split('.'):\n",
    "                \n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', sentence.lower())      \n",
    "                \n",
    "                for w, word in enumerate(words):\n",
    "                    \n",
    "                    if create_dictionary_entry(word, dictionary):\n",
    "                        word_count += 1\n",
    "                        \n",
    "                    # update probabilities for Hidden Markov Model\n",
    "                    if w == 0:\n",
    "\n",
    "                        # probability of a word being at the\n",
    "                        # beginning of a sentence\n",
    "                        if word in start_prob:\n",
    "                            start_prob[word] += 1\n",
    "                        else:\n",
    "                            start_prob[word] = 1\n",
    "                    else:\n",
    "                        \n",
    "                        # probability of transitionining from one\n",
    "                        # word to another\n",
    "                        # dictionary format:\n",
    "                        # {previous word: {word1 : P(word1|prevous\n",
    "                        # word), word2 : P(word2|prevous word)}}\n",
    "                        \n",
    "                        # check that prior word is present\n",
    "                        # - create if not\n",
    "                        if words[w - 1] not in transition_prob:\n",
    "                            transition_prob[words[w - 1]] = dict()\n",
    "                            \n",
    "                        # check that current word is present\n",
    "                        # - create if not\n",
    "                        if word not in transition_prob[words[w - 1]]:\n",
    "                            transition_prob[words[w - 1]][word] = 0\n",
    "                            \n",
    "                        # update value\n",
    "                        transition_prob[words[w - 1]][word] += 1\n",
    "                        transitions += 1\n",
    "                    \n",
    "    # convert counts to log-probabilities, to avoid underflow in\n",
    "    # later calculations (note: natural logarithm, not base-10)\n",
    "    \n",
    "    total_start_words = float(sum(start_prob.values()))\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    start_prob.update( \n",
    "        {k: math.log(v/total_start_words)\n",
    "         for k, v in start_prob.items()})\n",
    "    \n",
    "    default_transition_prob = math.log(1./transitions)\n",
    "    transition_prob.update(\n",
    "        {k: {k1: math.log(float(v1)/sum(v.values()))\n",
    "             for k1, v1 in v.items()} \n",
    "         for k, v in transition_prob.items()})\n",
    "\n",
    "    print 'Total unique words in corpus: %i' % word_count\n",
    "    print 'Total items in dictionary: %i' \\\n",
    "        % len(dictionary)\n",
    "    print '  Edit distance for deletions: %i' % MAX_EDIT_DISTANCE\n",
    "    print 'Total unique words at the start of a sentence: %i' \\\n",
    "        % len(start_prob)\n",
    "    print 'Total unique word transitions: %i' % len(transition_prob)\n",
    "        \n",
    "    return dictionary, start_prob, default_start_prob, \\\n",
    "        transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>SPELL-CHECKING CODE</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# v 1.0 last revised 27 Nov 2015\n",
    "#\n",
    "# Reads in a text file, breaks it down into individual sentences (by\n",
    "# splitting on periods), and then carries out context-based spell-\n",
    "# checking on each sentence in turn. In cases where the 'suggested'\n",
    "# word does not match the actual word in the text, both the original\n",
    "# and the suggested sentences are printed.\n",
    "#\n",
    "# Probabilistic model:\n",
    "#\n",
    "# Each sentence is modeled as a hidden Markov model, where the\n",
    "# hidden states are the words that the user intended to type, and\n",
    "# the emissions are the words\n",
    "# that were actually typed.\n",
    "#\n",
    "# For each word in a sentence, we can define:\n",
    "# - emission probabilities: P(observed word|intended word)\n",
    "# - prior probabilities (for first words in sentences only):\n",
    "# P(being the first word in a sentence)\n",
    "# - transition probabilities (for all subsequent words):\n",
    "# P(intended word|previous intended word)\n",
    "#\n",
    "# Prior and transition probabilities were calculated in the pre-\n",
    "# processing step, using the same corpus as the dictionary.\n",
    "# \n",
    "# Emission probabilities are calculated on the fly using a Poisson\n",
    "# distribution, where P(observed word|intended word) = PMF of \n",
    "# Poisson(k, l), where k = edit distance between word type and word\n",
    "# intended, and l=0.01. This approach was taken from the 2015\n",
    "# lecture notes of AM207 Stochastic Optimization, as was the\n",
    "# parameter l=0.01. Various parameters between 0 and 1 were tested,\n",
    "# confirming that 0.01 yields the most accurate word suggestions.\n",
    "# The shape of the PMF is shown in the cell below.\n",
    "#\n",
    "# All probabilities are stored in log-space to avoid underflow. Pre-\n",
    "# defined minimum values (also defined at the pre-processing stage)\n",
    "# are used for words that are not present in the dictionary and/or\n",
    "# probability tables.\n",
    "#\n",
    "# The spell-checking itself is carried out using a modified version\n",
    "# of the Viterbi algorithm, which yields the most likely sequence of\n",
    "# hidden states, i.e. the most likely sequence of words that the\n",
    "# user intended to type. The main difference to the 'standard'\n",
    "# Viterbi algorithm is that the state space (i.e. list of possible\n",
    "# corrections) is generated (and therefore varies) for each word,\n",
    "# instead of considering the state space of all possible words in\n",
    "# the dictionary for every word that is checked. This ensures that\n",
    "# the problem remains tractable.\n",
    "#\n",
    "# The algorithm is best illustrated by way of an example.\n",
    "# e.g. suppose that we are checking the sentence 'This is ax test.'\n",
    "# The emissions are 'This is ax test.' and the hidden states are\n",
    "# 'This is a test.'\n",
    "#\n",
    "# As a pre-processing step, we convert everything to lowercase,\n",
    "# eliminate punctuation, and break the sentence up into a list of\n",
    "# words: ['this', 'is', 'ax', 'text']\n",
    "# This list is passed as a parameter to the viterbi function.\n",
    "#\n",
    "# The algorithm tackles each word in turn, starting with 'this'.\n",
    "#\n",
    "# We first use get_suggestions to obtain a list of all words that\n",
    "# may have been intended, i.e. all possible hidden states (intended\n",
    "# words) for the emission (word typed).\n",
    "# get_suggestions returns 1,004 possible corrections, including:\n",
    "# - 1 word with an edit distance of 0 ['this']\n",
    "# - 5 words with an edit distance of 1 ['his', 'thus', 'thin',\n",
    "# 'tis', 'thins']\n",
    "# - 109 words with an edit distance of 2 ['the', 'that', 'is',\n",
    "# 'him', 'they', ...]\n",
    "# - 889 words with an edit distance of 3 ['to', 'in', 'he', 'was',\n",
    "# 'it', 'as', ...]\n",
    "# Note: get_suggestions is capped at an edit distance of 3.\n",
    "# \n",
    "# These 1,004 words represent our state space, i.e. all possible\n",
    "# words that may have been intended. They each have an emission\n",
    "# probability = PMF of Poisson(edit distance, 0.01)\n",
    "# We refer to this below as the list of possible corrections.\n",
    "#\n",
    "# For each word in the list of possible corrections, we calculate:\n",
    "# P(word starting a sentence) * P(observed 'this'|intended word)\n",
    "# This is a simple application of Bayes' rule: by normalizing the\n",
    "# probabilities we obtain P(intended word|oberved 'this') for\n",
    "# each of the 1,004 words.\n",
    "# These probabilities are referred to as the belief state, and they\n",
    "# are stored for\n",
    "# future use. We also store the possible paths, which at this stage\n",
    "# are only one word long.\n",
    "# \n",
    "# We now move on to the next word. After the first word, all\n",
    "# subsequent words are\n",
    "# treated as follows.\n",
    "#\n",
    "# The second word in our test sentence is 'is'. Once again, we use\n",
    "# get_suggestions to obtain a list of all words that may have been\n",
    "# intended. get_suggestions returns 1,124 possible corrections,\n",
    "# including:\n",
    "# - 1 word with an edit distance of 0 ['is']\n",
    "# - 31 words with an edit distance of 1 ['in', 'it', 'his', 'as',\n",
    "# 'i', ...]\n",
    "# - 213 words with an edit distance of 2 ['was', 'him', 'this',\n",
    "# 'so', 'did', ...]\n",
    "# - 879 words with an edit distance of 3 ['with', 'she', 'said',\n",
    "# 'into', ...]\n",
    "# These 1,124 words represent our state space for the second word.\n",
    "#\n",
    "# For each word in the list of possible corrections, we loop through\n",
    "# all the words in the previous list of possible corrections, and\n",
    "# calculate:\n",
    "# belief state(previous word) * P(current word|previous word)\n",
    "#    * P(typing 'is'|meaning to type current word)\n",
    "# We store the previous word that maximizes this calculation, as\n",
    "# well as the probability that it results in.\n",
    "#\n",
    "# For example, suppose that we are considering the possibility that\n",
    "# 'is' was indeed intended to be 'is'. We then calculate: \n",
    "# belief state(previous word) * P('is'|previous word) * P('is'|'is')\n",
    "# for all possible previous words, and discover that the previous\n",
    "# word 'this' maximizes the above calculation. We therefore store\n",
    "# 'this is' as the optimal path for the suggested correction 'is'\n",
    "# (more specifically, path['is'] = 'this is'), and the above\n",
    "# (normalized) probability as the belief state for 'is' (more\n",
    "# specifically, belief['is'] = prob('this is')).\n",
    "#\n",
    "# If the sentence had been only 2 words long, then at this point we\n",
    "# would return the path that maximizes the most recent belief state.\n",
    "# As it is not, we repeat the previous steps for 'ax' and 'test',\n",
    "# and then return the path that is associated with the most likely\n",
    "# belief state at the last step.\n",
    "#\n",
    "######################\n",
    "\n",
    "\n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, longest_word_length=20, \n",
    "                    min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, \n",
    "                        transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "\n",
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)  \n",
    "    \n",
    "def viterbi(words, dictionary, start_prob, default_start_prob, \n",
    "            transition_prob, default_transition_prob):\n",
    "    \n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    path_context = []\n",
    "    \n",
    "    # character level correction - used to determine state space\n",
    "    corrections = get_suggestions(words[0], dictionary)\n",
    "        \n",
    "    # Initialize base cases (t == 0)\n",
    "    for sug_word in corrections:\n",
    "        \n",
    "        # compute the value for all possible starting states\n",
    "        V[0][sug_word[0]] = math.exp(\n",
    "            get_start_prob(sug_word[0], start_prob, \n",
    "                           default_start_prob)\n",
    "            + get_emission_prob(sug_word[1]))\n",
    "        \n",
    "        # remember all the different paths (only one state so far)\n",
    "        path[sug_word[0]] = [sug_word[0]]\n",
    " \n",
    "    # normalize for numerical stability\n",
    "    path_temp_sum = sum(V[0].values())\n",
    "    V[0].update({k: math.log(v/path_temp_sum) \n",
    "                 for k, v in V[0].items()})\n",
    "    \n",
    "    # keep track of previous state space\n",
    "    prev_corrections = [i[0] for i in corrections]\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        path_context = [max(V[0], key=lambda i: V[0][i])]\n",
    "        return path_context\n",
    "\n",
    "    # run Viterbi for t > 0\n",
    "    for t in range(1, len(words)):\n",
    "\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        \n",
    "        # character level correction\n",
    "        corrections = get_suggestions(words[t], dictionary)\n",
    " \n",
    "        for sug_word in corrections:\n",
    "        \n",
    "            sug_word_emission_prob = get_emission_prob(sug_word[1])\n",
    "            \n",
    "            # compute the values coming from all possible previous\n",
    "            # states, only keep the maximum\n",
    "            (prob, word) = max(\n",
    "                (get_belief(prev_word, V[t-1]) \n",
    "                + get_transition_prob(sug_word[0], prev_word, \n",
    "                    transition_prob, default_transition_prob)\n",
    "                + sug_word_emission_prob, prev_word) \n",
    "                               for prev_word in prev_corrections)\n",
    "\n",
    "            # save the maximum value for each state\n",
    "            V[t][sug_word[0]] = math.exp(prob)\n",
    "            \n",
    "            # remember the path we came from to get this maximum value\n",
    "            new_path[sug_word[0]] = path[word] + [sug_word[0]]\n",
    "        \n",
    "        # normalize for numerical stability\n",
    "        path_temp_sum = sum(V[t].values())\n",
    "        V[t].update({k: math.log(v/path_temp_sum) \n",
    "                     for k, v in V[t].items()})\n",
    "        \n",
    "        # keep track of previous state space\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "        # don't need to remember the old paths\n",
    "        path = new_path\n",
    "     \n",
    "    (prob, word) = max((V[t][sug_word[0]], sug_word[0]) \n",
    "                       for sug_word in corrections)\n",
    "    path_context = path[word]\n",
    "    \n",
    "    return path_context\n",
    "\n",
    "def correct_document_context(fname, dictionary, start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob):\n",
    "    \n",
    "    doc_word_count = 0\n",
    "    corrected_word_count = 0\n",
    "    \n",
    "    with open(fname) as file:\n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            \n",
    "            for sentence in line.split('.'):\n",
    "                \n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', sentence.lower())  \n",
    "                doc_word_count += len(words)\n",
    "                \n",
    "                if len(words) > 0:\n",
    "                \n",
    "                    suggestion = viterbi(words, dictionary,\n",
    "                                start_prob, default_start_prob, \n",
    "                                transition_prob, default_transition_prob)\n",
    "\n",
    "                    # display sentences with corrections\n",
    "                    if words != suggestion:\n",
    "                        \n",
    "                        print 'Line %i: %s --> %s' % \\\n",
    "                        (i, ' '.join(words), ' '.join(suggestion))\n",
    "                        \n",
    "                        # update count of corrected words\n",
    "                        corrected_word_count += \\\n",
    "                        sum([words[j]!=suggestion[j] \n",
    "                             for j in range(len(words))])\n",
    "  \n",
    "    print '-----'\n",
    "    print 'Total words checked: %i' % doc_word_count\n",
    "    print 'Total potential errors found: %i' % corrected_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>SAMPLE OUTPUTS</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary: 2151998\n",
      "  Edit distance for deletions: 3\n",
      "Total unique words at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n",
      "CPU times: user 40.6 s, sys: 738 ms, total: 41.3 s\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, start_prob, default_start_prob, \\\n",
    "transition_prob, default_transition_prob \\\n",
    "= create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 3: this is ax test --> this is a test\n",
      "Line 4: this is za test --> this is a test\n",
      "Line 5: thee is a test --> there is a test\n",
      "Line 6: her tee set --> her to set\n",
      "-----\n",
      "Total words checked: 27\n",
      "Total potential errors found: 4\n",
      "CPU times: user 936 ms, sys: 11.6 ms, total: 948 ms\n",
      "Wall time: 951 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context('testdata/test.txt', dictionary,\n",
    "                         start_prob, default_start_prob, \n",
    "                         transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>To run the SPARK version, restart notebook, and start executing the cells of this section starting here.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Pre-Processing SPARK Code Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# DOCUMENTATION HERE\n",
    "#\n",
    "######################\n",
    "\n",
    "# number of partitions to be used\n",
    "n_partitions = 6\n",
    "MAX_EDIT_DISTANCE = 3\n",
    "\n",
    "def get_n_deletes_list(w, n):\n",
    "    '''given a word, derive list of strings with up to n characters deleted'''\n",
    "    # since this list is generally of the same magnitude as the number of \n",
    "    # characters in a word, it may not make sense to parallelize this\n",
    "    # so we use python to create the list\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(n):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes\n",
    "\n",
    "def get_transitions(sentence):\n",
    "    if len(sentence)<2:\n",
    "        return None\n",
    "    else:\n",
    "        return [((sentence[i], sentence[i+1]), 1) for i in range(len(sentence)-1)]\n",
    "    \n",
    "def map_transition_prob(x):\n",
    "    vals = x[1]\n",
    "    total = float(sum(vals.values()))\n",
    "    probs = {k: math.log(v/total) for k, v in vals.items()}\n",
    "    return (x[0], probs)\n",
    "\n",
    "def parallel_create_dictionary(fname):\n",
    "    '''\n",
    "    Create dictionary, start probabilities and transition\n",
    "    probabilities using Spark RDDs.\n",
    "    '''\n",
    "    # we generate and count all words for the corpus,\n",
    "    # then add deletes to the dictionary\n",
    "    # this is a slightly different approach from the SymSpell algorithm\n",
    "    # that may be more appropriate for Spark processing\n",
    "    \n",
    "    print 'Creating dictionary...'\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "\n",
    "    # convert file into one long sequence of words\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "            .map(lambda line: line.lower()) \\\n",
    "            .filter(lambda x: x!='').cache()\n",
    "    \n",
    "    # split into individual sentences and remove other punctuation\n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate start probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # only focus on words at the start of sentences\n",
    "    start_words = split_sentence.map(lambda sentence: sentence[0] if len(sentence)>0 else None) \\\n",
    "        .filter(lambda word: word!=None)\n",
    "    \n",
    "    # add a count to each word\n",
    "    count_start_words_once = start_words.map(lambda word: (word, 1)).cache()\n",
    "\n",
    "    # use accumulator to count the number of words at the start of sentences\n",
    "    accum_total_start_words = sc.accumulator(0)\n",
    "    count_total_start_words = count_start_words_once.foreach(lambda x: accum_total_start_words.add(1))\n",
    "    total_start_words = float(accum_total_start_words.value)\n",
    "    \n",
    "    # reduce into count of unique words at the start of sentences\n",
    "    unique_start_words = count_start_words_once.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # convert counts to probabilities\n",
    "    start_prob_calc = unique_start_words.mapValues(lambda v: math.log(v/total_start_words))\n",
    "    \n",
    "    # get default start probabilities (for words not in corpus)\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    \n",
    "    # store start probabilities as a dictionary (will be used as a lookup table)\n",
    "    start_prob = start_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate transition probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # focus on continuous word pairs within the sentence\n",
    "    # e.g. \"this is a test\" -> \"this is\", \"is a\", \"a test\"\n",
    "    # note: as the relevant probability is P(word|previous word)\n",
    "    # the tuples are ordered as (previous word, word)\n",
    "    other_words = split_sentence.map(lambda sentence: get_transitions(sentence)).filter(lambda x: x!=None). \\\n",
    "                flatMap(lambda x: x).cache()\n",
    "\n",
    "    # use accumulator to count the number of transitions\n",
    "    accum_total_other_words = sc.accumulator(0)\n",
    "    count_total_other_words = other_words.foreach(lambda x: accum_total_other_words.add(1))\n",
    "    total_other_words = float(accum_total_other_words.value)\n",
    "    \n",
    "    # reduce into count of unique word pairs\n",
    "    unique_other_words = other_words.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # aggregate by previous word\n",
    "    # i.e. (previous word, [(word1, word1-previous word count), (word2, word2-previous word count), ...])\n",
    "    other_words_collapsed = unique_other_words.map(lambda x: (x[0][0], (x[0][1], x[1]))).groupByKey().mapValues(dict)\n",
    "\n",
    "    # POTENTIAL OPTIMIZATION: FIND AN ALTERNATIVE TO GROUPBYKEY (CREATES ~9.3MB SHUFFLE)\n",
    "    \n",
    "    # convert counts to probabilities\n",
    "    transition_prob_calc = other_words_collapsed.map(lambda x: map_transition_prob(x))\n",
    "    \n",
    "    # get default transition probabilities (for word pairs not in corpus)\n",
    "    default_transition_prob = math.log(1/total_other_words)\n",
    "    \n",
    "    # store transition probabilities as dictionary (will be used as lookup table)\n",
    "    transition_prob = transition_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # process corpus for dictionary\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    replace_nonalphs = make_all_lower.map(lambda line: regex.sub(' ', line))\n",
    "    all_words = replace_nonalphs.flatMap(lambda line: line.split())\n",
    "\n",
    "    # create core corpus dictionary (i.e. only words appearing in file, no \"deletes\") and cache it\n",
    "    # output RDD of unique_words_with_count: [(word1, count1), (word2, count2), (word3, count3)...]\n",
    "    count_once = all_words.map(lambda word: (word, 1))\n",
    "    unique_words_with_count = count_once.reduceByKey(lambda a, b: a + b).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate deletes list\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # generate list of n-deletes from words in a corpus of the form: [(word1, count1), (word2, count2), ...]\n",
    "     \n",
    "    assert MAX_EDIT_DISTANCE > 0  \n",
    "    \n",
    "    generate_deletes = unique_words_with_count.map(lambda (parent, count): \n",
    "                                                   (parent, get_n_deletes_list(parent, MAX_EDIT_DISTANCE)))\n",
    "    expand_deletes = generate_deletes.flatMapValues(lambda x: x)\n",
    "    swap = expand_deletes.map(lambda (orig, delete): (delete, ([orig], 0)))\n",
    "   \n",
    "    ############\n",
    "    #\n",
    "    # combine delete elements with main dictionary\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    corpus = unique_words_with_count.mapValues(lambda count: ([], count))\n",
    "    combine = swap.union(corpus)  # combine deletes with main dictionary, eliminate duplicates\n",
    "    \n",
    "    # since the dictionary will only be a lookup table once created, we can\n",
    "    # pass on as a Python dictionary rather than RDD by reducing locally and\n",
    "    # avoiding an extra shuffle from reduceByKey\n",
    "    dictionary = combine.reduceByKeyLocally(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "\n",
    "    words_processed = unique_words_with_count.map(lambda (k, v): v).reduce(lambda a, b: a + b)\n",
    "    word_count = unique_words_with_count.count()   \n",
    "    \n",
    "    # output stats\n",
    "    print 'Total words processed: %i' % words_processed\n",
    "    print 'Total unique words in corpus: %i' % word_count \n",
    "    print 'Total items in dictionary (corpus words and deletions): %i' % len(dictionary)\n",
    "    print '  Edit distance for deletions: %i' % MAX_EDIT_DISTANCE\n",
    "    print 'Total unique words at the start of a sentence: %i' \\\n",
    "        % len(start_prob)\n",
    "    print 'Total unique word transitions: %i' % len(transition_prob)\n",
    "    \n",
    "    return dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3. Spellchecking SPARK Code Performance - Parallelizing Across Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# DOCUMENTATION HERE\n",
    "#\n",
    "######################\n",
    "\n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "    \n",
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)\n",
    "    \n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # this block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, longest_word_length=20, \n",
    "                    min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def viterbi(words, dictionary, start_prob, default_start_prob, \n",
    "            transition_prob, default_transition_prob):\n",
    "    \n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    path_context = []\n",
    "    \n",
    "    # character level correction - used to determine state space\n",
    "    corrections = get_suggestions(words[0], dictionary)\n",
    "        \n",
    "    # Initialize base cases (t == 0)\n",
    "    for sug_word in corrections:\n",
    "        \n",
    "        # compute the value for all possible starting states\n",
    "        V[0][sug_word[0]] = math.exp(\n",
    "            get_start_prob(sug_word[0], start_prob, \n",
    "                           default_start_prob)\n",
    "            + get_emission_prob(sug_word[1]))\n",
    "        \n",
    "        # remember all the different paths (only one state so far)\n",
    "        path[sug_word[0]] = [sug_word[0]]\n",
    " \n",
    "    # normalize for numerical stability\n",
    "    path_temp_sum = sum(V[0].values())\n",
    "    V[0].update({k: math.log(v/path_temp_sum) \n",
    "                 for k, v in V[0].items()})\n",
    "    \n",
    "    # keep track of previous state space\n",
    "    prev_corrections = [i[0] for i in corrections]\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        path_context = [max(V[0], key=lambda i: V[0][i])]\n",
    "        return path_context\n",
    "\n",
    "    # run Viterbi for t > 0\n",
    "    for t in range(1, len(words)):\n",
    "\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        \n",
    "        # character level correction\n",
    "        corrections = get_suggestions(words[t], dictionary)\n",
    " \n",
    "        for sug_word in corrections:\n",
    "        \n",
    "            sug_word_emission_prob = get_emission_prob(sug_word[1])\n",
    "            \n",
    "            # compute the values coming from all possible previous\n",
    "            # states, only keep the maximum\n",
    "            (prob, word) = max(\n",
    "                (get_belief(prev_word, V[t-1]) \n",
    "                + get_transition_prob(sug_word[0], prev_word, \n",
    "                    transition_prob, default_transition_prob)\n",
    "                + sug_word_emission_prob, prev_word) \n",
    "                               for prev_word in prev_corrections)\n",
    "\n",
    "            # save the maximum value for each state\n",
    "            V[t][sug_word[0]] = math.exp(prob)\n",
    "            \n",
    "            # remember the path we came from to get this maximum value\n",
    "            new_path[sug_word[0]] = path[word] + [sug_word[0]]\n",
    "            \n",
    "        # normalize for numerical stability\n",
    "        path_temp_sum = sum(V[t].values())\n",
    "        V[t].update({k: math.log(v/path_temp_sum) \n",
    "                     for k, v in V[t].items()})\n",
    "        \n",
    "        # keep track of previous state space\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "        # don't need to remember the old paths\n",
    "        path = new_path\n",
    "     \n",
    "    (prob, word) = max((V[t][sug_word[0]], sug_word[0]) \n",
    "                       for sug_word in corrections)\n",
    "    path_context = path[word]\n",
    "    \n",
    "    return path_context\n",
    "\n",
    "def get_count_mismatches(sentences):\n",
    "    orig_sentence, sug_sentence = sentences\n",
    "    count_mismatches = len([(orig_sentence[i], sug_sentence[i]) for i in range(len(orig_sentence))\n",
    "            if orig_sentence[i]!=sug_sentence[i]])\n",
    "    return count_mismatches, orig_sentence, sug_sentence\n",
    "\n",
    "def correct_document_context_parallel_sentences(fname, dictionary,\n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob):\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # broadcast Python dictionaries to workers\n",
    "    bc_dictionary = sc.broadcast(dictionary)\n",
    "    bc_start_prob = sc.broadcast(start_prob)\n",
    "    bc_transition_prob = sc.broadcast(transition_prob)\n",
    "    \n",
    "    # convert all text to lowercase and drop empty lines\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "        .map(lambda line: line.lower()) \\\n",
    "        .filter(lambda x: x!='')\n",
    "    \n",
    "    regex = re.compile('[^a-z ]')\n",
    "    \n",
    "    # split into sentences -> remove special characters -> convert into list of words\n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()\n",
    "    \n",
    "    # use accumulator to count the number of words checked\n",
    "    accum_total_words = sc.accumulator(0)\n",
    "    split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "    \n",
    "    # assign each sentence a unique id\n",
    "    sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k)).cache()\n",
    "\n",
    "    ############\n",
    "    #\n",
    "    # spell-checking\n",
    "    #\n",
    "    ############\n",
    "\n",
    "    # apply Viterbi algorithm to each sentence\n",
    "    sentence_correction = sentence_id.mapValues(lambda v: (v, \n",
    "                viterbi(v, bc_dictionary.value, bc_start_prob.value, \n",
    "                        default_start_prob, bc_transition_prob.value, default_transition_prob)))\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # output results\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # count the number of errors per sentence, drop any sentences without errors\n",
    "    sentence_errors = sentence_correction.mapValues(lambda v: (get_count_mismatches(v))). \\\n",
    "            filter(lambda (k, v): v[0]>0).cache()               \n",
    "    \n",
    "    # collect all sentences with identified errors\n",
    "    sentence_errors_list = sentence_errors.collect()\n",
    "    \n",
    "    # number of potentially misspelled words\n",
    "    num_errors = sum([s[1][0] for s in sentence_errors_list])\n",
    "    \n",
    "    # print identified errors (eventually output to file)\n",
    "    for sentence in sentence_errors_list:\n",
    "        print 'Line %i: %s --> %s' % (sentence[0], ' '.join(sentence[1][1]), ' '.join(sentence[1][2]))\n",
    "    \n",
    "    print '-----'\n",
    "    print 'Total words checked: %i' % accum_total_words.value\n",
    "    print 'Total potential errors found: %i' % num_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>SAMPLE OUTPUTS</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Total words processed: 1105285\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary (corpus words and deletions): 2151998\n",
      "  Edit distance for deletions: 3\n",
      "Total unique words at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n",
      "CPU times: user 12.8 s, sys: 1.45 s, total: 14.3 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "    parallel_create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 3: this is ax test --> this is a test\n",
      "Line 4: this is za test --> this is a test\n",
      "Line 5: thee is a test --> there is a test\n",
      "Line 6: her tee set --> her to set\n",
      "-----\n",
      "Total words checked: 27\n",
      "Total potential errors found: 4\n",
      "CPU times: user 7.44 s, sys: 527 ms, total: 7.97 s\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context_parallel_sentences('testdata/test.txt', dictionary,\n",
    "        start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4. Spellchecking SPARK Code Performance - Parallelizing Across Possible Word Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# DOCUMENTATION HERE\n",
    "#\n",
    "######################\n",
    "\n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, longest_word_length=20, \n",
    "                    min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "\n",
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)  \n",
    "\n",
    "def map_sentence_words(sentence, tmp_dict):\n",
    "    return [[word, get_suggestions(word, tmp_dict)] \n",
    "            for i, word in enumerate(sentence)]\n",
    "\n",
    "def split_suggestions(sentence):\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        result.append([(word[0], s[0], get_emission_prob(s[1])) for s in word[1]])\n",
    "    return result\n",
    "\n",
    "def get_word_combos(sug_lists):\n",
    "    return list(itertools.product(*sug_lists))\n",
    "\n",
    "def split_combos(combos):\n",
    "    sent_id, combo_list = combos\n",
    "    return [[sent_id, c] for c in combo_list]\n",
    "\n",
    "def get_combo_prob(combo, tmp_sp, d_sp, tmp_tp, d_tp):\n",
    "    \n",
    "    # first word in sentence\n",
    "    # emission prob * start prob\n",
    "    orig_path = [combo[0][0]]\n",
    "    sug_path = [combo[0][1]]\n",
    "    prob = combo[0][2] + get_start_prob(combo[0][1], tmp_sp, d_sp)\n",
    "    \n",
    "    # subsequent words\n",
    "    for i, w in enumerate(combo[1:]):\n",
    "        orig_path.append(w[0])\n",
    "        sug_path.append(w[1])\n",
    "        prob += w[2] + get_transition_prob(w[1], combo[i-1][1], tmp_tp, d_tp)\n",
    "    \n",
    "    return orig_path, sug_path, prob\n",
    "\n",
    "def get_count_mismatches_prob(sentences):\n",
    "    orig_sentence, sug_sentence, prob = sentences\n",
    "    count_mismatches = len([(orig_sentence[i], sug_sentence[i]) for i in range(len(orig_sentence))\n",
    "            if orig_sentence[i]!=sug_sentence[i]])\n",
    "    return count_mismatches, orig_sentence, sug_sentence\n",
    "\n",
    "def correct_document_context_parallel_combos(fname, dictionary,\n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob):\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # broadcast Python dictionaries to workers\n",
    "    bc_dictionary = sc.broadcast(dictionary)\n",
    "    bc_start_prob = sc.broadcast(start_prob)\n",
    "    bc_transition_prob = sc.broadcast(transition_prob)\n",
    "    \n",
    "    # convert all text to lowercase and drop empty lines\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "        .map(lambda line: line.lower()) \\\n",
    "        .filter(lambda x: x!='')\n",
    "    \n",
    "    regex = re.compile('[^a-z ]')\n",
    "    \n",
    "    # split into sentences -> remove special characters -> convert into list of words\n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()\n",
    "    \n",
    "    # use accumulator to count the number of words checked\n",
    "    accum_total_words = sc.accumulator(0)\n",
    "    split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "    \n",
    "    # assign each sentence a unique id\n",
    "    sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k)).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # spell-checking\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # look up possible suggestions for each word in each sentence\n",
    "    sentence_words = sentence_id.mapValues(lambda v: map_sentence_words(v, bc_dictionary.value))\n",
    "    \n",
    "    # look up emission probabilities for each word\n",
    "    # i.e. P(observed word|intended word)\n",
    "    sentence_word_sug = sentence_words.mapValues(lambda v: split_suggestions(v))\n",
    "    \n",
    "    # generate all possible corrected combinations (using Cartesian product)\n",
    "    # i.e. a sentence with 4 word, each of which have 5 possible suggestions,\n",
    "    # will yield 5^4 possible combinations\n",
    "    sentence_word_combos = sentence_word_sug.mapValues(lambda v: get_word_combos(v))\n",
    "    \n",
    "    # flatmap into all possible combinations per sentence\n",
    "    # format: [sentence id, \n",
    "    # [(observed first word, potential first word, P(observed first word|intended first word)]), \n",
    "    # (observed second word, potential second word, P(observed second word|intended second word)]), ...]\n",
    "    sentence_word_combos_split = sentence_word_combos.flatMap(lambda x: split_combos(x))\n",
    "    \n",
    "    # calculate the probability of each word combination being the intended one, given what was observed\n",
    "    # note: the approach does not allow for normalization across iterations, so may yield different results\n",
    "    sentence_word_combos_prob = sentence_word_combos_split.mapValues(lambda v:  \n",
    "                                get_combo_prob(v, bc_start_prob.value, default_start_prob, \n",
    "                                               bc_transition_prob.value, default_transition_prob))\n",
    "    \n",
    "    # identify the word combination with the highest probability for each sentence\n",
    "    sentence_max_prob = sentence_word_combos_prob.reduceByKey(lambda a,b: a if a[2] > b[2] else b)\n",
    "\n",
    "    ############\n",
    "    #\n",
    "    # output results\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # count the number of errors per sentence, drop any sentences without errors\n",
    "    sentence_errors = sentence_max_prob.mapValues(lambda v: (get_count_mismatches_prob(v))) \\\n",
    "            .filter(lambda (k, v): v[0]>0).cache()\n",
    "               \n",
    "    # collect all sentences with identified errors\n",
    "    sentence_errors_list = sentence_errors.collect()\n",
    "    \n",
    "    # number of potentially misspelled words\n",
    "    num_errors = sum([s[1][0] for s in sentence_errors_list])\n",
    "    \n",
    "    # print identified errors (eventually output to file)\n",
    "    for sentence in sentence_errors_list:\n",
    "        print 'Line %i: %s --> %s' % (sentence[0], ' '.join(sentence[1][1]), ' '.join(sentence[1][2]))\n",
    "    \n",
    "    print '-----'\n",
    "    print 'Total words checked: %i' % accum_total_words.value\n",
    "    print 'Total potential errors found: %i' % num_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>SAMPLE OUTPUTS</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Total words processed: 1105285\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary (corpus words and deletions): 2151998\n",
      "  Edit distance for deletions: 3\n",
      "Total unique words at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n",
      "CPU times: user 11.3 s, sys: 1.01 s, total: 12.3 s\n",
      "Wall time: 54.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "    parallel_create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 6: her tee set --> her the set\n",
      "Line 3: this is ax test --> this is as test\n",
      "Line 4: this is za test --> this is a test\n",
      "Line 5: thee is a test --> then is a test\n",
      "-----\n",
      "Total words checked: 27\n",
      "Total potential errors found: 4\n",
      "CPU times: user 9.14 s, sys: 660 ms, total: 9.8 s\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context_parallel_combos('testdata/test.txt', dictionary,\n",
    "        start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### 5. Spellchecking SPARK Code Performance - Parallelizing Across Viterbi Algorithm Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# DOCUMENTATION HERE\n",
    "#\n",
    "######################\n",
    "\n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, longest_word_length=20, \n",
    "                    min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "\n",
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)  \n",
    "\n",
    "def get_count_mismatches(sentences):\n",
    "    orig_sentence, sug_sentence = sentences\n",
    "    count_mismatches = len([(orig_sentence[i], sug_sentence[i]) for i in range(len(orig_sentence))\n",
    "            if orig_sentence[i]!=sug_sentence[i]])\n",
    "    return count_mismatches, orig_sentence, sug_sentence\n",
    "\n",
    "def get_sentence_word_id(words):\n",
    "    return [(i, w) for i, w in enumerate(words)]\n",
    "\n",
    "def split_sentence_words(sentence):\n",
    "    sent_id, words = sentence\n",
    "    return [[sent_id, w] for w in words]\n",
    "\n",
    "def start_word_prob(words, tmp_sp, d_sp):\n",
    "    orig_word, sug_words = words\n",
    "    probs = [(w[0], \n",
    "              math.exp(get_start_prob(w[0], tmp_sp, d_sp) + get_emission_prob(w[1]))\n",
    "             ) \n",
    "             for w in sug_words]\n",
    "    sum_probs = sum([p[1] for p in probs])\n",
    "    probs = [([p[0]], math.log(p[1]/sum_probs)) for p in probs]\n",
    "    return probs\n",
    "\n",
    "def split_suggestions(sentence):\n",
    "    sent_id, (word, word_sug)  = sentence\n",
    "    return [[sent_id, (word, w)] for w in word_sug]\n",
    "\n",
    "def subs_word_prob(words, tmp_tp, d_tp):\n",
    "    \n",
    "    # unpack values\n",
    "    sent_id = words[0]\n",
    "    cur_word = words[1][0][0]\n",
    "    cur_sug = words[1][0][1][0]\n",
    "    cur_sug_ed = words[1][0][1][1]\n",
    "    prev_sug = words[1][1]\n",
    "    \n",
    "    # belief + transition probability + emission probability\n",
    "    (prob, word) = max((p[1]\n",
    "                 + get_transition_prob(cur_sug, p[0][-1], tmp_tp, d_tp)\n",
    "                 + get_emission_prob(cur_sug_ed), p[0])\n",
    "                     for p in prev_sug)\n",
    "    \n",
    "    return sent_id, (word + [cur_sug], math.exp(prob))\n",
    "\n",
    "def normalize(probs):\n",
    "    sum_probs = sum([p[1] for p in probs])\n",
    "    return [(p[0], math.log(p[1]/sum_probs)) for p in probs]\n",
    "\n",
    "def get_max_path(final_paths):\n",
    "    max_path = max((p[1], p[0]) for p in final_paths)\n",
    "    return max_path[1]\n",
    "\n",
    "def correct_document_context_parallel_steps(fname, dictionary,\n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob):\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # broadcast Python dictionaries to workers\n",
    "    bc_dictionary = sc.broadcast(dictionary)\n",
    "    bc_start_prob = sc.broadcast(start_prob)\n",
    "    bc_transition_prob = sc.broadcast(transition_prob)\n",
    "    \n",
    "    # convert all text to lowercase and drop empty lines\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "        .map(lambda line: line.lower()) \\\n",
    "        .filter(lambda x: x!='')\n",
    "    \n",
    "    regex = re.compile('[^a-z ]')\n",
    "    \n",
    "    # split into sentences -> remove special characters -> convert into list of words\n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()\n",
    "    \n",
    "    # use accumulator to count the number of words checked\n",
    "    accum_total_words = sc.accumulator(0)\n",
    "    split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "    \n",
    "    # assign each sentence a unique id\n",
    "    sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k)).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # spell-checking\n",
    "    #\n",
    "    ############\n",
    "\n",
    "    # number each word in a sentence, and split into individual words\n",
    "    sentence_word_id = sentence_id.mapValues(lambda v: get_sentence_word_id(v)) \\\n",
    "            .flatMap(lambda x: split_sentence_words(x))\n",
    "    \n",
    "    # get suggestions for each word\n",
    "    sentence_word_suggestions = sentence_word_id.mapValues(lambda v: \n",
    "                                            (v[0], v[1], get_suggestions(v[1], bc_dictionary.value))).cache()\n",
    "    \n",
    "    # filter for the first words in sentences\n",
    "    sentence_word_1 = sentence_word_suggestions.filter(lambda (k, v): v[0]==0) \\\n",
    "            .mapValues(lambda v: (v[1], v[2]))\n",
    "    \n",
    "    # calculate probability for each suggestion\n",
    "    # format: (sentence id, [path-probability pairs])\n",
    "    sentence_path = sentence_word_1.mapValues(lambda v: \n",
    "                                              start_word_prob(v, bc_start_prob.value, default_start_prob))\n",
    "    \n",
    "    word_num = 1\n",
    "    \n",
    "    # filter for the next words in sentences\n",
    "    sentence_word_next = sentence_word_suggestions.filter(lambda (k,v): v[0]==word_num) \\\n",
    "            .mapValues(lambda v: (v[1], v[2]))\n",
    "    \n",
    "    # check that there are more words left\n",
    "    while not sentence_word_next.isEmpty():\n",
    "\n",
    "        # split into suggestions\n",
    "        sentence_word_next_split = sentence_word_next.flatMap(lambda x: split_suggestions(x))\n",
    "        \n",
    "        # join on previous path\n",
    "        # format: (sentence id, ((current word, (current word suggestion, edit distance)), \n",
    "        #         [(previous path-probability pairs)]))\n",
    "        sentence_word_next_path = sentence_word_next_split.join(sentence_path)\n",
    "        \n",
    "        # calculate path with max probability\n",
    "        sentence_word_next_path_prob = sentence_word_next_path.map(lambda x:\n",
    "                                                subs_word_prob(x, bc_transition_prob.value, default_transition_prob))\n",
    "        \n",
    "        # normalize for numerical stability\n",
    "        sentence_path = sentence_word_next_path_prob.groupByKey().mapValues(lambda v: normalize(v))\n",
    "        \n",
    "        word_num += 1\n",
    "        \n",
    "        # filter for the next words in sentences\n",
    "        sentence_word_next = sentence_word_suggestions.filter(lambda (k, v): v[0]==word_num) \\\n",
    "                .mapValues(lambda v: (v[1], v[2]))\n",
    "        \n",
    "    # get most likely path (sentence)\n",
    "    sentence_suggestion = sentence_path.mapValues(lambda v: get_max_path(v))\n",
    "\n",
    "    # join with original path (sentence)\n",
    "    sentence_max_prob = sentence_id.join(sentence_suggestion)\n",
    "        \n",
    "    ############\n",
    "    #\n",
    "    # output results\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # count the number of errors per sentence, drop any sentences without errors\n",
    "    sentence_errors = sentence_max_prob.mapValues(lambda v: (get_count_mismatches(v))) \\\n",
    "            .filter(lambda (k, v): v[0]>0).cache()\n",
    "               \n",
    "    # collect all sentences with identified errors\n",
    "    sentence_errors_list = sentence_errors.collect()\n",
    "    \n",
    "    # number of potentially misspelled words\n",
    "    num_errors = sum([s[1][0] for s in sentence_errors_list])\n",
    "    \n",
    "    # print identified errors (eventually output to file)\n",
    "    for sentence in sentence_errors_list:\n",
    "        print 'Line %i: %s --> %s' % (sentence[0], ' '.join(sentence[1][1]), ' '.join(sentence[1][2]))\n",
    "    \n",
    "    print '-----'\n",
    "    print 'Total words checked: %i' % accum_total_words.value\n",
    "    print 'Total potential errors found: %i' % num_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>SAMPLE OUTPUTS</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Total words processed: 1105285\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary (corpus words and deletions): 2151998\n",
      "  Edit distance for deletions: 3\n",
      "Total unique words at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n",
      "CPU times: user 10.1 s, sys: 880 ms, total: 11 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "    parallel_create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 3: this is ax test --> this is a test\n",
      "Line 4: this is za test --> this is a test\n",
      "Line 5: thee is a test --> there is a test\n",
      "-----\n",
      "Total words checked: 27\n",
      "Total potential errors found: 3\n",
      "CPU times: user 8.69 s, sys: 1.94 s, total: 10.6 s\n",
      "Wall time: 45.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context_parallel_steps('testdata/test.txt', dictionary,\n",
    "        start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
