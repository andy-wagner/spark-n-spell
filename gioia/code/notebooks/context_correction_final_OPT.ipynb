{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-Level Correction -  Serial Version and 3 SPARK Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context_correction.ipynb\n",
    "\n",
    "######################\n",
    "#\n",
    "# Submission by Gioia Dominedo (Harvard ID: 40966234) for\n",
    "# CS 205 - Computing Foundations for Computational Science\n",
    "# \n",
    "# This is part of a joint project with Kendrick Lo that includes a\n",
    "# separate component for word-level checking. This notebook outlines\n",
    "# algorithms for context-level correction, and includes a serial\n",
    "# Python algorithm adapted from third party algorithms (Symspell and\n",
    "# Viterbi algorithms), as well as a Spark/Python algorithm. \n",
    "#\n",
    "# The following were also used as references:\n",
    "# Peter Norvig, How to Write a Spelling Corrector\n",
    "#\t(http://norvig.com/spell-correct.html)\n",
    "# Peter Norvig, Natural Language Corpus Data: Beautiful Data\n",
    "#\t(http://norvig.com/ngrams/ch14.pdf)\n",
    "#\n",
    "# Two main approaches to parallelization were attempted: sentence-\n",
    "# level and word-level. Both attempts are documented in this notebook.\n",
    "#\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# SUMMARY OF CONTEXT-LEVEL CORRECTION LOGIC - VITERBI ALGORITHM\n",
    "#\n",
    "# v 1.0 last revised 30 Nov 2015\n",
    "#\n",
    "# Each sentence is modeled as a hidden Markov model. Prior\n",
    "# probabilities (for first words in the sentences) and transition\n",
    "# probabilities (for all subsequent words) are calculated when\n",
    "# generating the main dictionary, using the same corpus. Emission\n",
    "# probabilities are generated on the fly by parameterizing a Poisson \n",
    "# distribution with the edit distance between words and suggested\n",
    "# corrections.\n",
    "\n",
    "# The state space of possible corrections for each word is generated\n",
    "# using logic based on the Symspell spell-checker (see below for more\n",
    "# detail on Symspell). Valid suggestions must: (a) be 'real' words;\n",
    "# (b) appear at least 100 times in the corpus used to generate the\n",
    "# dictionary; (c) be one of the top 10 suggestions, based on frequency\n",
    "# and edit distance. This simplification ensures that the state space\n",
    "# remains manageable.\n",
    "#\n",
    "# All probabilities are stored in log-space to avoid underflow. Pre-\n",
    "# defined minimum values are used for words that are not present in\n",
    "# the dictionary and/or probability tables.\n",
    "#\n",
    "# More detail is included below.\n",
    "#\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>To run the serial version, restart notebook, and start executing the cells of this section starting here.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Serial Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>PRE-PROCESSING</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# PRE-PROCESSING STEPS\n",
    "#\n",
    "# The pre-processing steps have been adapted from the dictionary\n",
    "# creation of the word-level spellchecker, which in turn was based on\n",
    "# SymSpell, a Symmetric Delete spelling correction algorithm\n",
    "# developed by Wolf Garbe and originally written in C#. More detail\n",
    "# on SymSpell is included in the word-level spellcheck documentation.\n",
    "#\n",
    "# The main modifications to the word-level spellchecker pre-\n",
    "# processing stages are to create the additional outputs that are\n",
    "# required for the context-level checking, and to eliminate redundant\n",
    "# outputs that are not necessary.\n",
    "#\n",
    "# The outputs of the pre-processing stage are:\n",
    "#\n",
    "# - dictionary: A dictionary that combines both words present in the\n",
    "# corpus and other words that are within a given 'delete distance'. \n",
    "# The format of the dictionary is:\n",
    "# {word: ([list of words within the given 'delete distance'], \n",
    "# word count in corpus)}\n",
    "#\n",
    "# - start_prob: A dictionary with key, value pairs that correspond to\n",
    "# (word, probability of the word being the first word in a sentence)\n",
    "#\n",
    "# - transition_prob: A dictionary of dictionaries that stores the\n",
    "# probability of a given word following another. The format of the\n",
    "# dictionary is:\n",
    "# {previous word: {word1 : P(word1|prevous word), word2 : \n",
    "# P(word2|prevous word), ...}}\n",
    "#\n",
    "# - default_start_prob: A benchmark probability of a word being at\n",
    "# the start of a sentence, set to 1 / # of words at the beginning of\n",
    "# sentences. This ensures that all previously unseen words at the\n",
    "# beginning of sentences are not corrected unnecessarily.\n",
    "#\n",
    "# - default_transition_prob: A benchmark probability of a word being\n",
    "# seen, given the previous word in the sentence, also set to 1 / # of\n",
    "# transitions in corpus. This ensures that all previously unseen\n",
    "# transitions are not corrected unnecessarily.\n",
    "#\n",
    "######################\n",
    "\n",
    "def get_deletes_list(w, max_edit_distance):\n",
    "    '''\n",
    "    Given a word, derive strings with up to max_edit_distance\n",
    "    characters deleted.\n",
    "    '''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(max_edit_distance):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes\n",
    "\n",
    "def create_dictionary_entry(w, dictionary, max_edit_distance):\n",
    "    '''\n",
    "    Add a word and its derived deletions to the dictionary.\n",
    "    Dictionary entries are of the form:\n",
    "    ([list of suggested corrections], frequency of word in corpus)\n",
    "    '''\n",
    "\n",
    "    new_real_word_added = False\n",
    "    \n",
    "    # check if word is already in dictionary\n",
    "    if w in dictionary:\n",
    "        # increment count of word in corpus\n",
    "        dictionary[w] = (dictionary[w][0], dictionary[w][1] + 1)\n",
    "    else:\n",
    "        # create new entry in dictionary\n",
    "        dictionary[w] = ([], 1)  \n",
    "        \n",
    "    if dictionary[w][1]==1:\n",
    "        \n",
    "        # first appearance of a word in the corpus\n",
    "        # note: word may already be in dictionary as a derived word\n",
    "        # (e.g. by deleting character from a real word) but the\n",
    "        # word counter frequency is not incremented in those cases\n",
    "        \n",
    "        new_real_word_added = True\n",
    "        deletes = get_deletes_list(w, max_edit_distance)\n",
    "        \n",
    "        for item in deletes:\n",
    "            if item in dictionary:\n",
    "                # add (correct) word to delete's suggested correction\n",
    "                # list if not already there\n",
    "                if item not in dictionary[item][0]:\n",
    "                    dictionary[item][0].append(w)\n",
    "            else:\n",
    "                # note: frequency of word in corpus is not incremented\n",
    "                dictionary[item] = ([w], 0)  \n",
    "        \n",
    "    return new_real_word_added\n",
    "\n",
    "def pre_processing(fname, max_edit_distance=3):\n",
    "    '''\n",
    "    Load a text file and use it to create a dictionary and\n",
    "    to calculate start probabilities and transition probabilities. \n",
    "    '''\n",
    "\n",
    "    dictionary = dict()\n",
    "    start_prob = dict()\n",
    "    transition_prob = dict()\n",
    "    word_count = 0\n",
    "    transitions = 0\n",
    "    \n",
    "    with open(fname) as file:    \n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            # process each sentence separately\n",
    "            for sentence in line.split('.'):\n",
    "                \n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', sentence.lower())      \n",
    "                \n",
    "                for w, word in enumerate(words):\n",
    "                    \n",
    "                    # create/update dictionary entry\n",
    "                    if create_dictionary_entry(\n",
    "                        word, dictionary, max_edit_distance):\n",
    "                            word_count += 1\n",
    "                        \n",
    "                    # update probabilities for Hidden Markov Model\n",
    "                    if w == 0:\n",
    "\n",
    "                        # probability of a word being at the\n",
    "                        # beginning of a sentence\n",
    "                        if word in start_prob:\n",
    "                            start_prob[word] += 1\n",
    "                        else:\n",
    "                            start_prob[word] = 1\n",
    "                    else:\n",
    "                        \n",
    "                        # probability of transitionining from one\n",
    "                        # word to another\n",
    "                        # dictionary format:\n",
    "                        # {previous word: {word1 : P(word1|prevous\n",
    "                        # word), word2 : P(word2|prevous word)}}\n",
    "                        \n",
    "                        # check whether prior word is present\n",
    "                        # - create if not\n",
    "                        if words[w - 1] not in transition_prob:\n",
    "                            transition_prob[words[w - 1]] = dict()\n",
    "                            \n",
    "                        # check whether current word is present\n",
    "                        # - create if not\n",
    "                        if word not in transition_prob[words[w - 1]]:\n",
    "                            transition_prob[words[w - 1]][word] = 0\n",
    "                            \n",
    "                        # update value\n",
    "                        transition_prob[words[w - 1]][word] += 1\n",
    "                        transitions += 1\n",
    "                    \n",
    "    # convert counts to log-probabilities, to avoid underflow in\n",
    "    # later calculations (note: natural logarithm, not base-10)\n",
    "\n",
    "    # also calculate (smalle) default probabilities for words that \n",
    "    # have not already been seen\n",
    "    \n",
    "    # probability of a word being at the beginning of a sentence\n",
    "    total_start_words = float(sum(start_prob.values()))\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    start_prob.update( \n",
    "        {k: math.log(v/total_start_words)\n",
    "         for k, v in start_prob.items()})\n",
    "    \n",
    "    # probability of transitioning from one word to another\n",
    "    default_transition_prob = math.log(1./transitions)\n",
    "    transition_prob.update(\n",
    "        {k: {k1: math.log(float(v1)/sum(v.values()))\n",
    "             for k1, v1 in v.items()} \n",
    "         for k, v in transition_prob.items()})\n",
    "\n",
    "    # output summary statistics\n",
    "    print 'Total unique words in corpus: %i' % word_count\n",
    "    print 'Total items in dictionary: %i' \\\n",
    "        % len(dictionary)\n",
    "    print '  Edit distance for deletions: %i' % max_edit_distance\n",
    "    print 'Total unique words at the start of a sentence: %i' \\\n",
    "        % len(start_prob)\n",
    "    print 'Total unique word transitions: %i' % len(transition_prob)\n",
    "        \n",
    "    return dictionary, start_prob, default_start_prob, \\\n",
    "        transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<strong>SPELL-CHECKING</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# SPELL-CHECKING - VITERBI ALGORITHM\n",
    "#\n",
    "# The below functions are used to read in a text file, break it down\n",
    "# into individual sentences, and then carry out context-based spell-\n",
    "# checking on each sentence in turn. In cases where the 'suggested'\n",
    "# word does not match the actual word in the text, both the original\n",
    "# and the suggested sentences are printed/outputed to file.\n",
    "#\n",
    "# Probabilistic model:\n",
    "#\n",
    "# Each sentence is modeled as a hidden Markov model, where the\n",
    "# hidden states are the words that the user intended to type, and\n",
    "# the emissions are the words that were actually typed.\n",
    "#\n",
    "# For each word in a sentence, we can define:\n",
    "#\n",
    "# - emission probabilities: P(observed word|intended word)\n",
    "#\n",
    "# - prior probabilities (for first words in sentences only):\n",
    "# P(being the first word in a sentence)\n",
    "#\n",
    "# - transition probabilities (for all subsequent words):\n",
    "# P(intended word|previous intended word)\n",
    "#\n",
    "# Prior and transition probabilities were calculated in the pre-\n",
    "# processing steps above, using the same corpus as the dictionary.\n",
    "# \n",
    "# Emission probabilities are calculated on the fly using a Poisson\n",
    "# distribution as follows:\n",
    "# P(observed word|intended word) = PMF of Poisson(k, l), where\n",
    "# k = edit distance between word typed and word intended, and l=0.01.\n",
    "# Both the overall approach and the parameter of l=0.01 are based on\n",
    "# the 2015 lecture notes from AM207 Stochastic Optimization.\n",
    "# Various parameters for lambda between 0 and 1 were tested, which\n",
    "# confirmed that 0.01 yields the most accurate word suggestions.\n",
    "#\n",
    "# All probabilities are stored in log-space to avoid underflow. Pre-\n",
    "# defined minimum values (also defined at the pre-processing stage)\n",
    "# are used for words that are not present in the dictionary and/or\n",
    "# probability tables.\n",
    "#\n",
    "# Algorithm:\n",
    "#\n",
    "# The spell-checking itself is carried out using a modified version\n",
    "# of the Viterbi algorithm, which yields the most likely sequence of\n",
    "# hidden states, i.e. the most likely sequence of words that form a\n",
    "# sentence. The main difference to the 'standard' Viterbi algorithm\n",
    "# is that the state space (i.e. the list of possible corrections) is\n",
    "# generated (and therefore varies) for each word. This is in contrast\n",
    "# to the alternative of considering the state space of all possible\n",
    "# words in the dictionary for every word that is checked, which would\n",
    "# be intractable for larger dictionaries.\n",
    "#\n",
    "# Example:\n",
    "#\n",
    "# The algorithm is best illustrated by way of an example.\n",
    "#\n",
    "# Suppose that we are checking the sentence 'This is ax test.'\n",
    "# The emissions for the entire sentence are 'This is ax test.' and\n",
    "# the hidden states for the entire sentence are 'This is a test.'\n",
    "#\n",
    "# As a pre-processing step, we convert everything to lowercase,\n",
    "# eliminate punctuation, and break the sentence up into a list of\n",
    "# words: ['this', 'is', 'ax', 'text']\n",
    "# This list is passed as a parameter to the viterbi function.\n",
    "#\n",
    "# The algorithm tackles each word in turn, starting with 'this'.\n",
    "#\n",
    "# We first use get_suggestions to obtain a list of all words that\n",
    "# may have been intended instead of 'this', i.e. all possible hidden\n",
    "# states (intended words) for the emission (word typed).\n",
    "#\n",
    "# get_suggestions returns the 10 most likely corrections:\n",
    "# - 1 word with an edit distance of 0\n",
    "#   ['this']\n",
    "# - 3 words with an edit distance of 1\n",
    "#   ['his', 'thus', 'thin']\n",
    "# - 6 words with an edit distance of 2 \n",
    "#   ['the', 'that', 'is', 'him', 'they', 'their']\n",
    "# \n",
    "# These 10 words represent our state space, i.e. possible words that\n",
    "# may have been intended, and are referred to below as the list of\n",
    "# possible corrections. They each have an emission probability equal\n",
    "# to the PMF of Poisson(edit distance, 0.01).\n",
    "#\n",
    "# For each word in the list of possible corrections, we calculate:\n",
    "# P(word starting a sentence) * P(observed 'this'|intended word)\n",
    "# This is a simple application of Bayes' rule: by normalizing the\n",
    "# probabilities we obtain P(intended word|oberved 'this') for\n",
    "# each of the 10 words.\n",
    "#\n",
    "# We store the word-probability pairs for future use, and move on to\n",
    "# the next word. \n",
    "#\n",
    "# After the first word, all subsequent words are treated as follows.\n",
    "#\n",
    "# The second word in our test sentence is 'is'. Once again, we use\n",
    "# get_suggestions to obtain a list of all words that may have been\n",
    "# intended. get_suggestions returns the 10 most likely suggestions:\n",
    "# - 1 word with an edit distance of 0\n",
    "#   ['is']\n",
    "# - 9 words with an edit distance of 1\n",
    "#   ['in', 'it', 'his', 'as', 'i', 's', 'if', 'its', 'us']\n",
    "# These 10 words represent our state space for the second word.\n",
    "#\n",
    "# For each word in the current list of possible corrections, we loop\n",
    "# through all the words in the previous list of possible corrections,\n",
    "# and calculate:\n",
    "#    probability(previous suggested word) \n",
    "#    * P(current suggested word|previous suggested word)\n",
    "#    * P(typing 'is'|meaning to type current suggested word)\n",
    "# We determine which previous word maximizes this calculation and\n",
    "# store that 'path' and probability for each current suggested word.\n",
    "#\n",
    "# For example, suppose that we are considering the possibility that\n",
    "# 'is' was indeed intended to be 'is'. We then calculate: \n",
    "#    probability(previous suggested word)\n",
    "#    * P('is'|previous suggested word) * P('is'|'is')\n",
    "# for all previous suggested words, and discover that the previous\n",
    "# suggested word 'this' maximizes the above calculation. We therefore\n",
    "# store 'this is' as the optimal path for the suggested correction\n",
    "# 'is' and the above (normalized) probability associated with this\n",
    "# path.\n",
    "#\n",
    "# If the sentence had been only 2 words long, then at this point we\n",
    "# would return the path that maximizes the most probability for the\n",
    "# most recent step (word).\n",
    "#\n",
    "# As it is not, we repeat the previous steps for 'ax' and 'test',\n",
    "# and then return the path that is associated with the highest\n",
    "# probability at the last step.\n",
    "#\n",
    "######################\n",
    "\n",
    "\n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    matrix. However, only the current and two previous rows are\n",
    "    needed at once, so we only store those.\n",
    "\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, max_edit_distance, \n",
    "                    longest_word_length=20, min_count=100, max_sug=4):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count and max_sug parameters.\n",
    "    - min_count: minimum number of times a word must have appeared\n",
    "    in the dictionary corpus to be considered a valid suggestion\n",
    "    - max_sug: number of suggestions that are returned (ranked by\n",
    "    frequency of appearance in dictionary corpus and edit distance\n",
    "    from word being checked)\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > max_edit_distance:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=max_edit_distance:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<max_edit_distance \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # note: make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance between the observed word and the intended\n",
    "    word and l=0.01.\n",
    "    \n",
    "    Both the overall approach and the parameter of l=0.01 are based on\n",
    "    the 2015 lecture notes from AM207 Stochastic Optimization.\n",
    "    Various parameters for lambda between 0 and 1 were tested, which\n",
    "    confirmed that 0.01 yields the most accurate word suggestions.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    '''\n",
    "    P(word being at the beginning of a sentence)\n",
    "    '''\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, \n",
    "                        transition_prob, default_transition_prob):\n",
    "    '''\n",
    "    P(word|previous word)\n",
    "    '''\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "\n",
    "def get_path_prob(prev_word, prev_path_prob):\n",
    "    '''\n",
    "    P(previous path)\n",
    "    '''\n",
    "    try:\n",
    "        return prev_path_prob[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_path_prob.values()))/2.)  \n",
    "    \n",
    "def viterbi(words, dictionary, start_prob, default_start_prob, \n",
    "            transition_prob, default_transition_prob, max_edit_distance):\n",
    "    '''\n",
    "    Determines the most likely (intended) sequence, based on the\n",
    "    observed sequence. Full details in preamble above.\n",
    "    '''\n",
    "\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    path_context = []\n",
    "    \n",
    "    # character level correction - used to determine state space\n",
    "    corrections = get_suggestions(words[0], dictionary, max_edit_distance)\n",
    "        \n",
    "    # Initialize base cases (first word in the sentence)\n",
    "    for sug_word in corrections:\n",
    "        \n",
    "        # compute the value for all possible starting states\n",
    "        V[0][sug_word[0]] = math.exp(\n",
    "            get_start_prob(sug_word[0], start_prob, \n",
    "                           default_start_prob)\n",
    "            + get_emission_prob(sug_word[1]))\n",
    "        \n",
    "        # remember all the different paths (only one word so far)\n",
    "        path[sug_word[0]] = [sug_word[0]]\n",
    " \n",
    "    # normalize for numerical stability\n",
    "    path_temp_sum = sum(V[0].values())\n",
    "    V[0].update({k: math.log(v/path_temp_sum) \n",
    "                 for k, v in V[0].items()})\n",
    "    \n",
    "    # keep track of previous state space\n",
    "    prev_corrections = [i[0] for i in corrections]\n",
    "    \n",
    "    # return if the sentence only has one word\n",
    "    if len(words) == 1:\n",
    "        path_context = [max(V[0], key=lambda i: V[0][i])]\n",
    "        return path_context\n",
    "\n",
    "    # run Viterbi for all subsequent words in the sentence\n",
    "    for t in range(1, len(words)):\n",
    "\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        \n",
    "        # character level correction\n",
    "        corrections = get_suggestions(words[t], dictionary, max_edit_distance)\n",
    " \n",
    "        for sug_word in corrections:\n",
    "        \n",
    "            sug_word_emission_prob = get_emission_prob(sug_word[1])\n",
    "            \n",
    "            # compute the probabilities associated with all previous\n",
    "            # states (paths), only keep the maximum\n",
    "            (prob, word) = max(\n",
    "                (get_path_prob(prev_word, V[t-1]) \n",
    "                + get_transition_prob(sug_word[0], prev_word, \n",
    "                    transition_prob, default_transition_prob)\n",
    "                + sug_word_emission_prob, prev_word) \n",
    "                               for prev_word in prev_corrections)\n",
    "\n",
    "            # save the maximum probability for each state\n",
    "            V[t][sug_word[0]] = math.exp(prob)\n",
    "            \n",
    "            # store the full path that results in this probability\n",
    "            new_path[sug_word[0]] = path[word] + [sug_word[0]]\n",
    "        \n",
    "        # normalize for numerical stability\n",
    "        path_temp_sum = sum(V[t].values())\n",
    "        V[t].update({k: math.log(v/path_temp_sum) \n",
    "                     for k, v in V[t].items()})\n",
    "        \n",
    "        print V[t]\n",
    "        print new_path\n",
    "        \n",
    "        # keep track of previous state space\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "        # don't need to remember the old paths\n",
    "        path = new_path\n",
    "     \n",
    "    # after all iterations are completed, look up the word with the\n",
    "    # highest probability\n",
    "    (prob, word) = max((V[t][sug_word[0]], sug_word[0]) \n",
    "                       for sug_word in corrections)\n",
    "\n",
    "    # look up the full path associated with this word\n",
    "    path_context = path[word]\n",
    "\n",
    "    return path_context\n",
    "\n",
    "def correct_document_context(fname, dictionary, \n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob,\n",
    "                             max_edit_distance=3, display_results=False):\n",
    "    \n",
    "    doc_word_count = 0\n",
    "    corrected_word_count = 0\n",
    "    sentence_errors_list = []\n",
    "    total_sentences = 0\n",
    "    \n",
    "    with open(fname) as file:\n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            \n",
    "            for sentence in line.split('.'):\n",
    "                \n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', sentence.lower())  \n",
    "                doc_word_count += len(words)\n",
    "                \n",
    "                if len(words) > 0:\n",
    "                \n",
    "                    # run Viterbi algorithm for each sentence and\n",
    "                    # obtain most likely correction (may be the same\n",
    "                    # as the original sentence)\n",
    "                    suggestion = viterbi(words, dictionary,\n",
    "                                start_prob, default_start_prob, \n",
    "                                transition_prob, default_transition_prob,\n",
    "                                max_edit_distance)\n",
    "\n",
    "                    # display sentences with suggested changes\n",
    "                    if words != suggestion:\n",
    "                        \n",
    "                        # keep track of all potential errors\n",
    "                        sentence_errors_list.append([total_sentences, \n",
    "                            (words, suggestion)])\n",
    "                        \n",
    "                        # update count of corrected words\n",
    "                        corrected_word_count += \\\n",
    "                        sum([words[j]!=suggestion[j] \n",
    "                             for j in range(len(words))])\n",
    "                        \n",
    "                    # used for display purposes\n",
    "                    total_sentences += 1\n",
    "  \n",
    "    # print suggested corrections\n",
    "    if display_results:\n",
    "        for sentence in sentence_errors_list:\n",
    "            print 'Sentence %i: %s --> %s' % (sentence[0],\n",
    "                ' '.join(sentence[1][0]), ' '.join(sentence[1][1]))\n",
    "            print '-----'\n",
    "    \n",
    "    # output suggested corrections to file\n",
    "    else:\n",
    "        f = open('spell-log.txt', 'w')\n",
    "        for sentence in sentence_errors_list:\n",
    "            f.write('Sentence %i: %s --> %s\\n' % (sentence[0], \n",
    "                ' '.join(sentence[1][0]), ' '.join(sentence[1][1])))\n",
    "        f.close()\n",
    "            \n",
    "    # display summary statistics\n",
    "    print 'Total words checked: %i' % doc_word_count\n",
    "    print 'Total potential errors found: %i' % corrected_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>SAMPLE OUTPUTS</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary: 2151998\n",
      "  Edit distance for deletions: 3\n",
      "Total unique words at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n",
      "CPU times: user 32.6 s, sys: 617 ms, total: 33.2 s\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, start_prob, default_start_prob, \\\n",
    "transition_prob, default_transition_prob \\\n",
    "    = pre_processing('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0074216132810001915"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(-4.903358822120482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his': -9.207803824324186, 'is': -0.0008023536135176925, 'it': -8.920121751872406, 'in': -7.47320276893608}\n",
      "{'is': ['this', 'is'], 'his': ['this', 'his'], 'it': ['this', 'it'], 'in': ['this', 'in']}\n",
      "{'a': -0.12916391243323155, 'as': -2.79061666555702, 'at': -2.8406270861316822, 'ax': -6.572921069019194}\n",
      "{'a': ['this', 'is', 'a'], 'ax': ['this', 'is', 'ax'], 'as': ['this', 'is', 'as'], 'at': ['this', 'is', 'at']}\n",
      "{'west': -9.922355665522849, 'rest': -4.903358822120482, 'test': -0.010506563680610304, 'best': -5.815535440715409}\n",
      "{'test': ['this', 'is', 'a', 'test'], 'west': ['this', 'is', 'a', 'west'], 'rest': ['this', 'is', 'a', 'rest'], 'best': ['this', 'is', 'at', 'best']}\n",
      "Total words checked: 4\n",
      "Total potential errors found: 1\n",
      "CPU times: user 129 ms, sys: 3.81 ms, total: 133 ms\n",
      "Wall time: 132 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context('testdata/test1.txt', dictionary,\n",
    "                         start_prob, default_start_prob, \n",
    "                         transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<strong>To run the SPARK implementations, restart notebook, and start executing the cells of this section starting here.</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 2. Pre-Processing SPARK Code Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# DOCUMENTATION HERE\n",
    "#\n",
    "######################\n",
    "\n",
    "# number of partitions to be used\n",
    "n_partitions = 6\n",
    "MAX_EDIT_DISTANCE = 3\n",
    "\n",
    "def get_n_deletes_list(w, n):\n",
    "    '''\n",
    "    Given a word, derive list of strings with up to n characters deleted\n",
    "    '''\n",
    "    # since this list is generally of the same magnitude as the number of \n",
    "    # characters in a word, it may not make sense to parallelize this\n",
    "    # so we use python to create the list\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(n):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes\n",
    "\n",
    "def get_transitions(sentence):\n",
    "    if len(sentence)<2:\n",
    "        return None\n",
    "    else:\n",
    "        return [((sentence[i], sentence[i+1]), 1) \n",
    "                for i in range(len(sentence)-1)]\n",
    "    \n",
    "def map_transition_prob(x):\n",
    "    vals = x[1]\n",
    "    total = float(sum(vals.values()))\n",
    "    probs = {k: math.log(v/total) for k, v in vals.items()}\n",
    "    return (x[0], probs)\n",
    "\n",
    "def parallel_create_dictionary(fname):\n",
    "    '''\n",
    "    Create dictionary, start probabilities and transition\n",
    "    probabilities using Spark RDDs.\n",
    "    '''\n",
    "    # we generate and count all words for the corpus,\n",
    "    # then add deletes to the dictionary\n",
    "    # this is a slightly different approach from the SymSpell algorithm\n",
    "    # that may be more appropriate for Spark processing\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "\n",
    "    # convert file into one long sequence of words\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "            .map(lambda line: line.lower()) \\\n",
    "            .filter(lambda x: x!='').cache()\n",
    "    \n",
    "    # split into individual sentences and remove other punctuation\n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate start probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # only focus on words at the start of sentences\n",
    "    start_words = split_sentence.map(lambda sentence: sentence[0] if len(sentence)>0 else None) \\\n",
    "            .filter(lambda word: word!=None)\n",
    "    \n",
    "    # add a count to each word\n",
    "    count_start_words_once = start_words.map(lambda word: (word, 1)).partitionBy(n_partitions).cache() ##\n",
    "\n",
    "    # use accumulator to count the number of words at the start of sentences\n",
    "    accum_total_start_words = sc.accumulator(0)\n",
    "    count_start_words_once.foreach(lambda x: accum_total_start_words.add(1))\n",
    "    total_start_words = float(accum_total_start_words.value)\n",
    "    \n",
    "    # reduce into count of unique words at the start of sentences\n",
    "    unique_start_words = count_start_words_once.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # convert counts to probabilities\n",
    "    start_prob_calc = unique_start_words.mapValues(lambda v: math.log(v/total_start_words))\n",
    "    \n",
    "    # get default start probabilities (for words not in corpus)\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    \n",
    "    # store start probabilities as a dictionary (will be used as a lookup table)\n",
    "    start_prob = start_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate transition probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # focus on continuous word pairs within the sentence\n",
    "    # e.g. \"this is a test\" -> \"this is\", \"is a\", \"a test\"\n",
    "    # note: as the relevant probability is P(word|previous word)\n",
    "    # the tuples are ordered as (previous word, word)\n",
    "    other_words = split_sentence.map(lambda sentence: get_transitions(sentence)) \\\n",
    "            .filter(lambda x: x!=None). \\\n",
    "            flatMap(lambda x: x).cache()\n",
    "\n",
    "    # use accumulator to count the number of transitions\n",
    "    accum_total_other_words = sc.accumulator(0)\n",
    "    count_total_other_words = other_words.foreach(lambda x: accum_total_other_words.add(1))\n",
    "    total_other_words = float(accum_total_other_words.value)\n",
    "    \n",
    "    # reduce into count of unique word pairs\n",
    "    unique_other_words = other_words.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # aggregate by previous word\n",
    "    # i.e. (previous word, [(word1, word1-previous word count), (word2, word2-previous word count), ...])\n",
    "    other_words_collapsed2 = unique_other_words.map(lambda x: (x[0][0], (x[0][1], x[1])))\n",
    "    other_words_collapsed = other_words_collapsed2.groupByKey().mapValues(dict)\n",
    "    \n",
    "    # convert counts to probabilities\n",
    "    transition_prob_calc = other_words_collapsed.map(lambda x: map_transition_prob(x))\n",
    "    \n",
    "    # get default transition probabilities (for word pairs not in corpus)\n",
    "    default_transition_prob = math.log(1/total_other_words)\n",
    "    \n",
    "    # store transition probabilities as dictionary (will be used as lookup table)\n",
    "    transition_prob = transition_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # process corpus for dictionary\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    replace_nonalphs = make_all_lower.map(lambda line: regex.sub(' ', line))\n",
    "    all_words = replace_nonalphs.flatMap(lambda line: line.split())\n",
    "\n",
    "    # create core corpus dictionary (i.e. only words appearing in file, no \"deletes\") and cache it\n",
    "    # output RDD of unique_words_with_count: [(word1, count1), (word2, count2), (word3, count3)...]\n",
    "    count_once = all_words.map(lambda word: (word, 1))\n",
    "    unique_words_with_count = count_once.reduceByKey(lambda a, b: a + b).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate deletes list\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # generate list of n-deletes from words in a corpus of the form: [(word1, count1), (word2, count2), ...]\n",
    "     \n",
    "    assert MAX_EDIT_DISTANCE > 0  \n",
    "    \n",
    "    generate_deletes = unique_words_with_count.map(lambda (parent, count): \n",
    "                                                   (parent, get_n_deletes_list(parent, MAX_EDIT_DISTANCE)))\n",
    "    expand_deletes = generate_deletes.flatMapValues(lambda x: x)\n",
    "    swap = expand_deletes.map(lambda (orig, delete): (delete, ([orig], 0)))\n",
    "   \n",
    "    ############\n",
    "    #\n",
    "    # combine delete elements with main dictionary\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    corpus = unique_words_with_count.mapValues(lambda count: ([], count))\n",
    "    combine = swap.union(corpus)  # combine deletes with main dictionary, eliminate duplicates\n",
    "    \n",
    "    # since the dictionary will only be a lookup table once created, we can\n",
    "    # pass on as a Python dictionary rather than RDD by reducing locally and\n",
    "    # avoiding an extra shuffle from reduceByKey\n",
    "    dictionary = combine.reduceByKeyLocally(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "\n",
    "    words_processed = unique_words_with_count.map(lambda (k, v): v) \\\n",
    "            .reduce(lambda a, b: a + b)\n",
    "        \n",
    "    word_count = unique_words_with_count.count()   \n",
    "    \n",
    "    # output stats\n",
    "    print 'Total words processed: %i' % words_processed\n",
    "    print 'Total unique words in corpus: %i' % word_count \n",
    "    print 'Total items in dictionary (corpus words and deletions): %i' % len(dictionary)\n",
    "    print '  Edit distance for deletions: %i' % MAX_EDIT_DISTANCE\n",
    "    print 'Total unique words at the start of a sentence: %i' \\\n",
    "        % len(start_prob)\n",
    "    print 'Total unique word transitions: %i' % len(transition_prob)\n",
    "    \n",
    "    return dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words processed: 1105285\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary (corpus words and deletions): 2151998\n",
      "  Edit distance for deletions: 3\n",
      "Total unique words at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n",
      "CPU times: user 16.3 s, sys: 803 ms, total: 17.2 s\n",
      "Wall time: 59.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "    parallel_create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3. Spellchecking SPARK Code Performance - Parallelizing Across Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# DOCUMENTATION HERE\n",
    "#\n",
    "######################\n",
    "\n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, \n",
    "                        transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "    \n",
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)\n",
    "    \n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # this block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, longest_word_length=20, \n",
    "                    min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def viterbi(words, dictionary, start_prob, default_start_prob, \n",
    "            transition_prob, default_transition_prob):\n",
    "    \n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    path_context = []\n",
    "    \n",
    "    # character level correction - used to determine state space\n",
    "    corrections = get_suggestions(words[0], dictionary)\n",
    "        \n",
    "    # Initialize base cases (t == 0)\n",
    "    for sug_word in corrections:\n",
    "        \n",
    "        # compute the value for all possible starting states\n",
    "        V[0][sug_word[0]] = math.exp(\n",
    "            get_start_prob(sug_word[0], start_prob, \n",
    "                           default_start_prob)\n",
    "            + get_emission_prob(sug_word[1]))\n",
    "        \n",
    "        # remember all the different paths (only one state so far)\n",
    "        path[sug_word[0]] = [sug_word[0]]\n",
    " \n",
    "    # normalize for numerical stability\n",
    "    path_temp_sum = sum(V[0].values())\n",
    "    V[0].update({k: math.log(v/path_temp_sum) \n",
    "                 for k, v in V[0].items()})\n",
    "    \n",
    "    # keep track of previous state space\n",
    "    prev_corrections = [i[0] for i in corrections]\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        path_context = [max(V[0], key=lambda i: V[0][i])]\n",
    "        return path_context\n",
    "\n",
    "    # run Viterbi for t > 0\n",
    "    for t in range(1, len(words)):\n",
    "\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        \n",
    "        # character level correction\n",
    "        corrections = get_suggestions(words[t], dictionary)\n",
    " \n",
    "        for sug_word in corrections:\n",
    "        \n",
    "            sug_word_emission_prob = get_emission_prob(sug_word[1])\n",
    "            \n",
    "            # compute the values coming from all possible previous\n",
    "            # states, only keep the maximum\n",
    "            (prob, word) = max(\n",
    "                (get_belief(prev_word, V[t-1]) \n",
    "                + get_transition_prob(sug_word[0], prev_word, \n",
    "                    transition_prob, default_transition_prob)\n",
    "                + sug_word_emission_prob, prev_word) \n",
    "                               for prev_word in prev_corrections)\n",
    "\n",
    "            # save the maximum value for each state\n",
    "            V[t][sug_word[0]] = math.exp(prob)\n",
    "            \n",
    "            # remember the path we came from to get this maximum value\n",
    "            new_path[sug_word[0]] = path[word] + [sug_word[0]]\n",
    "            \n",
    "        # normalize for numerical stability\n",
    "        path_temp_sum = sum(V[t].values())\n",
    "        V[t].update({k: math.log(v/path_temp_sum) \n",
    "                     for k, v in V[t].items()})\n",
    "        \n",
    "        # keep track of previous state space\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "        # don't need to remember the old paths\n",
    "        path = new_path\n",
    "     \n",
    "    (prob, word) = max((V[t][sug_word[0]], sug_word[0]) \n",
    "                       for sug_word in corrections)\n",
    "    path_context = path[word]\n",
    "    \n",
    "    return path_context\n",
    "\n",
    "def get_count_mismatches(sentences):\n",
    "    orig_sentence, sug_sentence = sentences\n",
    "    count_mismatches = len([(orig_sentence[i], sug_sentence[i]) \n",
    "            for i in range(len(orig_sentence))\n",
    "            if orig_sentence[i]!=sug_sentence[i]])\n",
    "    return count_mismatches, orig_sentence, sug_sentence\n",
    "\n",
    "def correct_document_context_parallel_sentences(fname, dictionary,\n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob):\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # broadcast Python dictionaries to workers\n",
    "    bc_dictionary = sc.broadcast(dictionary)\n",
    "    bc_start_prob = sc.broadcast(start_prob)\n",
    "    bc_transition_prob = sc.broadcast(transition_prob)\n",
    "    \n",
    "    # convert all text to lowercase and drop empty lines\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "        .map(lambda line: line.lower()) \\\n",
    "        .filter(lambda x: x!='')\n",
    "    \n",
    "    regex = re.compile('[^a-z ]')\n",
    "    \n",
    "    # split into sentences -> remove special characters -> convert into list of words\n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()\n",
    "    \n",
    "    # use accumulator to count the number of words checked\n",
    "    accum_total_words = sc.accumulator(0)\n",
    "    split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "    \n",
    "    # assign each sentence a unique id\n",
    "    sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k))\n",
    "\n",
    "    ############\n",
    "    #\n",
    "    # spell-checking\n",
    "    #\n",
    "    ############\n",
    "\n",
    "    # apply Viterbi algorithm to each sentence\n",
    "    sentence_correction = sentence_id.mapValues(lambda v: (v, \n",
    "                viterbi(v, bc_dictionary.value, bc_start_prob.value, \n",
    "                        default_start_prob, bc_transition_prob.value, default_transition_prob)))#.cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # output results\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # count the number of errors per sentence, drop any sentences without errors\n",
    "    sentence_errors = sentence_correction.mapValues(lambda v: (get_count_mismatches(v))). \\\n",
    "            filter(lambda (k, v): v[0]>0).cache()               \n",
    "    \n",
    "    # collect all sentences with identified errors\n",
    "    sentence_errors_list = sentence_errors.collect()\n",
    "    \n",
    "    # number of potentially misspelled words\n",
    "    num_errors = sum([s[1][0] for s in sentence_errors_list])\n",
    "    \n",
    "    # print identified errors (eventually output to file)\n",
    "    for sentence in sentence_errors_list:\n",
    "        print 'Sentence %i: %s --> %s' % (sentence[0], ' '.join(sentence[1][1]), ' '.join(sentence[1][2]))\n",
    "    \n",
    "    print '-----'\n",
    "    print 'Total words checked: %i' % accum_total_words.value\n",
    "    print 'Total potential errors found: %i' % num_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>SAMPLE OUTPUTS</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "#     parallel_create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 811.0 failed 1 times, most recent failure: Lost task 0.0 in stage 811.0 (TID 8679, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-31-ed8d17502b53>\", line 307, in <lambda>\n  File \"<ipython-input-31-ed8d17502b53>\", line 190, in viterbi\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:262)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:373)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-31-ed8d17502b53>\", line 307, in <lambda>\n  File \"<ipython-input-31-ed8d17502b53>\", line 190, in viterbi\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:262)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-b6790f0640c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"correct_document_context_parallel_sentences('testdata/tiny.txt', dictionary,\\n        start_prob, default_start_prob, transition_prob, default_transition_prob)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2262\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-ed8d17502b53>\u001b[0m in \u001b[0;36mcorrect_document_context_parallel_sentences\u001b[0;34m(fname, dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# collect all sentences with identified errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0msentence_errors_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;31m# number of potentially misspelled words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \"\"\"\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 811.0 failed 1 times, most recent failure: Lost task 0.0 in stage 811.0 (TID 8679, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-31-ed8d17502b53>\", line 307, in <lambda>\n  File \"<ipython-input-31-ed8d17502b53>\", line 190, in viterbi\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:262)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:373)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-31-ed8d17502b53>\", line 307, in <lambda>\n  File \"<ipython-input-31-ed8d17502b53>\", line 190, in viterbi\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:69)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:262)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context_parallel_sentences('testdata/tiny.txt', dictionary,\n",
    "        start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 4. Spellchecking SPARK Code Performance - Parallelizing Across Possible Word Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# DOCUMENTATION HERE\n",
    "#\n",
    "######################\n",
    "\n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, longest_word_length=20, \n",
    "                    min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, transition_prob, \n",
    "                        default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "\n",
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)  \n",
    "\n",
    "def map_sentence_words(sentence, tmp_dict):\n",
    "    return [[word, get_suggestions(word, tmp_dict)] \n",
    "            for i, word in enumerate(sentence)]\n",
    "\n",
    "def split_suggestions(sentence):\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        result.append([(word[0], s[0], get_emission_prob(s[1])) \n",
    "                       for s in word[1]])\n",
    "    return result\n",
    "\n",
    "def get_word_combos(sug_lists):\n",
    "    return list(itertools.product(*sug_lists))\n",
    "\n",
    "def split_combos(combos):\n",
    "    sent_id, combo_list = combos\n",
    "    return [[sent_id, c] for c in combo_list]\n",
    "\n",
    "def get_combo_prob(combo, tmp_sp, d_sp, tmp_tp, d_tp):\n",
    "    \n",
    "    # first word in sentence\n",
    "    # emission prob * start prob\n",
    "    orig_path = [combo[0][0]]\n",
    "    sug_path = [combo[0][1]]\n",
    "    prob = combo[0][2] + get_start_prob(combo[0][1], tmp_sp, d_sp)\n",
    "    \n",
    "    # subsequent words\n",
    "    for i, w in enumerate(combo[1:]):\n",
    "        orig_path.append(w[0])\n",
    "        sug_path.append(w[1])\n",
    "        prob += w[2] + get_transition_prob(w[1], combo[i-1][1], tmp_tp, d_tp)\n",
    "    \n",
    "    return orig_path, sug_path, prob\n",
    "\n",
    "def get_count_mismatches_prob(sentences):\n",
    "    orig_sentence, sug_sentence, prob = sentences\n",
    "    count_mismatches = len([(orig_sentence[i], sug_sentence[i]) \n",
    "            for i in range(len(orig_sentence))\n",
    "            if orig_sentence[i]!=sug_sentence[i]])\n",
    "    return count_mismatches, orig_sentence, sug_sentence\n",
    "\n",
    "def correct_document_context_parallel_combos(fname, dictionary,\n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob):\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # broadcast Python dictionaries to workers\n",
    "    bc_dictionary = sc.broadcast(dictionary)\n",
    "    bc_start_prob = sc.broadcast(start_prob)\n",
    "    bc_transition_prob = sc.broadcast(transition_prob)\n",
    "    \n",
    "    # convert all text to lowercase and drop empty lines\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "        .map(lambda line: line.lower()) \\\n",
    "        .filter(lambda x: x!='')\n",
    "    \n",
    "    regex = re.compile('[^a-z ]')\n",
    "    \n",
    "    # split into sentences -> remove special characters -> convert into list of words\n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()\n",
    "    \n",
    "    # use accumulator to count the number of words checked\n",
    "    accum_total_words = sc.accumulator(0)\n",
    "    split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "    \n",
    "    # assign each sentence a unique id\n",
    "    sentence_id = split_sentence.zipWithIndex() \\\n",
    "            .map(lambda (k, v): (v, k)).cache()   ### \n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # spell-checking\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # look up possible suggestions for each word in each sentence\n",
    "    sentence_words = sentence_id.mapValues(lambda v: map_sentence_words(v, bc_dictionary.value))\n",
    "    \n",
    "    # look up emission probabilities for each word\n",
    "    # i.e. P(observed word|intended word)\n",
    "    sentence_word_sug = sentence_words.mapValues(lambda v: split_suggestions(v))\n",
    "    \n",
    "    # generate all possible corrected combinations (using Cartesian product)\n",
    "    # i.e. a sentence with 4 word, each of which have 5 possible suggestions,\n",
    "    # will yield 5^4 possible combinations\n",
    "    sentence_word_combos = sentence_word_sug.mapValues(lambda v: get_word_combos(v))\n",
    "    \n",
    "    # flatmap into all possible combinations per sentence\n",
    "    # format: [sentence id, \n",
    "    # [(observed first word, potential first word, P(observed first word|intended first word)]), \n",
    "    # (observed second word, potential second word, P(observed second word|intended second word)]), ...]\n",
    "    sentence_word_combos_split = sentence_word_combos.flatMap(lambda x: split_combos(x))\n",
    "    \n",
    "    # calculate the probability of each word combination being the intended one, given what was observed\n",
    "    # note: the approach does not allow for normalization across iterations, so may yield different results\n",
    "    sentence_word_combos_prob = sentence_word_combos_split.mapValues(lambda v:  \n",
    "                                get_combo_prob(v, bc_start_prob.value, default_start_prob, \n",
    "                                               bc_transition_prob.value, default_transition_prob))\n",
    "    \n",
    "    # identify the word combination with the highest probability for each sentence\n",
    "    sentence_max_prob = sentence_word_combos_prob.reduceByKey(lambda a,b: a if a[2] > b[2] else b)\n",
    "\n",
    "    ############\n",
    "    #\n",
    "    # output results\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # count the number of errors per sentence, drop any sentences without errors\n",
    "    sentence_errors = sentence_max_prob.mapValues(lambda v: (get_count_mismatches_prob(v))) \\\n",
    "            .filter(lambda (k, v): v[0]>0).cache()\n",
    "               \n",
    "    # collect all sentences with identified errors\n",
    "    sentence_errors_list = sentence_errors.collect()\n",
    "    \n",
    "    # number of potentially misspelled words\n",
    "    num_errors = sum([s[1][0] for s in sentence_errors_list])\n",
    "    \n",
    "    # print identified errors (eventually output to file)\n",
    "    for sentence in sentence_errors_list:\n",
    "        print 'Sentence %i: %s --> %s' % (sentence[0], ' '.join(sentence[1][1]), ' '.join(sentence[1][2]))\n",
    "    \n",
    "    print '-----'\n",
    "    print 'Total words checked: %i' % accum_total_words.value\n",
    "    print 'Total potential errors found: %i' % num_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>SAMPLE OUTPUTS</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "#     parallel_create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 807.0 failed 1 times, most recent failure: Lost task 3.0 in stage 807.0 (TID 8673, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 2352, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 2352, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1780, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-42-8c1459f408c6>\", line 270, in <lambda>\n  File \"<ipython-input-78-a15a7e89c592>\", line 214, in split_suggestions\nValueError: too many values to unpack\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:373)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 2352, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 2352, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1780, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-42-8c1459f408c6>\", line 270, in <lambda>\n  File \"<ipython-input-78-a15a7e89c592>\", line 214, in split_suggestions\nValueError: too many values to unpack\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-ee151948fa6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu\"correct_document_context_parallel_combos('testdata/test.txt', dictionary,\\n        start_prob, default_start_prob, transition_prob, default_transition_prob)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2262\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-8c1459f408c6>\u001b[0m in \u001b[0;36mcorrect_document_context_parallel_combos\u001b[0;34m(fname, dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;31m# collect all sentences with identified errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0msentence_errors_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;31m# number of potentially misspelled words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \"\"\"\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 538\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 807.0 failed 1 times, most recent failure: Lost task 3.0 in stage 807.0 (TID 8673, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 2352, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 2352, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1780, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-42-8c1459f408c6>\", line 270, in <lambda>\n  File \"<ipython-input-78-a15a7e89c592>\", line 214, in split_suggestions\nValueError: too many values to unpack\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1280)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1268)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1267)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1267)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1493)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1455)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1444)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1813)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1826)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1839)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1910)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:904)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:373)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 2352, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 2352, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1780, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/usr/local/opt/apache-spark/libexec/python/lib/pyspark.zip/pyspark/shuffle.py\", line 266, in mergeValues\n    for k, v in iterator:\n  File \"/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.py\", line 1907, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-42-8c1459f408c6>\", line 270, in <lambda>\n  File \"<ipython-input-78-a15a7e89c592>\", line 214, in split_suggestions\nValueError: too many values to unpack\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:138)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:179)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:97)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:311)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:264)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:88)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context_parallel_combos('testdata/test.txt', dictionary,\n",
    "        start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### 5. Spellchecking SPARK Code Performance - Parallelizing Across Viterbi Algorithm Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# DOCUMENTATION HERE\n",
    "#\n",
    "######################\n",
    "\n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, longest_word_length=20, \n",
    "                    min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, transition_prob, \n",
    "                        default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "\n",
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)  \n",
    "\n",
    "def get_count_mismatches(sentences):\n",
    "    orig_sentence, sug_sentence = sentences\n",
    "    count_mismatches = len([(orig_sentence[i], sug_sentence[i]) \n",
    "            for i in range(len(orig_sentence))\n",
    "            if orig_sentence[i]!=sug_sentence[i]])\n",
    "    return count_mismatches, orig_sentence, sug_sentence\n",
    "\n",
    "def get_sentence_word_id(words):\n",
    "    return [(i, w) for i, w in enumerate(words)]\n",
    "\n",
    "def split_sentence_words(sentence):\n",
    "    sent_id, words = sentence\n",
    "    return [[sent_id, w] for w in words]\n",
    "\n",
    "def start_word_prob(words, tmp_sp, d_sp):\n",
    "    orig_word, sug_words = words\n",
    "    probs = [(w[0], math.exp(\n",
    "                get_start_prob(w[0], tmp_sp, d_sp) \n",
    "                + get_emission_prob(w[1])\n",
    "            )) \n",
    "             for w in sug_words]\n",
    "    sum_probs = sum([p[1] for p in probs])\n",
    "    probs = [([p[0]], math.log(p[1]/sum_probs)) for p in probs]\n",
    "    return probs\n",
    "\n",
    "def split_suggestions(sentence):\n",
    "    sent_id, (word, word_sug)  = sentence\n",
    "    return [[sent_id, (word, w)] for w in word_sug]\n",
    "\n",
    "def subs_word_prob(words, tmp_tp, d_tp):\n",
    "    \n",
    "    # unpack values\n",
    "    sent_id = words[0]\n",
    "    cur_word = words[1][0][0]\n",
    "    cur_sug = words[1][0][1][0]\n",
    "    cur_sug_ed = words[1][0][1][1]\n",
    "    prev_sug = words[1][1]\n",
    "    \n",
    "    # belief + transition probability + emission probability\n",
    "    (prob, word) = max((p[1]\n",
    "                 + get_transition_prob(cur_sug, p[0][-1], tmp_tp, d_tp)\n",
    "                 + get_emission_prob(cur_sug_ed), p[0])\n",
    "                     for p in prev_sug)\n",
    "    \n",
    "    return sent_id, (word + [cur_sug], math.exp(prob))\n",
    "\n",
    "def normalize(probs):\n",
    "    sum_probs = sum([p[1] for p in probs])\n",
    "    return [(p[0], math.log(p[1]/sum_probs)) for p in probs]\n",
    "\n",
    "def get_max_path(final_paths):\n",
    "    max_path = max((p[1], p[0]) for p in final_paths)\n",
    "    return max_path[1]\n",
    "\n",
    "def correct_document_context_parallel_steps(fname, dictionary,\n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob):\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # broadcast Python dictionaries to workers\n",
    "    bc_dictionary = sc.broadcast(dictionary)\n",
    "    bc_start_prob = sc.broadcast(start_prob)\n",
    "    bc_transition_prob = sc.broadcast(transition_prob)\n",
    "    \n",
    "    # convert all text to lowercase and drop empty lines\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "        .map(lambda line: line.lower()) \\\n",
    "        .filter(lambda x: x!='')\n",
    "    \n",
    "    regex = re.compile('[^a-z ]')\n",
    "    \n",
    "    # split into sentences -> remove special characters -> convert into list of words\n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()\n",
    "    \n",
    "    # use accumulator to count the number of words checked\n",
    "    accum_total_words = sc.accumulator(0)\n",
    "    split_words = split_sentence.flatMap(lambda x: x) \\\n",
    "            .foreach(lambda x: accum_total_words.add(1))\n",
    "    \n",
    "    # assign each sentence a unique id\n",
    "    sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k)).partitionBy(n_partitions).cache() #\n",
    "    \n",
    "    # count the number of words in each sentence\n",
    "    sentence_word_count = sentence_id.mapValues(lambda v: len(v))\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # spell-checking\n",
    "    #\n",
    "    ############\n",
    "\n",
    "    # number each word in a sentence, and split into individual words\n",
    "    sentence_word_id = sentence_id.mapValues(lambda v: get_sentence_word_id(v)) \\\n",
    "            .flatMap(lambda x: split_sentence_words(x), preservesPartitioning=True)  #\n",
    "    \n",
    "    # get suggestions for each word\n",
    "    sentence_word_suggestions = sentence_word_id.mapValues(lambda v: \n",
    "                                            (v[0], v[1], get_suggestions(v[1], bc_dictionary.value))).cache()\n",
    "    \n",
    "    # filter for the first words in sentences\n",
    "    sentence_word_1 = sentence_word_suggestions.filter(lambda (k, v): v[0]==0) \\\n",
    "            .mapValues(lambda v: (v[1], v[2]))\n",
    "    \n",
    "    # calculate probability for each suggestion\n",
    "    # format: (sentence id, [path-probability pairs])\n",
    "    sentence_path = sentence_word_1.mapValues(lambda v: \n",
    "                                              start_word_prob(v, bc_start_prob.value, default_start_prob))\n",
    "    # start from second word (zero-indexed)\n",
    "    word_num = 1\n",
    "    \n",
    "    # extract any sentences that are ready\n",
    "    completed = sentence_word_count.filter(lambda (k, v): v==word_num) \\\n",
    "            .join(sentence_path).mapValues(lambda v: v[1]).cache() #\n",
    "    \n",
    "    # filter for the next words in sentences\n",
    "    sentence_word_next = sentence_word_suggestions.filter(lambda (k,v): v[0]==word_num) \\\n",
    "            .mapValues(lambda v: (v[1], v[2])).cache()  #\n",
    "    \n",
    "    # check that there are more words left\n",
    "    while not sentence_word_next.isEmpty():\n",
    "\n",
    "        # split into suggestions\n",
    "        sentence_word_next_split = sentence_word_next.flatMap(lambda x: split_suggestions(x), preservesPartitioning=True) #\n",
    "        \n",
    "        # join on previous path\n",
    "        # format: (sentence id, ((current word, (current word suggestion, edit distance)), \n",
    "        #         [(previous path-probability pairs)]))\n",
    "        sentence_word_next_path = sentence_word_next_split.join(sentence_path)\n",
    "        \n",
    "        ## swap to mapvalues\n",
    "        \n",
    "        # calculate path with max probability\n",
    "        sentence_word_next_path_prob = sentence_word_next_path.map(lambda x:\n",
    "                                                subs_word_prob(x, bc_transition_prob.value, default_transition_prob))\n",
    "        \n",
    "        # normalize for numerical stability\n",
    "        sentence_path = sentence_word_next_path_prob.groupByKey().mapValues(lambda v: normalize(v))\n",
    "        \n",
    "        # move on to next word\n",
    "        word_num += 1\n",
    "        \n",
    "        # extract any sentences that are ready\n",
    "        completed = completed.union(\n",
    "            sentence_word_count.filter(lambda (k, v): v==word_num) \\\n",
    "            .join(sentence_path) \\\n",
    "            .mapValues(lambda v: v[1])).cache()  # carry over to next iteration\n",
    "        \n",
    "        # filter for the next words in sentences\n",
    "        sentence_word_next = sentence_word_suggestions.filter(lambda (k, v): v[0]==word_num) \\\n",
    "                .mapValues(lambda v: (v[1], v[2])).cache()  # carry over to next iteration\n",
    "        \n",
    "    # get most likely path (sentence)\n",
    "    sentence_suggestion = completed.mapValues(lambda v: get_max_path(v))\n",
    "\n",
    "    # join with original path (sentence)\n",
    "    sentence_max_prob = sentence_id.join(sentence_suggestion)\n",
    "        \n",
    "    ############\n",
    "    #\n",
    "    # output results\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # count the number of errors per sentence, drop any sentences without errors\n",
    "    sentence_errors = sentence_max_prob.mapValues(lambda v: (get_count_mismatches(v))) \\\n",
    "            .filter(lambda (k, v): v[0]>0).cache()\n",
    "               \n",
    "    # collect all sentences with identified errors\n",
    "    sentence_errors_list = sentence_errors.collect()\n",
    "    \n",
    "    # number of potentially misspelled words\n",
    "    num_errors = sum([s[1][0] for s in sentence_errors_list])\n",
    "    \n",
    "    # print identified errors (eventually output to file)\n",
    "    for sentence in sentence_errors_list:\n",
    "        print 'Sentence %i: %s --> %s' % (sentence[0], ' '.join(sentence[1][1]), ' '.join(sentence[1][2]))\n",
    "    \n",
    "    print '-----'\n",
    "    print 'Total words checked: %i' % accum_total_words.value\n",
    "    print 'Total potential errors found: %i' % num_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "  <strong>SAMPLE OUTPUTS</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "#     parallel_create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 0: ny --> no\n",
      "Line 66: made a th rb ghmhvestigat of the rd nb rg case and --> made a to be ghmhvestigat of the rd and are cases and\n",
      "Line 36: next spoke and said that the fcsf ber and s b ll were --> next spoke and said that he was her and so by all were\n",
      "Line 6: n mnnff t gjpt n p t t --> in off to get on p t it\n",
      "Line 72: bths i --> this i\n",
      "Line 48: hysteria she saij that as soon as it was decided that --> hysteria she said that as soon as it was decided that\n",
      "Line 84: congress in the fight for justice in the i q v berg case --> congress in the fight for justice in the i q v berg came\n",
      "Line 54: bl g --> ll go\n",
      "Line 60: killer in korea by stepping the operation killer of the --> killed in area by stepping the operation killer of the\n",
      "Line 30: he claimed that the conviction of the r sln rgs --> he claimed that the conviction of the r s as\n",
      "Line 1: di taiths --> do this\n",
      "Line 67: that she came to the conclusion that the rosenbt rgs were --> that she came to the conclusion that the present as were\n",
      "Line 37: convicted not because of espionage but f r political --> convicted not because of espionage but of or political\n",
      "Line 79: she read a telegrnm from jillia i patt rson --> she read a telegrnm from till i put on\n",
      "Line 49: the defendants were cornr nists the trial became a massacre --> the defendants here count with the trial became a massacre\n",
      "Line 19: confidential informant t l of unknown reliability --> confidential informant t l or unknown reliability\n",
      "Line 25: b ainin commented that the r se ergs --> b in commented that the r s eyes\n",
      "Line 91: ile stated the way this case was conducted the rosi t rg --> the stated the way this case was conducted to rise to re\n",
      "Line 61: ro e g --> to e g\n",
      "Line 31: and their sentence too death is an eternal shame on american --> and their sentence to death is an eternal shame on american\n",
      "Line 2: cord n --> cord in\n",
      "Line 74: j tchf next spoke and told the people --> j the next spoke and told the people\n",
      "Line 44: ny loo --> no look\n",
      "Line 14: on march l the national c mthiitt to secure --> on march all the national c mthiitt to secure\n",
      "Line 56: ath t k n made q collection speech in which --> it to ask in made q collection speech in which\n",
      "Line 26: were convicted on trumped up evidence and that the main --> were convicted of true but evidence and that the main\n",
      "Line 92: could not get a fnir trial --> could not get as for trial\n",
      "Line 62: they face doatli because they fought for u --> the face doctor because they fought for us\n",
      "Line 3: en raised by the oonipiittee to pay the expenses and --> he raised by the oonipiittee to pay the expenses and\n",
      "Line 39: he claimed thpt the r senbrnrgs wore victims --> he claimed that the r senbrnrgs wore victims\n",
      "Line 9: meetings have been held bh c snc uth c country unuer --> meetings have been held by c and its c country under\n",
      "Line 45: h ththn scbell the i ife of i yktcn sob thl next --> oh the cells they in life of it yktcn so the next\n",
      "Line 15: justice in the r s j b f cas held a meeting at the --> justice in the r s j b if as held a meeting at the\n",
      "Line 81: an organization listed by the attorney general as coming --> an organization listed by the attorney general s coming\n",
      "Line 51: she appealed to the people before we were helping you in --> he appealed to the people before we were helping you in\n",
      "Line 21: was chairman and opened the meeting with greetings jfl the --> was chairman and opened the meeting with greetings of the\n",
      "Line 57: he st ed tti t the r s n rg were being acrific on --> he s end it to the r s in or were being acrific on\n",
      "Line 27: aim of their conviction was to warn the erican people --> aim of their conviction was to warn the german people\n",
      "Line 93: jhy is it that the azi and --> why is it that they are and\n",
      "Line 63: flow we must fight for them --> now we must fight for them\n",
      "Line 34: williaij eu i n writer for the national --> williaij but i in writer or the national\n",
      "Line 4: charges for preparing and filing the printed record and --> change or preparing a feeling the printed record in\n",
      "Line 70: fighti g for progress and a better world --> first go for progress and a better world\n",
      "Line 40: of the cold war of the forces which are trying to plunge --> of the old war of the forces which are trying to plunge\n",
      "Line 46: addressed thi ing nd stated th t her husband and the --> addressed the king and states th to her husband and the\n",
      "Line 16: pythian hall l west th street new york city which --> than half l west the street new york city with\n",
      "Line 82: within the purview of executive rder in which he --> within the view of executive order in which he\n",
      "Line 52: the fight for a better world flow you must help us to free --> the fight for a better world for you must help us to free\n",
      "Line 22: names of ji stice blaci justice douglas eug n debbs and --> names of his voice black justice douglas but no less and\n",
      "Line 88: gor dthethg cor ared the ro enb rg case --> for dthethg nor are the so on or case\n",
      "Line 28: that all holders of unorthodox views are a nenance to thg --> that all holders of unorthodox view are a nenance to the\n",
      "Line 94: fascist spies were net given a death sentence in time of --> fact spies were not given a death sentence in time of\n",
      "Line 65: ith ry van kl ech next spoke and stated that she --> it by an old each next spoke and stated that she\n",
      "Line 35: guardian a newspaper published weekly in new york city --> guardian and newspaper published weekly and new york city\n",
      "Line 5: brief for julius and t tj f d th t --> tried for julius and it to of and th to\n",
      "Line 41: humanity into chaos and fascism --> humanity in chaos and fascism\n",
      "Line 11: raise money for the defense of the os i r g and to rouse --> raise money for the defense of the so i or go and to rouse\n",
      "Line 77: about the ro ijb g c se and fight for their lives and --> about the so i go back so and fight for their lives and\n",
      "Line 47: os b f c are innocent and that they are victims of red --> or b if c are innocent and that they are victims of red\n",
      "Line 17: was attended by approximately oe to people --> has attended by approximately one to people\n",
      "Line 53: my husband and the ro --> my husband and the so\n",
      "Line 89: to the case of sacc and vthnz ti and the dri yfu s case --> to the case of face and that it and the day you s case\n",
      "Line 59: he stated we must stop the operation --> the states we must stop the operation\n",
      "Line 95: war and the o nb g wore given a death sentence in time --> war and the so on go were given a death sentence in time\n",
      "-----\n",
      "Total words checked: 700\n",
      "Total potential errors found: 175\n",
      "CPU times: user 9.96 s, sys: 468 ms, total: 10.4 s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context_parallel_steps('testdata/OCRsample.txt', dictionary,\n",
    "        start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
