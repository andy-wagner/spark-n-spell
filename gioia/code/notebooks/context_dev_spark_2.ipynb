{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_partitions = 6\n",
    "MAX_EDIT_DISTANCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# DOCUMENTATION HERE\n",
    "#\n",
    "######################\n",
    "\n",
    "# number of partitions to be used\n",
    "n_partitions = 6\n",
    "MAX_EDIT_DISTANCE = 3\n",
    "\n",
    "def get_n_deletes_list(w, n):\n",
    "    '''given a word, derive list of strings with up to n characters deleted'''\n",
    "    # since this list is generally of the same magnitude as the number of \n",
    "    # characters in a word, it may not make sense to parallelize this\n",
    "    # so we use python to create the list\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(n):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes\n",
    "\n",
    "def get_transitions(sentence):\n",
    "    if len(sentence)<2:\n",
    "        return None\n",
    "    else:\n",
    "        return [((sentence[i], sentence[i+1]), 1) for i in range(len(sentence)-1)]\n",
    "    \n",
    "def map_transition_prob(x):\n",
    "    vals = x[1]\n",
    "    total = float(sum(vals.values()))\n",
    "    probs = {k: math.log(v/total) for k, v in vals.items()}\n",
    "    return (x[0], probs)\n",
    "\n",
    "def parallel_create_dictionary(fname):\n",
    "    '''\n",
    "    Create dictionary, start probabilities and transition\n",
    "    probabilities using Spark RDDs.\n",
    "    '''\n",
    "    # we generate and count all words for the corpus,\n",
    "    # then add deletes to the dictionary\n",
    "    # this is a slightly different approach from the SymSpell algorithm\n",
    "    # that may be more appropriate for Spark processing\n",
    "    \n",
    "    print 'Creating dictionary...'\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "\n",
    "    # convert file into one long sequence of words\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "            .map(lambda line: line.lower()) \\\n",
    "            .filter(lambda x: x!='').cache()\n",
    "    \n",
    "    # split into individual sentences and remove other punctuation\n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate start probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # only focus on words at the start of sentences\n",
    "    start_words = split_sentence.map(lambda sentence: sentence[0] if len(sentence)>0 else None) \\\n",
    "        .filter(lambda word: word!=None)\n",
    "    \n",
    "    # add a count to each word\n",
    "    count_start_words_once = start_words.map(lambda word: (word, 1)).cache()\n",
    "\n",
    "    # use accumulator to count the number of words at the start of sentences\n",
    "    accum_total_start_words = sc.accumulator(0)\n",
    "    count_total_start_words = count_start_words_once.foreach(lambda x: accum_total_start_words.add(1))\n",
    "    total_start_words = float(accum_total_start_words.value)\n",
    "    \n",
    "    # reduce into count of unique words at the start of sentences\n",
    "    unique_start_words = count_start_words_once.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)\n",
    "    \n",
    "    # convert counts to probabilities\n",
    "    start_prob_calc = unique_start_words.mapValues(lambda v: math.log(v/total_start_words))\n",
    "    \n",
    "    # get default start probabilities (for words not in corpus)\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    \n",
    "    # store start probabilities as a dictionary (will be used as a lookup table)\n",
    "    start_prob = start_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate transition probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # focus on continuous word pairs within the sentence\n",
    "    # e.g. \"this is a test\" -> \"this is\", \"is a\", \"a test\"\n",
    "    # note: as the relevant probability is P(word|previous word)\n",
    "    # the tuples are ordered as (previous word, word)\n",
    "    other_words = split_sentence.map(lambda sentence: get_transitions(sentence)).filter(lambda x: x!=None). \\\n",
    "                flatMap(lambda x: x).cache()\n",
    "\n",
    "    # use accumulator to count the number of transitions\n",
    "    accum_total_other_words = sc.accumulator(0)\n",
    "    count_total_other_words = other_words.foreach(lambda x: accum_total_other_words.add(1))\n",
    "    total_other_words = float(accum_total_other_words.value)\n",
    "    \n",
    "    # reduce into count of unique word pairs\n",
    "    unique_other_words = other_words.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)\n",
    "    \n",
    "    # aggregate by previous word\n",
    "    # i.e. (previous word, [(word1, word1-previous word count), (word2, word2-previous word count), ...])\n",
    "    other_words_collapsed = unique_other_words.map(lambda x: (x[0][0], (x[0][1], x[1]))).groupByKey().mapValues(dict)\n",
    "\n",
    "    # POTENTIAL OPTIMIZATION: FIND AN ALTERNATIVE TO GROUPBYKEY (CREATES ~9.3MB SHUFFLE)\n",
    "    \n",
    "    # convert counts to probabilities\n",
    "    transition_prob_calc = other_words_collapsed.map(lambda x: map_transition_prob(x))\n",
    "    \n",
    "    # get default transition probabilities (for word pairs not in corpus)\n",
    "    default_transition_prob = math.log(1/total_other_words)\n",
    "    \n",
    "    # store transition probabilities as dictionary (will be used as lookup table)\n",
    "    transition_prob = transition_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # process corpus for dictionary\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    replace_nonalphs = make_all_lower.map(lambda line: regex.sub(' ', line))\n",
    "    all_words = replace_nonalphs.flatMap(lambda line: line.split())\n",
    "\n",
    "    # create core corpus dictionary (i.e. only words appearing in file, no \"deletes\") and cache it\n",
    "    # output RDD of unique_words_with_count: [(word1, count1), (word2, count2), (word3, count3)...]\n",
    "    count_once = all_words.map(lambda word: (word, 1))\n",
    "    unique_words_with_count = count_once.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate deletes list\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # generate list of n-deletes from words in a corpus of the form: [(word1, count1), (word2, count2), ...]\n",
    "     \n",
    "    assert MAX_EDIT_DISTANCE > 0  \n",
    "    \n",
    "    generate_deletes = unique_words_with_count.map(lambda (parent, count): \n",
    "                                                   (parent, get_n_deletes_list(parent, MAX_EDIT_DISTANCE)))\n",
    "    expand_deletes = generate_deletes.flatMapValues(lambda x: x)\n",
    "    swap = expand_deletes.map(lambda (orig, delete): (delete, ([orig], 0)))\n",
    "   \n",
    "    ############\n",
    "    #\n",
    "    # combine delete elements with main dictionary\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    corpus = unique_words_with_count.mapValues(lambda count: ([], count))\n",
    "    combine = swap.union(corpus)  # combine deletes with main dictionary, eliminate duplicates\n",
    "    \n",
    "    # since the dictionary will only be a lookup table once created, we can\n",
    "    # pass on as a Python dictionary rather than RDD by reducing locally and\n",
    "    # avoiding an extra shuffle from reduceByKey\n",
    "    dictionary = combine.reduceByKeyLocally(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "\n",
    "    words_processed = unique_words_with_count.map(lambda (k, v): v).reduce(lambda a, b: a + b)\n",
    "    word_count = unique_words_with_count.count()   \n",
    "    \n",
    "    # output stats\n",
    "    print 'Total words processed: %i' % words_processed\n",
    "    print 'Total unique words in corpus: %i' % word_count \n",
    "    print 'Total items in dictionary (corpus words and deletions): %i' % len(dictionary)\n",
    "    print '  Edit distance for deletions: %i' % MAX_EDIT_DISTANCE\n",
    "    print 'Total unique words at the start of a sentence: %i' \\\n",
    "        % len(start_prob)\n",
    "    print 'Total unique word transitions: %i' % len(transition_prob)\n",
    "    \n",
    "    return dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Total words processed: 1105285\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary (corpus words and deletions): 2151998\n",
      "  Edit distance for deletions: 3\n",
      "Total unique words at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n",
      "CPU times: user 11.9 s, sys: 1.24 s, total: 13.1 s\n",
      "Wall time: 58.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "    parallel_create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, longest_word_length=20, \n",
    "                    min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "\n",
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [u'this', u'is', u'a', u'test']),\n",
       " (1, [u'this', u'is', u'a', u'test']),\n",
       " (2, [u'here', u'is', u'a', u'test']),\n",
       " (3, [u'this', u'is', u'ax', u'test']),\n",
       " (4, [u'this', u'is', u'za', u'test'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = \"testdata/test.txt\"\n",
    "\n",
    "# broadcast Python dictionaries to workers\n",
    "bc_dictionary = sc.broadcast(dictionary)\n",
    "bc_start_prob = sc.broadcast(start_prob)\n",
    "bc_transition_prob = sc.broadcast(transition_prob)\n",
    "\n",
    "# convert all text to lowercase and drop empty lines\n",
    "make_all_lower = sc.textFile(fname) \\\n",
    "    .map(lambda line: line.lower()) \\\n",
    "    .filter(lambda x: x!='')\n",
    "\n",
    "regex = re.compile('[^a-z ]')\n",
    "\n",
    "# split into sentences -> remove special characters -> convert into list of words\n",
    "split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "        .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "        .map(lambda sentence: sentence.split()).cache()\n",
    "\n",
    "# use accumulator to count the number of words checked\n",
    "accum_total_words = sc.accumulator(0)\n",
    "split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "\n",
    "# assign each sentence a unique id\n",
    "sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k)).cache()\n",
    "\n",
    "sentence_id.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_word_id(words):\n",
    "    return [(i, w) for i, w in enumerate(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_word_count = sentence_id.mapValues(lambda v: len(v))\n",
    "sentence_word_count.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (0, u'this')),\n",
       " (0, (1, u'is')),\n",
       " (0, (2, u'a')),\n",
       " (0, (3, u'test')),\n",
       " (1, (0, u'this'))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number each word in a sentence, and split into individual words\n",
    "sentence_word_id = sentence_id.mapValues(lambda v: get_sentence_word_id(v)).flatMapValues(lambda x: x)\n",
    "sentence_word_id.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (0,\n",
       "   u'this',\n",
       "   [(u'this', 0),\n",
       "    (u'his', 1),\n",
       "    (u'thus', 1),\n",
       "    (u'thin', 1),\n",
       "    (u'the', 2),\n",
       "    (u'that', 2),\n",
       "    (u'is', 2),\n",
       "    (u'him', 2),\n",
       "    (u'they', 2),\n",
       "    (u'their', 2)])),\n",
       " (0,\n",
       "  (1,\n",
       "   u'is',\n",
       "   [(u'is', 0),\n",
       "    (u'in', 1),\n",
       "    (u'it', 1),\n",
       "    (u'his', 1),\n",
       "    (u'as', 1),\n",
       "    (u'i', 1),\n",
       "    (u's', 1),\n",
       "    (u'if', 1),\n",
       "    (u'its', 1),\n",
       "    (u'us', 1)])),\n",
       " (0,\n",
       "  (2,\n",
       "   u'a',\n",
       "   [(u'a', 0),\n",
       "    (u'as', 1),\n",
       "    (u'at', 1),\n",
       "    (u'an', 1),\n",
       "    (u'am', 1),\n",
       "    (u'ah', 1),\n",
       "    (u'and', 2),\n",
       "    (u'was', 2),\n",
       "    (u'had', 2),\n",
       "    (u'all', 2)])),\n",
       " (0,\n",
       "  (3,\n",
       "   u'test',\n",
       "   [(u'test', 0),\n",
       "    (u'west', 1),\n",
       "    (u'best', 1),\n",
       "    (u'rest', 1),\n",
       "    (u'that', 2),\n",
       "    (u'these', 2),\n",
       "    (u'went', 2),\n",
       "    (u'must', 2),\n",
       "    (u'most', 2),\n",
       "    (u'left', 2)])),\n",
       " (1,\n",
       "  (0,\n",
       "   u'this',\n",
       "   [(u'this', 0),\n",
       "    (u'his', 1),\n",
       "    (u'thus', 1),\n",
       "    (u'thin', 1),\n",
       "    (u'the', 2),\n",
       "    (u'that', 2),\n",
       "    (u'is', 2),\n",
       "    (u'him', 2),\n",
       "    (u'they', 2),\n",
       "    (u'their', 2)]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get suggestions for each word\n",
    "sentence_word_suggestions = sentence_word_id.mapValues(lambda v: \n",
    "                                                       (v[0], v[1], get_suggestions(v[1], bc_dictionary.value))).cache()\n",
    "sentence_word_suggestions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (u'this',\n",
       "   [(u'this', 0),\n",
       "    (u'his', 1),\n",
       "    (u'thus', 1),\n",
       "    (u'thin', 1),\n",
       "    (u'the', 2),\n",
       "    (u'that', 2),\n",
       "    (u'is', 2),\n",
       "    (u'him', 2),\n",
       "    (u'they', 2),\n",
       "    (u'their', 2)])),\n",
       " (1,\n",
       "  (u'this',\n",
       "   [(u'this', 0),\n",
       "    (u'his', 1),\n",
       "    (u'thus', 1),\n",
       "    (u'thin', 1),\n",
       "    (u'the', 2),\n",
       "    (u'that', 2),\n",
       "    (u'is', 2),\n",
       "    (u'him', 2),\n",
       "    (u'they', 2),\n",
       "    (u'their', 2)])),\n",
       " (2,\n",
       "  (u'here',\n",
       "   [(u'here', 0),\n",
       "    (u'her', 1),\n",
       "    (u'were', 1),\n",
       "    (u'there', 1),\n",
       "    (u'where', 1),\n",
       "    (u'he', 2),\n",
       "    (u'are', 2),\n",
       "    (u'have', 2),\n",
       "    (u'more', 2),\n",
       "    (u'very', 2)])),\n",
       " (3,\n",
       "  (u'this',\n",
       "   [(u'this', 0),\n",
       "    (u'his', 1),\n",
       "    (u'thus', 1),\n",
       "    (u'thin', 1),\n",
       "    (u'the', 2),\n",
       "    (u'that', 2),\n",
       "    (u'is', 2),\n",
       "    (u'him', 2),\n",
       "    (u'they', 2),\n",
       "    (u'their', 2)])),\n",
       " (4,\n",
       "  (u'this',\n",
       "   [(u'this', 0),\n",
       "    (u'his', 1),\n",
       "    (u'thus', 1),\n",
       "    (u'thin', 1),\n",
       "    (u'the', 2),\n",
       "    (u'that', 2),\n",
       "    (u'is', 2),\n",
       "    (u'him', 2),\n",
       "    (u'they', 2),\n",
       "    (u'their', 2)]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for the first words in sentences\n",
    "sentence_word_1 = sentence_word_suggestions.filter(lambda (k, v): v[0]==0).mapValues(lambda v: (v[1], v[2]))\n",
    "sentence_word_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start_word_prob(words, tmp_sp, d_sp):\n",
    "    orig_word, sug_words = words\n",
    "    probs = [(w[0], \n",
    "              math.exp(get_start_prob(w[0], tmp_sp, d_sp) + get_emission_prob(w[1]))\n",
    "             ) \n",
    "             for w in sug_words]\n",
    "    sum_probs = sum([p[1] for p in probs])\n",
    "    probs = [([p[0]], math.log(p[1]/sum_probs)) for p in probs]\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [([u'this'], -0.010416136127922377),\n",
       "   ([u'his'], -4.751501506095812),\n",
       "   ([u'thus'], -7.051702807734582),\n",
       "   ([u'thin'], -8.997612956789895),\n",
       "   ([u'the'], -7.479924902965362),\n",
       "   ([u'that'], -9.697449125500638),\n",
       "   ([u'is'], -10.680518021105867),\n",
       "   ([u'him'], -11.02509475953902),\n",
       "   ([u'they'], -9.835785909400098),\n",
       "   ([u'their'], -11.093861616786112)]),\n",
       " (1,\n",
       "  [([u'this'], -0.010416136127922377),\n",
       "   ([u'his'], -4.751501506095812),\n",
       "   ([u'thus'], -7.051702807734582),\n",
       "   ([u'thin'], -8.997612956789895),\n",
       "   ([u'the'], -7.479924902965362),\n",
       "   ([u'that'], -9.697449125500638),\n",
       "   ([u'is'], -10.680518021105867),\n",
       "   ([u'him'], -11.02509475953902),\n",
       "   ([u'they'], -9.835785909400098),\n",
       "   ([u'their'], -11.093861616786112)]),\n",
       " (2,\n",
       "  [([u'here'], -0.11732116430434793),\n",
       "   ([u'her'], -3.6949749833080596),\n",
       "   ([u'were'], -3.9618229688569984),\n",
       "   ([u'there'], -2.9282838994257436),\n",
       "   ([u'where'], -4.404037619173904),\n",
       "   ([u'he'], -7.042132844914638),\n",
       "   ([u'are'], -9.595925522875207),\n",
       "   ([u'have'], -9.558185194892362),\n",
       "   ([u'more'], -9.852059237592064),\n",
       "   ([u'very'], -10.08143333865691)]),\n",
       " (3,\n",
       "  [([u'this'], -0.010416136127922377),\n",
       "   ([u'his'], -4.751501506095812),\n",
       "   ([u'thus'], -7.051702807734582),\n",
       "   ([u'thin'], -8.997612956789895),\n",
       "   ([u'the'], -7.479924902965362),\n",
       "   ([u'that'], -9.697449125500638),\n",
       "   ([u'is'], -10.680518021105867),\n",
       "   ([u'him'], -11.02509475953902),\n",
       "   ([u'they'], -9.835785909400098),\n",
       "   ([u'their'], -11.093861616786112)]),\n",
       " (4,\n",
       "  [([u'this'], -0.010416136127922377),\n",
       "   ([u'his'], -4.751501506095812),\n",
       "   ([u'thus'], -7.051702807734582),\n",
       "   ([u'thin'], -8.997612956789895),\n",
       "   ([u'the'], -7.479924902965362),\n",
       "   ([u'that'], -9.697449125500638),\n",
       "   ([u'is'], -10.680518021105867),\n",
       "   ([u'him'], -11.02509475953902),\n",
       "   ([u'they'], -9.835785909400098),\n",
       "   ([u'their'], -11.093861616786112)])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate probability for each suggestion\n",
    "# format: (sentence id, [path-probability pairs])\n",
    "sentence_path = sentence_word_1.mapValues(lambda v: start_word_prob(v, bc_start_prob.value, default_start_prob))\n",
    "sentence_path.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###LOOP STARTS HERE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_num = 1\n",
    "word_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed = sentence_word_count.filter(lambda (k, v): v==word_num).join(sentence_path).mapValues(lambda v: v[1])\n",
    "completed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (u'is',\n",
       "   [(u'is', 0),\n",
       "    (u'in', 1),\n",
       "    (u'it', 1),\n",
       "    (u'his', 1),\n",
       "    (u'as', 1),\n",
       "    (u'i', 1),\n",
       "    (u's', 1),\n",
       "    (u'if', 1),\n",
       "    (u'its', 1),\n",
       "    (u'us', 1)])),\n",
       " (1,\n",
       "  (u'is',\n",
       "   [(u'is', 0),\n",
       "    (u'in', 1),\n",
       "    (u'it', 1),\n",
       "    (u'his', 1),\n",
       "    (u'as', 1),\n",
       "    (u'i', 1),\n",
       "    (u's', 1),\n",
       "    (u'if', 1),\n",
       "    (u'its', 1),\n",
       "    (u'us', 1)])),\n",
       " (2,\n",
       "  (u'is',\n",
       "   [(u'is', 0),\n",
       "    (u'in', 1),\n",
       "    (u'it', 1),\n",
       "    (u'his', 1),\n",
       "    (u'as', 1),\n",
       "    (u'i', 1),\n",
       "    (u's', 1),\n",
       "    (u'if', 1),\n",
       "    (u'its', 1),\n",
       "    (u'us', 1)])),\n",
       " (3,\n",
       "  (u'is',\n",
       "   [(u'is', 0),\n",
       "    (u'in', 1),\n",
       "    (u'it', 1),\n",
       "    (u'his', 1),\n",
       "    (u'as', 1),\n",
       "    (u'i', 1),\n",
       "    (u's', 1),\n",
       "    (u'if', 1),\n",
       "    (u'its', 1),\n",
       "    (u'us', 1)])),\n",
       " (4,\n",
       "  (u'is',\n",
       "   [(u'is', 0),\n",
       "    (u'in', 1),\n",
       "    (u'it', 1),\n",
       "    (u'his', 1),\n",
       "    (u'as', 1),\n",
       "    (u'i', 1),\n",
       "    (u's', 1),\n",
       "    (u'if', 1),\n",
       "    (u'its', 1),\n",
       "    (u'us', 1)]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for the next words in sentences\n",
    "sentence_word_next = sentence_word_suggestions.filter(lambda (k, v): v[0]==word_num).mapValues(lambda v: (v[1], v[2]))\n",
    "sentence_word_next.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that there are more words left\n",
    "sentence_word_next.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_suggestions(sentence):\n",
    "    sent_id, (word, word_sug)  = sentence\n",
    "    return [[sent_id, (word, w)] for w in word_sug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, (u'is', (u'is', 0))],\n",
       " [0, (u'is', (u'in', 1))],\n",
       " [0, (u'is', (u'it', 1))],\n",
       " [0, (u'is', (u'his', 1))],\n",
       " [0, (u'is', (u'as', 1))]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into suggestions\n",
    "sentence_word_next_split = sentence_word_next.flatMap(lambda x: split_suggestions(x))\n",
    "sentence_word_next_split.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ((u'is', (u'is', 0)),\n",
       "   [([u'this'], -0.010416136127922377),\n",
       "    ([u'his'], -4.751501506095812),\n",
       "    ([u'thus'], -7.051702807734582),\n",
       "    ([u'thin'], -8.997612956789895),\n",
       "    ([u'the'], -7.479924902965362),\n",
       "    ([u'that'], -9.697449125500638),\n",
       "    ([u'is'], -10.680518021105867),\n",
       "    ([u'him'], -11.02509475953902),\n",
       "    ([u'they'], -9.835785909400098),\n",
       "    ([u'their'], -11.093861616786112)])),\n",
       " (0,\n",
       "  ((u'is', (u'in', 1)),\n",
       "   [([u'this'], -0.010416136127922377),\n",
       "    ([u'his'], -4.751501506095812),\n",
       "    ([u'thus'], -7.051702807734582),\n",
       "    ([u'thin'], -8.997612956789895),\n",
       "    ([u'the'], -7.479924902965362),\n",
       "    ([u'that'], -9.697449125500638),\n",
       "    ([u'is'], -10.680518021105867),\n",
       "    ([u'him'], -11.02509475953902),\n",
       "    ([u'they'], -9.835785909400098),\n",
       "    ([u'their'], -11.093861616786112)])),\n",
       " (0,\n",
       "  ((u'is', (u'it', 1)),\n",
       "   [([u'this'], -0.010416136127922377),\n",
       "    ([u'his'], -4.751501506095812),\n",
       "    ([u'thus'], -7.051702807734582),\n",
       "    ([u'thin'], -8.997612956789895),\n",
       "    ([u'the'], -7.479924902965362),\n",
       "    ([u'that'], -9.697449125500638),\n",
       "    ([u'is'], -10.680518021105867),\n",
       "    ([u'him'], -11.02509475953902),\n",
       "    ([u'they'], -9.835785909400098),\n",
       "    ([u'their'], -11.093861616786112)])),\n",
       " (0,\n",
       "  ((u'is', (u'his', 1)),\n",
       "   [([u'this'], -0.010416136127922377),\n",
       "    ([u'his'], -4.751501506095812),\n",
       "    ([u'thus'], -7.051702807734582),\n",
       "    ([u'thin'], -8.997612956789895),\n",
       "    ([u'the'], -7.479924902965362),\n",
       "    ([u'that'], -9.697449125500638),\n",
       "    ([u'is'], -10.680518021105867),\n",
       "    ([u'him'], -11.02509475953902),\n",
       "    ([u'they'], -9.835785909400098),\n",
       "    ([u'their'], -11.093861616786112)])),\n",
       " (0,\n",
       "  ((u'is', (u'as', 1)),\n",
       "   [([u'this'], -0.010416136127922377),\n",
       "    ([u'his'], -4.751501506095812),\n",
       "    ([u'thus'], -7.051702807734582),\n",
       "    ([u'thin'], -8.997612956789895),\n",
       "    ([u'the'], -7.479924902965362),\n",
       "    ([u'that'], -9.697449125500638),\n",
       "    ([u'is'], -10.680518021105867),\n",
       "    ([u'him'], -11.02509475953902),\n",
       "    ([u'they'], -9.835785909400098),\n",
       "    ([u'their'], -11.093861616786112)]))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join on previous path\n",
    "# format: (sentence id, ((current word, (current word suggestion, edit distance)), \n",
    "#         [(previous path-probability pairs)]))\n",
    "sentence_word_next_path = sentence_word_next_split.join(sentence_path)\n",
    "sentence_word_next_path.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subs_word_prob(words, tmp_tp, d_tp):\n",
    "    \n",
    "    # unpack values\n",
    "    cur_word = words[0][0]\n",
    "    cur_sug = words[0][1][0]\n",
    "    cur_sug_ed = words[0][1][1]\n",
    "    prev_sug = words[1]\n",
    "    \n",
    "    # belief + transition probability + emission probability\n",
    "    (prob, word) = max((p[1]\n",
    "                 + get_transition_prob(cur_sug, p[0][-1], tmp_tp, d_tp)\n",
    "                 + get_emission_prob(cur_sug_ed), p[0])\n",
    "                     for p in prev_sug)\n",
    "    \n",
    "    return word + [cur_sug], math.exp(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([u'this', u'is'], 0.07965129629236094)),\n",
       " (0, ([u'this', u'in'], 4.52866901996701e-05)),\n",
       " (0, ([u'this', u'it'], 1.0655691811687093e-05)),\n",
       " (0, ([u'this', u'his'], 7.991768858765314e-06)),\n",
       " (0, ([u'this', u'as'], 4.7950613152591945e-05))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate path with max probability\n",
    "sentence_word_next_path_prob = sentence_word_next_path.mapValues(lambda v:\n",
    "                                        subs_word_prob(v, bc_transition_prob.value, default_transition_prob))\n",
    "sentence_word_next_path_prob.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(probs):\n",
    "    sum_probs = sum([p[1] for p in probs])\n",
    "    return [(p[0], math.log(p[1]/sum_probs)) for p in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [([u'this', u'is'], -0.001838225822312142),\n",
       "   ([u'this', u'in'], -7.474238641144874),\n",
       "   ([u'this', u'it'], -8.921157624081198),\n",
       "   ([u'this', u'his'], -9.20883969653298),\n",
       "   ([u'this', u'as'], -7.417080227304924),\n",
       "   ([u'this', u'i'], -7.822545335413089),\n",
       "   ([u'that', u's'], -15.439796609526178),\n",
       "   ([u'this', u'if'], -10.307451985201089),\n",
       "   ([u'this', u'its'], -15.880533930581192),\n",
       "   ([u'this', u'us'], -15.880533930581192)]),\n",
       " (4,\n",
       "  [([u'this', u'is'], -0.001838225822312142),\n",
       "   ([u'this', u'in'], -7.474238641144874),\n",
       "   ([u'this', u'it'], -8.921157624081198),\n",
       "   ([u'this', u'his'], -9.20883969653298),\n",
       "   ([u'this', u'as'], -7.417080227304924),\n",
       "   ([u'this', u'i'], -7.822545335413089),\n",
       "   ([u'that', u's'], -15.439796609526178),\n",
       "   ([u'this', u'if'], -10.307451985201089),\n",
       "   ([u'this', u'its'], -15.880533930581192),\n",
       "   ([u'this', u'us'], -15.880533930581192)]),\n",
       " (1,\n",
       "  [([u'this', u'is'], -0.001838225822312142),\n",
       "   ([u'this', u'in'], -7.474238641144874),\n",
       "   ([u'this', u'it'], -8.921157624081198),\n",
       "   ([u'this', u'his'], -9.20883969653298),\n",
       "   ([u'this', u'as'], -7.417080227304924),\n",
       "   ([u'this', u'i'], -7.822545335413089),\n",
       "   ([u'that', u's'], -15.439796609526178),\n",
       "   ([u'this', u'if'], -10.307451985201089),\n",
       "   ([u'this', u'its'], -15.880533930581192),\n",
       "   ([u'this', u'us'], -15.880533930581192)]),\n",
       " (5,\n",
       "  [([u'there', u'is'], -0.004274714851496214),\n",
       "   ([u'thee', u'in'], -7.373143092463868),\n",
       "   ([u'then', u'it'], -7.663392192916442),\n",
       "   ([u'then', u'his'], -8.900154820065369),\n",
       "   ([u'then', u'as'], -8.264166053345372),\n",
       "   ([u'thee', u'i'], -6.679995911903921),\n",
       "   ([u'there', u's'], -6.584746183266825),\n",
       "   ([u'then', u'if'], -9.151469248346276),\n",
       "   ([u'the', u'its'], -14.381166017155921),\n",
       "   ([u'the', u'us'], -10.402910589948872)]),\n",
       " (2,\n",
       "  [([u'here', u'is'], -0.01879679481383516),\n",
       "   ([u'here', u'in'], -5.199331125705488),\n",
       "   ([u'here', u'it'], -5.550729012543377),\n",
       "   ([u'where', u'his'], -11.142336732673904),\n",
       "   ([u'here', u'as'], -6.192582898715772),\n",
       "   ([u'here', u'i'], -6.192582898715772),\n",
       "   ([u'here', u's'], -5.359673775780667),\n",
       "   ([u'here', u'if'], -7.802020811149873),\n",
       "   ([u'were', u'its'], -13.537436944595711),\n",
       "   ([u'here', u'us'], -15.89825180535442)]),\n",
       " (6,\n",
       "  [([u'her', u'the'], -0.12135666115371971),\n",
       "   ([u'her', u'see'], -2.8294068622559303),\n",
       "   ([u'her', u'ten'], -8.906225245412237),\n",
       "   ([u'her', u'tea'], -3.522554042815874),\n",
       "   ([u'her', u'to'], -4.1295235271347694),\n",
       "   ([u'her', u'he'], -5.642817579015966),\n",
       "   ([u'he', u'be'], -10.21949099950891),\n",
       "   ([u'her', u'her'], -8.127724228803967),\n",
       "   ([u'her', u'were'], -8.82087140936391),\n",
       "   ([u'her', u'she'], -5.209953496719689)]),\n",
       " (3,\n",
       "  [([u'this', u'is'], -0.001838225822312142),\n",
       "   ([u'this', u'in'], -7.474238641144874),\n",
       "   ([u'this', u'it'], -8.921157624081198),\n",
       "   ([u'this', u'his'], -9.20883969653298),\n",
       "   ([u'this', u'as'], -7.417080227304924),\n",
       "   ([u'this', u'i'], -7.822545335413089),\n",
       "   ([u'that', u's'], -15.439796609526178),\n",
       "   ([u'this', u'if'], -10.307451985201089),\n",
       "   ([u'this', u'its'], -15.880533930581192),\n",
       "   ([u'this', u'us'], -15.880533930581192)])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize for numerical stability\n",
    "sentence_path = sentence_word_next_path_prob.groupByKey().mapValues(lambda v: normalize(v))\n",
    "sentence_path.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_num += 1\n",
    "word_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed = completed.union(\n",
    "    sentence_word_count.filter(lambda (k, v): v==word_num).join(sentence_path).mapValues(lambda v: v[1]))\n",
    "completed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (u'a',\n",
       "   [(u'a', 0),\n",
       "    (u'as', 1),\n",
       "    (u'at', 1),\n",
       "    (u'an', 1),\n",
       "    (u'am', 1),\n",
       "    (u'ah', 1),\n",
       "    (u'and', 2),\n",
       "    (u'was', 2),\n",
       "    (u'had', 2),\n",
       "    (u'all', 2)])),\n",
       " (1,\n",
       "  (u'a',\n",
       "   [(u'a', 0),\n",
       "    (u'as', 1),\n",
       "    (u'at', 1),\n",
       "    (u'an', 1),\n",
       "    (u'am', 1),\n",
       "    (u'ah', 1),\n",
       "    (u'and', 2),\n",
       "    (u'was', 2),\n",
       "    (u'had', 2),\n",
       "    (u'all', 2)])),\n",
       " (2,\n",
       "  (u'a',\n",
       "   [(u'a', 0),\n",
       "    (u'as', 1),\n",
       "    (u'at', 1),\n",
       "    (u'an', 1),\n",
       "    (u'am', 1),\n",
       "    (u'ah', 1),\n",
       "    (u'and', 2),\n",
       "    (u'was', 2),\n",
       "    (u'had', 2),\n",
       "    (u'all', 2)])),\n",
       " (3,\n",
       "  (u'ax',\n",
       "   [(u'ax', 0),\n",
       "    (u'a', 1),\n",
       "    (u'as', 1),\n",
       "    (u'at', 1),\n",
       "    (u'an', 1),\n",
       "    (u'am', 1),\n",
       "    (u'ah', 1),\n",
       "    (u'x', 1),\n",
       "    (u'and', 2),\n",
       "    (u'was', 2)])),\n",
       " (4,\n",
       "  (u'za',\n",
       "   [(u'a', 1),\n",
       "    (u'was', 2),\n",
       "    (u'as', 2),\n",
       "    (u'had', 2),\n",
       "    (u'at', 2),\n",
       "    (u'an', 2),\n",
       "    (u'may', 2),\n",
       "    (u'man', 2),\n",
       "    (u'has', 2),\n",
       "    (u'can', 2)])),\n",
       " (5,\n",
       "  (u'a',\n",
       "   [(u'a', 0),\n",
       "    (u'as', 1),\n",
       "    (u'at', 1),\n",
       "    (u'an', 1),\n",
       "    (u'am', 1),\n",
       "    (u'ah', 1),\n",
       "    (u'and', 2),\n",
       "    (u'was', 2),\n",
       "    (u'had', 2),\n",
       "    (u'all', 2)])),\n",
       " (6,\n",
       "  (u'set',\n",
       "   [(u'set', 0),\n",
       "    (u'see', 1),\n",
       "    (u'met', 1),\n",
       "    (u'let', 1),\n",
       "    (u'yet', 1),\n",
       "    (u'get', 1),\n",
       "    (u'sat', 1),\n",
       "    (u'sent', 1),\n",
       "    (u'seat', 1),\n",
       "    (u'st', 1)]))]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for the next words in sentences\n",
    "sentence_word_next = sentence_word_suggestions.filter(lambda (k, v): v[0]==word_num).mapValues(lambda v: (v[1], v[2]))\n",
    "sentence_word_next.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that there are more words left\n",
    "sentence_word_next.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, (u'a', (u'a', 0))],\n",
       " [0, (u'a', (u'as', 1))],\n",
       " [0, (u'a', (u'at', 1))],\n",
       " [0, (u'a', (u'an', 1))],\n",
       " [0, (u'a', (u'am', 1))]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into suggestions\n",
    "sentence_word_next_split = sentence_word_next.flatMap(lambda x: split_suggestions(x))\n",
    "sentence_word_next_split.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ((u'a', (u'a', 0)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'a', (u'as', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'a', (u'at', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'a', (u'an', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'a', (u'am', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)]))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join on previous path\n",
    "# format: (sentence id, ((current word, (current word suggestion, edit distance)), \n",
    "#         [(previous path-probability pairs)]))\n",
    "sentence_word_next_path = sentence_word_next_split.join(sentence_path)\n",
    "sentence_word_next_path.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([u'this', u'is', u'a'], 0.06417656065436952)),\n",
       " (0, ([u'this', u'is', u'as'], 4.482519568703837e-05)),\n",
       " (0, ([u'this', u'is', u'at'], 4.263860077547551e-05)),\n",
       " (0, ([u'this', u'is', u'an'], 0.00012026272013595663)),\n",
       " (0, ([u'this', u'i', u'am'], 3.3681971618496976e-07))]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate path with max probability\n",
    "sentence_word_next_path_prob = sentence_word_next_path.mapValues(lambda v:\n",
    "                                        subs_word_prob(v, bc_transition_prob.value, default_transition_prob))\n",
    "sentence_word_next_path_prob.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [([u'this', u'is', u'a'], -0.003259988991643064),\n",
       "   ([u'this', u'is', u'as'], -7.269882928103522),\n",
       "   ([u'this', u'is', u'at'], -7.319893348678184),\n",
       "   ([u'this', u'is', u'an'], -6.282974629015413),\n",
       "   ([u'this', u'i', u'am'], -12.160860771749043),\n",
       "   ([u'this', u'is', u'ah'], -10.983454994807829),\n",
       "   ([u'this', u'is', u'and'], -13.573722160253656),\n",
       "   ([u'this', u'is', u'was'], -15.588625180795919),\n",
       "   ([u'this', u'i', u'had'], -18.518446249400366),\n",
       "   ([u'this', u'is', u'all'], -12.256420670620717)]),\n",
       " (6,\n",
       "  [([u'her', u'to', u'set'], -0.7073236573108231),\n",
       "   ([u'her', u'to', u'see'], -2.8216524579910995),\n",
       "   ([u'her', u'she', u'met'], -5.43068860495902),\n",
       "   ([u'her', u'to', u'let'], -4.517563968429027),\n",
       "   ([u'her', u'the', u'yet'], -8.229793963975347),\n",
       "   ([u'her', u'to', u'get'], -3.4029513384144745),\n",
       "   ([u'her', u'she', u'sat'], -4.365977867966592),\n",
       "   ([u'her', u'he', u'sent'], -6.536828443436029),\n",
       "   ([u'her', u'the', u'seat'], -1.0631219193848052),\n",
       "   ([u'her', u'the', u'st'], -3.2399376250898158)]),\n",
       " (1,\n",
       "  [([u'this', u'is', u'a'], -0.003259988991643064),\n",
       "   ([u'this', u'is', u'as'], -7.269882928103522),\n",
       "   ([u'this', u'is', u'at'], -7.319893348678184),\n",
       "   ([u'this', u'is', u'an'], -6.282974629015413),\n",
       "   ([u'this', u'i', u'am'], -12.160860771749043),\n",
       "   ([u'this', u'is', u'ah'], -10.983454994807829),\n",
       "   ([u'this', u'is', u'and'], -13.573722160253656),\n",
       "   ([u'this', u'is', u'was'], -15.588625180795919),\n",
       "   ([u'this', u'i', u'had'], -18.518446249400366),\n",
       "   ([u'this', u'is', u'all'], -12.256420670620717)]),\n",
       " (2,\n",
       "  [([u'here', u'is', u'a'], -0.003282265855188137),\n",
       "   ([u'here', u'is', u'as'], -7.269905204967068),\n",
       "   ([u'here', u'is', u'at'], -7.3199156255417295),\n",
       "   ([u'here', u'is', u'an'], -6.282996905878959),\n",
       "   ([u'here', u'i', u'am'], -10.51396204292375),\n",
       "   ([u'here', u'is', u'ah'], -10.983477271671374),\n",
       "   ([u'here', u'is', u'and'], -13.5737444371172),\n",
       "   ([u'here', u'it', u'was'], -14.555757589368744),\n",
       "   ([u'here', u'it', u'had'], -16.71661452253409),\n",
       "   ([u'here', u'is', u'all'], -12.256442947484262)]),\n",
       " (3,\n",
       "  [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "   ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "   ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "   ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "   ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "   ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "   ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "   ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "   ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "   ([u'this', u'is', u'was'], -11.263612447240725)])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize for numerical stability\n",
    "sentence_path = sentence_word_next_path_prob.groupByKey().mapValues(lambda v: normalize(v))\n",
    "sentence_path.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_num += 1\n",
    "word_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  [([u'her', u'to', u'set'], -0.7073236573108231),\n",
       "   ([u'her', u'to', u'see'], -2.8216524579910995),\n",
       "   ([u'her', u'she', u'met'], -5.43068860495902),\n",
       "   ([u'her', u'to', u'let'], -4.517563968429027),\n",
       "   ([u'her', u'the', u'yet'], -8.229793963975347),\n",
       "   ([u'her', u'to', u'get'], -3.4029513384144745),\n",
       "   ([u'her', u'she', u'sat'], -4.365977867966592),\n",
       "   ([u'her', u'he', u'sent'], -6.536828443436029),\n",
       "   ([u'her', u'the', u'seat'], -1.0631219193848052),\n",
       "   ([u'her', u'the', u'st'], -3.2399376250898158)])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed = completed.union(\n",
    "    sentence_word_count.filter(lambda (k, v): v==word_num).join(sentence_path).mapValues(lambda v: v[1]))\n",
    "completed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (u'test',\n",
       "   [(u'test', 0),\n",
       "    (u'west', 1),\n",
       "    (u'best', 1),\n",
       "    (u'rest', 1),\n",
       "    (u'that', 2),\n",
       "    (u'these', 2),\n",
       "    (u'went', 2),\n",
       "    (u'must', 2),\n",
       "    (u'most', 2),\n",
       "    (u'left', 2)])),\n",
       " (1,\n",
       "  (u'test',\n",
       "   [(u'test', 0),\n",
       "    (u'west', 1),\n",
       "    (u'best', 1),\n",
       "    (u'rest', 1),\n",
       "    (u'that', 2),\n",
       "    (u'these', 2),\n",
       "    (u'went', 2),\n",
       "    (u'must', 2),\n",
       "    (u'most', 2),\n",
       "    (u'left', 2)])),\n",
       " (2,\n",
       "  (u'test',\n",
       "   [(u'test', 0),\n",
       "    (u'west', 1),\n",
       "    (u'best', 1),\n",
       "    (u'rest', 1),\n",
       "    (u'that', 2),\n",
       "    (u'these', 2),\n",
       "    (u'went', 2),\n",
       "    (u'must', 2),\n",
       "    (u'most', 2),\n",
       "    (u'left', 2)])),\n",
       " (3,\n",
       "  (u'test',\n",
       "   [(u'test', 0),\n",
       "    (u'west', 1),\n",
       "    (u'best', 1),\n",
       "    (u'rest', 1),\n",
       "    (u'that', 2),\n",
       "    (u'these', 2),\n",
       "    (u'went', 2),\n",
       "    (u'must', 2),\n",
       "    (u'most', 2),\n",
       "    (u'left', 2)])),\n",
       " (4,\n",
       "  (u'test',\n",
       "   [(u'test', 0),\n",
       "    (u'west', 1),\n",
       "    (u'best', 1),\n",
       "    (u'rest', 1),\n",
       "    (u'that', 2),\n",
       "    (u'these', 2),\n",
       "    (u'went', 2),\n",
       "    (u'must', 2),\n",
       "    (u'most', 2),\n",
       "    (u'left', 2)]))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_word_next = sentence_word_suggestions.filter(lambda (k, v): v[0]==word_num) \\\n",
    "                .mapValues(lambda v: (v[1], v[2]))\n",
    "sentence_word_next.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, (u'test', (u'test', 0))],\n",
       " [0, (u'test', (u'west', 1))],\n",
       " [0, (u'test', (u'best', 1))],\n",
       " [0, (u'test', (u'rest', 1))],\n",
       " [0, (u'test', (u'that', 2))]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_word_next_split = sentence_word_next.flatMap(lambda x: split_suggestions(x))\n",
    "sentence_word_next_split.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ((u'test', (u'test', 0)),\n",
       "   [([u'this', u'is', u'a'], -0.003259988991643064),\n",
       "    ([u'this', u'is', u'as'], -7.269882928103522),\n",
       "    ([u'this', u'is', u'at'], -7.319893348678184),\n",
       "    ([u'this', u'is', u'an'], -6.282974629015413),\n",
       "    ([u'this', u'i', u'am'], -12.160860771749043),\n",
       "    ([u'this', u'is', u'ah'], -10.983454994807829),\n",
       "    ([u'this', u'is', u'and'], -13.573722160253656),\n",
       "    ([u'this', u'is', u'was'], -15.588625180795919),\n",
       "    ([u'this', u'i', u'had'], -18.518446249400366),\n",
       "    ([u'this', u'is', u'all'], -12.256420670620717)])),\n",
       " (0,\n",
       "  ((u'test', (u'west', 1)),\n",
       "   [([u'this', u'is', u'a'], -0.003259988991643064),\n",
       "    ([u'this', u'is', u'as'], -7.269882928103522),\n",
       "    ([u'this', u'is', u'at'], -7.319893348678184),\n",
       "    ([u'this', u'is', u'an'], -6.282974629015413),\n",
       "    ([u'this', u'i', u'am'], -12.160860771749043),\n",
       "    ([u'this', u'is', u'ah'], -10.983454994807829),\n",
       "    ([u'this', u'is', u'and'], -13.573722160253656),\n",
       "    ([u'this', u'is', u'was'], -15.588625180795919),\n",
       "    ([u'this', u'i', u'had'], -18.518446249400366),\n",
       "    ([u'this', u'is', u'all'], -12.256420670620717)])),\n",
       " (0,\n",
       "  ((u'test', (u'best', 1)),\n",
       "   [([u'this', u'is', u'a'], -0.003259988991643064),\n",
       "    ([u'this', u'is', u'as'], -7.269882928103522),\n",
       "    ([u'this', u'is', u'at'], -7.319893348678184),\n",
       "    ([u'this', u'is', u'an'], -6.282974629015413),\n",
       "    ([u'this', u'i', u'am'], -12.160860771749043),\n",
       "    ([u'this', u'is', u'ah'], -10.983454994807829),\n",
       "    ([u'this', u'is', u'and'], -13.573722160253656),\n",
       "    ([u'this', u'is', u'was'], -15.588625180795919),\n",
       "    ([u'this', u'i', u'had'], -18.518446249400366),\n",
       "    ([u'this', u'is', u'all'], -12.256420670620717)])),\n",
       " (0,\n",
       "  ((u'test', (u'rest', 1)),\n",
       "   [([u'this', u'is', u'a'], -0.003259988991643064),\n",
       "    ([u'this', u'is', u'as'], -7.269882928103522),\n",
       "    ([u'this', u'is', u'at'], -7.319893348678184),\n",
       "    ([u'this', u'is', u'an'], -6.282974629015413),\n",
       "    ([u'this', u'i', u'am'], -12.160860771749043),\n",
       "    ([u'this', u'is', u'ah'], -10.983454994807829),\n",
       "    ([u'this', u'is', u'and'], -13.573722160253656),\n",
       "    ([u'this', u'is', u'was'], -15.588625180795919),\n",
       "    ([u'this', u'i', u'had'], -18.518446249400366),\n",
       "    ([u'this', u'is', u'all'], -12.256420670620717)])),\n",
       " (0,\n",
       "  ((u'test', (u'that', 2)),\n",
       "   [([u'this', u'is', u'a'], -0.003259988991643064),\n",
       "    ([u'this', u'is', u'as'], -7.269882928103522),\n",
       "    ([u'this', u'is', u'at'], -7.319893348678184),\n",
       "    ([u'this', u'is', u'an'], -6.282974629015413),\n",
       "    ([u'this', u'i', u'am'], -12.160860771749043),\n",
       "    ([u'this', u'is', u'ah'], -10.983454994807829),\n",
       "    ([u'this', u'is', u'and'], -13.573722160253656),\n",
       "    ([u'this', u'is', u'was'], -15.588625180795919),\n",
       "    ([u'this', u'i', u'had'], -18.518446249400366),\n",
       "    ([u'this', u'is', u'all'], -12.256420670620717)]))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_word_next_path = sentence_word_next_split.join(sentence_path)\n",
    "sentence_word_next_path.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([u'this', u'is', u'a', u'test'], 0.00020555695201147437)),\n",
       " (0, ([u'this', u'is', u'a', u'west'], 1.0192267162244283e-08)),\n",
       " (0, ([u'this', u'is', u'a', u'best'], 5.138923800286865e-07)),\n",
       " (0, ([u'this', u'is', u'a', u'rest'], 1.5416771400860559e-06)),\n",
       " (0, ([u'this', u'is', u'at', u'that'], 9.59780360520677e-10))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_word_next_path_prob = sentence_word_next_path.mapValues(lambda v:\n",
    "                                                subs_word_prob(v, bc_transition_prob.value, default_transition_prob))\n",
    "sentence_word_next_path_prob.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [([u'this', u'is', u'a', u'test'], -0.010549285323548065),\n",
       "   ([u'this', u'is', u'a', u'west'], -9.922398387165785),\n",
       "   ([u'this', u'is', u'a', u'best'], -6.002013832431529),\n",
       "   ([u'this', u'is', u'a', u'rest'], -4.9034015437634215),\n",
       "   ([u'this', u'is', u'at', u'that'], -12.285078510409326),\n",
       "   ([u'this', u'is', u'at', u'these'], -14.802774983020315),\n",
       "   ([u'this', u'is', u'a', u'went'], -15.220715753713822),\n",
       "   ([u'this', u'is', u'a', u'must'], -15.220715753713822),\n",
       "   ([u'this', u'is', u'a', u'most'], -7.562661580696196),\n",
       "   ([u'this', u'is', u'a', u'left'], -10.60718401841962)]),\n",
       " (1,\n",
       "  [([u'this', u'is', u'a', u'test'], -0.010549285323548065),\n",
       "   ([u'this', u'is', u'a', u'west'], -9.922398387165785),\n",
       "   ([u'this', u'is', u'a', u'best'], -6.002013832431529),\n",
       "   ([u'this', u'is', u'a', u'rest'], -4.9034015437634215),\n",
       "   ([u'this', u'is', u'at', u'that'], -12.285078510409326),\n",
       "   ([u'this', u'is', u'at', u'these'], -14.802774983020315),\n",
       "   ([u'this', u'is', u'a', u'went'], -15.220715753713822),\n",
       "   ([u'this', u'is', u'a', u'must'], -15.220715753713822),\n",
       "   ([u'this', u'is', u'a', u'most'], -7.562661580696196),\n",
       "   ([u'this', u'is', u'a', u'left'], -10.60718401841962)]),\n",
       " (2,\n",
       "  [([u'here', u'is', u'a', u'test'], -0.010549285323548178),\n",
       "   ([u'here', u'is', u'a', u'west'], -9.922398387165785),\n",
       "   ([u'here', u'is', u'a', u'best'], -6.002013832431529),\n",
       "   ([u'here', u'is', u'a', u'rest'], -4.9034015437634215),\n",
       "   ([u'here', u'is', u'at', u'that'], -12.285078510409324),\n",
       "   ([u'here', u'is', u'at', u'these'], -14.802774983020317),\n",
       "   ([u'here', u'is', u'a', u'went'], -15.220715753713822),\n",
       "   ([u'here', u'is', u'a', u'must'], -15.220715753713822),\n",
       "   ([u'here', u'is', u'a', u'most'], -7.562661580696196),\n",
       "   ([u'here', u'is', u'a', u'left'], -10.60718401841962)]),\n",
       " (3,\n",
       "  [([u'this', u'is', u'a', u'test'], -0.011550059383306386),\n",
       "   ([u'this', u'is', u'a', u'west'], -9.923399161225545),\n",
       "   ([u'this', u'is', u'at', u'best'], -5.816578936418101),\n",
       "   ([u'this', u'is', u'a', u'rest'], -4.904402317823178),\n",
       "   ([u'this', u'is', u'at', u'that'], -7.680909098480993),\n",
       "   ([u'this', u'is', u'at', u'these'], -10.198605571091987),\n",
       "   ([u'this', u'is', u'a', u'went'], -15.221716527773578),\n",
       "   ([u'this', u'is', u'a', u'must'], -15.221716527773578),\n",
       "   ([u'this', u'is', u'a', u'most'], -7.563662354755956),\n",
       "   ([u'this', u'is', u'a', u'left'], -10.60818479247938)]),\n",
       " (4,\n",
       "  [([u'this', u'is', u'a', u'test'], -0.010546847956469676),\n",
       "   ([u'this', u'is', u'a', u'west'], -9.922395949798705),\n",
       "   ([u'this', u'is', u'a', u'best'], -6.002011395064451),\n",
       "   ([u'this', u'is', u'a', u'rest'], -4.903399106396344),\n",
       "   ([u'this', u'is', u'at', u'that'], -12.978223253602195),\n",
       "   ([u'this', u'is', u'a', u'these'], -15.220713316346743),\n",
       "   ([u'this', u'is', u'a', u'went'], -15.220713316346743),\n",
       "   ([u'this', u'is', u'a', u'must'], -15.220713316346743),\n",
       "   ([u'this', u'is', u'a', u'most'], -7.562659143329122),\n",
       "   ([u'this', u'is', u'a', u'left'], -10.607181581052544)])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_path = sentence_word_next_path_prob.groupByKey().mapValues(lambda v: normalize(v))\n",
    "sentence_path.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_num += 1\n",
    "word_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6,\n",
       "  [([u'her', u'to', u'set'], -0.7073236573108231),\n",
       "   ([u'her', u'to', u'see'], -2.8216524579910995),\n",
       "   ([u'her', u'she', u'met'], -5.43068860495902),\n",
       "   ([u'her', u'to', u'let'], -4.517563968429027),\n",
       "   ([u'her', u'the', u'yet'], -8.229793963975347),\n",
       "   ([u'her', u'to', u'get'], -3.4029513384144745),\n",
       "   ([u'her', u'she', u'sat'], -4.365977867966592),\n",
       "   ([u'her', u'he', u'sent'], -6.536828443436029),\n",
       "   ([u'her', u'the', u'seat'], -1.0631219193848052),\n",
       "   ([u'her', u'the', u'st'], -3.2399376250898158)]),\n",
       " (0,\n",
       "  [([u'this', u'is', u'a', u'test'], -0.010549285323548065),\n",
       "   ([u'this', u'is', u'a', u'west'], -9.922398387165785),\n",
       "   ([u'this', u'is', u'a', u'best'], -6.002013832431529),\n",
       "   ([u'this', u'is', u'a', u'rest'], -4.9034015437634215),\n",
       "   ([u'this', u'is', u'at', u'that'], -12.285078510409326),\n",
       "   ([u'this', u'is', u'at', u'these'], -14.802774983020315),\n",
       "   ([u'this', u'is', u'a', u'went'], -15.220715753713822),\n",
       "   ([u'this', u'is', u'a', u'must'], -15.220715753713822),\n",
       "   ([u'this', u'is', u'a', u'most'], -7.562661580696196),\n",
       "   ([u'this', u'is', u'a', u'left'], -10.60718401841962)]),\n",
       " (1,\n",
       "  [([u'this', u'is', u'a', u'test'], -0.010549285323548065),\n",
       "   ([u'this', u'is', u'a', u'west'], -9.922398387165785),\n",
       "   ([u'this', u'is', u'a', u'best'], -6.002013832431529),\n",
       "   ([u'this', u'is', u'a', u'rest'], -4.9034015437634215),\n",
       "   ([u'this', u'is', u'at', u'that'], -12.285078510409326),\n",
       "   ([u'this', u'is', u'at', u'these'], -14.802774983020315),\n",
       "   ([u'this', u'is', u'a', u'went'], -15.220715753713822),\n",
       "   ([u'this', u'is', u'a', u'must'], -15.220715753713822),\n",
       "   ([u'this', u'is', u'a', u'most'], -7.562661580696196),\n",
       "   ([u'this', u'is', u'a', u'left'], -10.60718401841962)]),\n",
       " (2,\n",
       "  [([u'here', u'is', u'a', u'test'], -0.010549285323548178),\n",
       "   ([u'here', u'is', u'a', u'west'], -9.922398387165785),\n",
       "   ([u'here', u'is', u'a', u'best'], -6.002013832431529),\n",
       "   ([u'here', u'is', u'a', u'rest'], -4.9034015437634215),\n",
       "   ([u'here', u'is', u'at', u'that'], -12.285078510409324),\n",
       "   ([u'here', u'is', u'at', u'these'], -14.802774983020317),\n",
       "   ([u'here', u'is', u'a', u'went'], -15.220715753713822),\n",
       "   ([u'here', u'is', u'a', u'must'], -15.220715753713822),\n",
       "   ([u'here', u'is', u'a', u'most'], -7.562661580696196),\n",
       "   ([u'here', u'is', u'a', u'left'], -10.60718401841962)]),\n",
       " (3,\n",
       "  [([u'this', u'is', u'a', u'test'], -0.011550059383306386),\n",
       "   ([u'this', u'is', u'a', u'west'], -9.923399161225545),\n",
       "   ([u'this', u'is', u'at', u'best'], -5.816578936418101),\n",
       "   ([u'this', u'is', u'a', u'rest'], -4.904402317823178),\n",
       "   ([u'this', u'is', u'at', u'that'], -7.680909098480993),\n",
       "   ([u'this', u'is', u'at', u'these'], -10.198605571091987),\n",
       "   ([u'this', u'is', u'a', u'went'], -15.221716527773578),\n",
       "   ([u'this', u'is', u'a', u'must'], -15.221716527773578),\n",
       "   ([u'this', u'is', u'a', u'most'], -7.563662354755956),\n",
       "   ([u'this', u'is', u'a', u'left'], -10.60818479247938)])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completed = completed.union(\n",
    "    sentence_word_count.filter(lambda (k, v): v==word_num).join(sentence_path).mapValues(lambda v: v[1]))\n",
    "completed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_path(final_paths):\n",
    "    return max((p[1], p[0]) for p in final_paths)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, [u'her', u'to', u'set']),\n",
       " (0, [u'this', u'is', u'a', u'test']),\n",
       " (1, [u'this', u'is', u'a', u'test']),\n",
       " (2, [u'here', u'is', u'a', u'test']),\n",
       " (3, [u'this', u'is', u'a', u'test'])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_suggestion = completed.mapValues(lambda v: get_max_path(v))\n",
    "sentence_suggestion.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'])),\n",
       " (1, ([u'this', u'is', u'a', u'test'], [u'this', u'is', u'a', u'test'])),\n",
       " (2, ([u'here', u'is', u'a', u'test'], [u'here', u'is', u'a', u'test'])),\n",
       " (3, ([u'this', u'is', u'ax', u'test'], [u'this', u'is', u'a', u'test'])),\n",
       " (4, ([u'this', u'is', u'za', u'test'], [u'this', u'is', u'a', u'test']))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_max_prob = sentence_id.join(sentence_suggestion)\n",
    "sentence_max_prob.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_count_mismatches(sentences):\n",
    "    '''\n",
    "    Helper function: compares the original sentence with the sentence\n",
    "    that has been suggested by the Viterbi algorithm, and calculates\n",
    "    the number of words that do not match.\n",
    "    '''\n",
    "    orig_sentence, sug_sentence = sentences\n",
    "    count_mismatches = len([(orig_sentence[i], sug_sentence[i]) \n",
    "            for i in range(len(orig_sentence))\n",
    "            if orig_sentence[i]!=sug_sentence[i]])\n",
    "    return count_mismatches, orig_sentence, sug_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (1, [u'this', u'is', u'ax', u'test'], [u'this', u'is', u'a', u'test'])),\n",
       " (4, (1, [u'this', u'is', u'za', u'test'], [u'this', u'is', u'a', u'test'])),\n",
       " (5, (1, [u'thee', u'is', u'a', u'test'], [u'there', u'is', u'a', u'test'])),\n",
       " (6, (1, [u'her', u'tee', u'set'], [u'her', u'to', u'set']))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_errors = sentence_max_prob.mapValues(lambda v: (get_count_mismatches(v))) \\\n",
    "            .filter(lambda (k, v): v[0]>0)\n",
    "sentence_errors.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
