{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_partitions = 6\n",
    "MAX_EDIT_DISTANCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################\n",
    "#\n",
    "# DOCUMENTATION HERE\n",
    "#\n",
    "######################\n",
    "\n",
    "# number of partitions to be used\n",
    "n_partitions = 6\n",
    "MAX_EDIT_DISTANCE = 3\n",
    "\n",
    "def get_n_deletes_list(w, n):\n",
    "    '''given a word, derive list of strings with up to n characters deleted'''\n",
    "    # since this list is generally of the same magnitude as the number of \n",
    "    # characters in a word, it may not make sense to parallelize this\n",
    "    # so we use python to create the list\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(n):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes\n",
    "\n",
    "def get_transitions(sentence):\n",
    "    if len(sentence)<2:\n",
    "        return None\n",
    "    else:\n",
    "        return [((sentence[i], sentence[i+1]), 1) for i in range(len(sentence)-1)]\n",
    "    \n",
    "def map_transition_prob(x):\n",
    "    vals = x[1]\n",
    "    total = float(sum(vals.values()))\n",
    "    probs = {k: math.log(v/total) for k, v in vals.items()}\n",
    "    return (x[0], probs)\n",
    "\n",
    "def parallel_create_dictionary(fname):\n",
    "    '''\n",
    "    Create dictionary, start probabilities and transition\n",
    "    probabilities using Spark RDDs.\n",
    "    '''\n",
    "    # we generate and count all words for the corpus,\n",
    "    # then add deletes to the dictionary\n",
    "    # this is a slightly different approach from the SymSpell algorithm\n",
    "    # that may be more appropriate for Spark processing\n",
    "    \n",
    "    print 'Creating dictionary...'\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "\n",
    "    # convert file into one long sequence of words\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "            .map(lambda line: line.lower()) \\\n",
    "            .filter(lambda x: x!='').cache()\n",
    "    \n",
    "    # split into individual sentences and remove other punctuation\n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate start probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # only focus on words at the start of sentences\n",
    "    start_words = split_sentence.map(lambda sentence: sentence[0] if len(sentence)>0 else None) \\\n",
    "        .filter(lambda word: word!=None)\n",
    "    \n",
    "    # add a count to each word\n",
    "    count_start_words_once = start_words.map(lambda word: (word, 1)).cache()\n",
    "\n",
    "    # use accumulator to count the number of words at the start of sentences\n",
    "    accum_total_start_words = sc.accumulator(0)\n",
    "    count_total_start_words = count_start_words_once.foreach(lambda x: accum_total_start_words.add(1))\n",
    "    total_start_words = float(accum_total_start_words.value)\n",
    "    \n",
    "    # reduce into count of unique words at the start of sentences\n",
    "    unique_start_words = count_start_words_once.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)\n",
    "    \n",
    "    # convert counts to probabilities\n",
    "    start_prob_calc = unique_start_words.mapValues(lambda v: math.log(v/total_start_words))\n",
    "    \n",
    "    # get default start probabilities (for words not in corpus)\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    \n",
    "    # store start probabilities as a dictionary (will be used as a lookup table)\n",
    "    start_prob = start_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate transition probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # focus on continuous word pairs within the sentence\n",
    "    # e.g. \"this is a test\" -> \"this is\", \"is a\", \"a test\"\n",
    "    # note: as the relevant probability is P(word|previous word)\n",
    "    # the tuples are ordered as (previous word, word)\n",
    "    other_words = split_sentence.map(lambda sentence: get_transitions(sentence)).filter(lambda x: x!=None). \\\n",
    "                flatMap(lambda x: x).cache()\n",
    "\n",
    "    # use accumulator to count the number of transitions\n",
    "    accum_total_other_words = sc.accumulator(0)\n",
    "    count_total_other_words = other_words.foreach(lambda x: accum_total_other_words.add(1))\n",
    "    total_other_words = float(accum_total_other_words.value)\n",
    "    \n",
    "    # reduce into count of unique word pairs\n",
    "    unique_other_words = other_words.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions)\n",
    "    \n",
    "    # aggregate by previous word\n",
    "    # i.e. (previous word, [(word1, word1-previous word count), (word2, word2-previous word count), ...])\n",
    "    other_words_collapsed = unique_other_words.map(lambda x: (x[0][0], (x[0][1], x[1]))).groupByKey().mapValues(dict)\n",
    "\n",
    "    # POTENTIAL OPTIMIZATION: FIND AN ALTERNATIVE TO GROUPBYKEY (CREATES ~9.3MB SHUFFLE)\n",
    "    \n",
    "    # convert counts to probabilities\n",
    "    transition_prob_calc = other_words_collapsed.map(lambda x: map_transition_prob(x))\n",
    "    \n",
    "    # get default transition probabilities (for word pairs not in corpus)\n",
    "    default_transition_prob = math.log(1/total_other_words)\n",
    "    \n",
    "    # store transition probabilities as dictionary (will be used as lookup table)\n",
    "    transition_prob = transition_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # process corpus for dictionary\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    replace_nonalphs = make_all_lower.map(lambda line: regex.sub(' ', line))\n",
    "    all_words = replace_nonalphs.flatMap(lambda line: line.split())\n",
    "\n",
    "    # create core corpus dictionary (i.e. only words appearing in file, no \"deletes\") and cache it\n",
    "    # output RDD of unique_words_with_count: [(word1, count1), (word2, count2), (word3, count3)...]\n",
    "    count_once = all_words.map(lambda word: (word, 1))\n",
    "    unique_words_with_count = count_once.reduceByKey(lambda a, b: a + b, numPartitions = n_partitions).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate deletes list\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # generate list of n-deletes from words in a corpus of the form: [(word1, count1), (word2, count2), ...]\n",
    "     \n",
    "    assert MAX_EDIT_DISTANCE > 0  \n",
    "    \n",
    "    generate_deletes = unique_words_with_count.map(lambda (parent, count): \n",
    "                                                   (parent, get_n_deletes_list(parent, MAX_EDIT_DISTANCE)))\n",
    "    expand_deletes = generate_deletes.flatMapValues(lambda x: x)\n",
    "    swap = expand_deletes.map(lambda (orig, delete): (delete, ([orig], 0)))\n",
    "   \n",
    "    ############\n",
    "    #\n",
    "    # combine delete elements with main dictionary\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    corpus = unique_words_with_count.mapValues(lambda count: ([], count))\n",
    "    combine = swap.union(corpus)  # combine deletes with main dictionary, eliminate duplicates\n",
    "    \n",
    "    # since the dictionary will only be a lookup table once created, we can\n",
    "    # pass on as a Python dictionary rather than RDD by reducing locally and\n",
    "    # avoiding an extra shuffle from reduceByKey\n",
    "    dictionary = combine.reduceByKeyLocally(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "\n",
    "    words_processed = unique_words_with_count.map(lambda (k, v): v).reduce(lambda a, b: a + b)\n",
    "    word_count = unique_words_with_count.count()   \n",
    "    \n",
    "    # output stats\n",
    "    print 'Total words processed: %i' % words_processed\n",
    "    print 'Total unique words in corpus: %i' % word_count \n",
    "    print 'Total items in dictionary (corpus words and deletions): %i' % len(dictionary)\n",
    "    print '  Edit distance for deletions: %i' % MAX_EDIT_DISTANCE\n",
    "    print 'Total unique words at the start of a sentence: %i' \\\n",
    "        % len(start_prob)\n",
    "    print 'Total unique word transitions: %i' % len(transition_prob)\n",
    "    \n",
    "    return dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Total words processed: 1105285\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary (corpus words and deletions): 2151998\n",
      "  Edit distance for deletions: 3\n",
      "Total unique words at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n",
      "CPU times: user 11.7 s, sys: 1.12 s, total: 12.8 s\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "    parallel_create_dictionary('testdata/big.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    # matrix. However, only the current and two previous rows are\n",
    "    # needed at once, so we only store those.\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, longest_word_length=20, \n",
    "                    min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count parameter, which only\n",
    "    considers words that have occur more than min_count times in the\n",
    "    (dictionary) corpus.\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > MAX_EDIT_DISTANCE:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=MAX_EDIT_DISTANCE:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<MAX_EDIT_DISTANCE \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance and l=0.01.\n",
    "    \n",
    "    The lambda parameter matches the one used in the AM207\n",
    "    lecture notes. Various parameters between 0 and 1 were tested\n",
    "    to confirm that 0.01 yields the most accurate results.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "\n",
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [u'this', u'is', u'ax', u'test']), (1, [u'her', u'tee', u'set'])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = \"testdata/test1.txt\"\n",
    "\n",
    "# broadcast Python dictionaries to workers\n",
    "bc_dictionary = sc.broadcast(dictionary)\n",
    "bc_start_prob = sc.broadcast(start_prob)\n",
    "bc_transition_prob = sc.broadcast(transition_prob)\n",
    "\n",
    "# convert all text to lowercase and drop empty lines\n",
    "make_all_lower = sc.textFile(fname) \\\n",
    "    .map(lambda line: line.lower()) \\\n",
    "    .filter(lambda x: x!='')\n",
    "\n",
    "regex = re.compile('[^a-z ]')\n",
    "\n",
    "# split into sentences -> remove special characters -> convert into list of words\n",
    "split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "        .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "        .map(lambda sentence: sentence.split()).cache()\n",
    "\n",
    "# use accumulator to count the number of words checked\n",
    "accum_total_words = sc.accumulator(0)\n",
    "split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "\n",
    "# assign each sentence a unique id\n",
    "sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k)).cache()\n",
    "\n",
    "sentence_id.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_word_id(words):\n",
    "    return [(i, w) for i, w in enumerate(words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_sentence_words(sentence):\n",
    "    sent_id, words = sentence\n",
    "    return [[sent_id, w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, (0, u'this')],\n",
       " [0, (1, u'is')],\n",
       " [0, (2, u'ax')],\n",
       " [0, (3, u'test')],\n",
       " [1, (0, u'her')],\n",
       " [1, (1, u'tee')],\n",
       " [1, (2, u'set')]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number each word in a sentence, and split into individual words\n",
    "sentence_word_id = sentence_id.mapValues(lambda v: get_sentence_word_id(v)).flatMap(lambda x: split_sentence_words(x))\n",
    "sentence_word_id.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get suggestions for each word\n",
    "sentence_word_suggestions = sentence_word_id.mapValues(lambda v: \n",
    "                                                       (v[0], v[1], get_suggestions(v[1], bc_dictionary.value))).cache()\n",
    "# sentence_word_suggestions.filter(lambda x: x[0]==0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (u'this',\n",
       "   [(u'this', 0),\n",
       "    (u'his', 1),\n",
       "    (u'thus', 1),\n",
       "    (u'thin', 1),\n",
       "    (u'the', 2),\n",
       "    (u'that', 2),\n",
       "    (u'is', 2),\n",
       "    (u'him', 2),\n",
       "    (u'they', 2),\n",
       "    (u'their', 2)])),\n",
       " (1,\n",
       "  (u'her',\n",
       "   [(u'her', 0),\n",
       "    (u'he', 1),\n",
       "    (u'here', 1),\n",
       "    (u'hear', 1),\n",
       "    (u'the', 2),\n",
       "    (u'his', 2),\n",
       "    (u'had', 2),\n",
       "    (u'for', 2),\n",
       "    (u'be', 2),\n",
       "    (u'or', 2)]))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for the first words in sentences\n",
    "sentence_word_1 = sentence_word_suggestions.filter(lambda (k, v): v[0]==0).mapValues(lambda v: (v[1], v[2]))\n",
    "sentence_word_1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def start_word_prob(words, tmp_sp, d_sp):\n",
    "    orig_word, sug_words = words\n",
    "    probs = [(w[0], \n",
    "              math.exp(get_start_prob(w[0], tmp_sp, d_sp) + get_emission_prob(w[1]))\n",
    "             ) \n",
    "             for w in sug_words]\n",
    "    sum_probs = sum([p[1] for p in probs])\n",
    "    probs = [([p[0]], math.log(p[1]/sum_probs)) for p in probs]\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [([u'this'], -0.010416136127922377),\n",
       "   ([u'his'], -4.751501506095812),\n",
       "   ([u'thus'], -7.051702807734582),\n",
       "   ([u'thin'], -8.997612956789895),\n",
       "   ([u'the'], -7.479924902965362),\n",
       "   ([u'that'], -9.697449125500638),\n",
       "   ([u'is'], -10.680518021105867),\n",
       "   ([u'him'], -11.02509475953902),\n",
       "   ([u'they'], -9.835785909400098),\n",
       "   ([u'their'], -11.093861616786112)]),\n",
       " (1,\n",
       "  [([u'her'], -0.07316779348212661),\n",
       "   ([u'he'], -2.72717847452876),\n",
       "   ([u'here'], -5.705854346454598),\n",
       "   ([u'hear'], -8.539067690510812),\n",
       "   ([u'the'], -6.615914528578116),\n",
       "   ([u'his'], -9.185808498256602),\n",
       "   ([u'had'], -9.857703403156888),\n",
       "   ([u'for'], -9.548639412388193),\n",
       "   ([u'be'], -10.411495062806324),\n",
       "   ([u'or'], -10.27847192940494)])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate probability for each suggestion\n",
    "# format: (sentence id, [path-probability pairs])\n",
    "sentence_path = sentence_word_1.mapValues(lambda v: start_word_prob(v, bc_start_prob.value, default_start_prob))\n",
    "sentence_path.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###LOOP STARTS HERE###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_num = 1\n",
    "word_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (u'is',\n",
       "   [(u'is', 0),\n",
       "    (u'in', 1),\n",
       "    (u'it', 1),\n",
       "    (u'his', 1),\n",
       "    (u'as', 1),\n",
       "    (u'i', 1),\n",
       "    (u's', 1),\n",
       "    (u'if', 1),\n",
       "    (u'its', 1),\n",
       "    (u'us', 1)])),\n",
       " (1,\n",
       "  (u'tee',\n",
       "   [(u'the', 1),\n",
       "    (u'see', 1),\n",
       "    (u'ten', 1),\n",
       "    (u'tea', 1),\n",
       "    (u'to', 2),\n",
       "    (u'he', 2),\n",
       "    (u'be', 2),\n",
       "    (u'her', 2),\n",
       "    (u'were', 2),\n",
       "    (u'she', 2)]))]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for the next words in sentences\n",
    "sentence_word_next = sentence_word_suggestions.filter(lambda (k, v): v[0]==word_num).mapValues(lambda v: (v[1], v[2]))\n",
    "sentence_word_next.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that there are more words left\n",
    "sentence_word_next.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_suggestions(sentence):\n",
    "    sent_id, (word, word_sug)  = sentence\n",
    "    return [[sent_id, (word, w)] for w in word_sug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, (u'is', (u'is', 0))],\n",
       " [0, (u'is', (u'in', 1))],\n",
       " [0, (u'is', (u'it', 1))],\n",
       " [0, (u'is', (u'his', 1))],\n",
       " [0, (u'is', (u'as', 1))],\n",
       " [0, (u'is', (u'i', 1))],\n",
       " [0, (u'is', (u's', 1))],\n",
       " [0, (u'is', (u'if', 1))],\n",
       " [0, (u'is', (u'its', 1))],\n",
       " [0, (u'is', (u'us', 1))],\n",
       " [1, (u'tee', (u'the', 1))],\n",
       " [1, (u'tee', (u'see', 1))],\n",
       " [1, (u'tee', (u'ten', 1))],\n",
       " [1, (u'tee', (u'tea', 1))],\n",
       " [1, (u'tee', (u'to', 2))],\n",
       " [1, (u'tee', (u'he', 2))],\n",
       " [1, (u'tee', (u'be', 2))],\n",
       " [1, (u'tee', (u'her', 2))],\n",
       " [1, (u'tee', (u'were', 2))],\n",
       " [1, (u'tee', (u'she', 2))]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into suggestions\n",
    "sentence_word_next_split = sentence_word_next.flatMap(lambda x: split_suggestions(x))\n",
    "sentence_word_next_split.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# join on previous path\n",
    "# format: (sentence id, ((current word, (current word suggestion, edit distance)), \n",
    "#         [(previous path-probability pairs)]))\n",
    "sentence_word_next_path = sentence_word_next_split.join(sentence_path)\n",
    "# sentence_word_next_path.filter(lambda x: x[0]==0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subs_word_prob(words, tmp_tp, d_tp):\n",
    "    \n",
    "    # unpack values\n",
    "    sent_id = words[0]\n",
    "    cur_word = words[1][0][0]\n",
    "    cur_sug = words[1][0][1][0]\n",
    "    cur_sug_ed = words[1][0][1][1]\n",
    "    prev_sug = words[1][1]\n",
    "    \n",
    "    # belief + transition probability + emission probability\n",
    "    (prob, word) = max((p[1]\n",
    "                 + get_transition_prob(cur_sug, p[0][-1], tmp_tp, d_tp)\n",
    "                 + get_emission_prob(cur_sug_ed), p[0])\n",
    "                     for p in prev_sug)\n",
    "    \n",
    "    return sent_id, (word + [cur_sug], math.exp(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([u'this', u'is'], 0.07965129629236094)),\n",
       " (0, ([u'this', u'in'], 4.52866901996701e-05)),\n",
       " (0, ([u'this', u'it'], 1.0655691811687093e-05)),\n",
       " (0, ([u'this', u'his'], 7.991768858765314e-06)),\n",
       " (0, ([u'this', u'as'], 4.7950613152591945e-05)),\n",
       " (0, ([u'this', u'i'], 3.196707543506126e-05)),\n",
       " (0, ([u'that', u's'], 1.5724350299737122e-08)),\n",
       " (0, ([u'this', u'if'], 2.663922952921773e-06)),\n",
       " (0, ([u'this', u'its'], 1.0119590152617674e-08)),\n",
       " (0, ([u'this', u'us'], 1.0119590152617674e-08)),\n",
       " (1, ([u'her', u'the'], 6.210551616996943e-05)),\n",
       " (1, ([u'her', u'see'], 4.140367744664627e-06)),\n",
       " (1, ([u'her', u'ten'], 9.504083106300208e-09)),\n",
       " (1, ([u'her', u'tea'], 2.070183872332317e-06)),\n",
       " (1, ([u'her', u'to'], 1.12825021042111e-06)),\n",
       " (1, ([u'her', u'he'], 2.4842206467987784e-07)),\n",
       " (1, ([u'he', u'be'], 2.556031225104305e-09)),\n",
       " (1, ([u'her', u'her'], 2.0701838723323124e-08)),\n",
       " (1, ([u'her', u'were'], 1.0350919361661578e-08)),\n",
       " (1, ([u'her', u'she'], 3.8298401638147756e-07))]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate path with max probability\n",
    "sentence_word_next_path_prob = sentence_word_next_path.map(lambda x:\n",
    "                                        subs_word_prob(x, bc_transition_prob.value, default_transition_prob))\n",
    "sentence_word_next_path_prob.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(probs):\n",
    "    sum_probs = sum([p[1] for p in probs])\n",
    "    return [(p[0], math.log(p[1]/sum_probs)) for p in probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [([u'this', u'is'], -0.001838225822312142),\n",
       "   ([u'this', u'in'], -7.474238641144874),\n",
       "   ([u'this', u'it'], -8.921157624081198),\n",
       "   ([u'this', u'his'], -9.20883969653298),\n",
       "   ([u'this', u'as'], -7.417080227304924),\n",
       "   ([u'this', u'i'], -7.822545335413089),\n",
       "   ([u'that', u's'], -15.439796609526178),\n",
       "   ([u'this', u'if'], -10.307451985201089),\n",
       "   ([u'this', u'its'], -15.880533930581192),\n",
       "   ([u'this', u'us'], -15.880533930581192)]),\n",
       " (1,\n",
       "  [([u'her', u'the'], -0.12135666115371971),\n",
       "   ([u'her', u'see'], -2.8294068622559303),\n",
       "   ([u'her', u'ten'], -8.906225245412237),\n",
       "   ([u'her', u'tea'], -3.522554042815874),\n",
       "   ([u'her', u'to'], -4.1295235271347694),\n",
       "   ([u'her', u'he'], -5.642817579015966),\n",
       "   ([u'he', u'be'], -10.21949099950891),\n",
       "   ([u'her', u'her'], -8.127724228803967),\n",
       "   ([u'her', u'were'], -8.82087140936391),\n",
       "   ([u'her', u'she'], -5.209953496719689)])]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize for numerical stability\n",
    "sentence_path = sentence_word_next_path_prob.groupByKey().mapValues(lambda v: normalize(v))\n",
    "sentence_path.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_num += 1\n",
    "word_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (u'ax',\n",
       "   [(u'ax', 0),\n",
       "    (u'a', 1),\n",
       "    (u'as', 1),\n",
       "    (u'at', 1),\n",
       "    (u'an', 1),\n",
       "    (u'am', 1),\n",
       "    (u'ah', 1),\n",
       "    (u'x', 1),\n",
       "    (u'and', 2),\n",
       "    (u'was', 2)])),\n",
       " (1,\n",
       "  (u'set',\n",
       "   [(u'set', 0),\n",
       "    (u'see', 1),\n",
       "    (u'met', 1),\n",
       "    (u'let', 1),\n",
       "    (u'yet', 1),\n",
       "    (u'get', 1),\n",
       "    (u'sat', 1),\n",
       "    (u'sent', 1),\n",
       "    (u'seat', 1),\n",
       "    (u'st', 1)]))]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter for the next words in sentences\n",
    "sentence_word_next = sentence_word_suggestions.filter(lambda (k, v): v[0]==word_num).mapValues(lambda v: (v[1], v[2]))\n",
    "sentence_word_next.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that there are more words left\n",
    "sentence_word_next.isEmpty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, (u'ax', (u'ax', 0))],\n",
       " [0, (u'ax', (u'a', 1))],\n",
       " [0, (u'ax', (u'as', 1))],\n",
       " [0, (u'ax', (u'at', 1))],\n",
       " [0, (u'ax', (u'an', 1))],\n",
       " [0, (u'ax', (u'am', 1))],\n",
       " [0, (u'ax', (u'ah', 1))],\n",
       " [0, (u'ax', (u'x', 1))],\n",
       " [0, (u'ax', (u'and', 2))],\n",
       " [0, (u'ax', (u'was', 2))],\n",
       " [1, (u'set', (u'set', 0))],\n",
       " [1, (u'set', (u'see', 1))],\n",
       " [1, (u'set', (u'met', 1))],\n",
       " [1, (u'set', (u'let', 1))],\n",
       " [1, (u'set', (u'yet', 1))],\n",
       " [1, (u'set', (u'get', 1))],\n",
       " [1, (u'set', (u'sat', 1))],\n",
       " [1, (u'set', (u'sent', 1))],\n",
       " [1, (u'set', (u'seat', 1))],\n",
       " [1, (u'set', (u'st', 1))]]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into suggestions\n",
    "sentence_word_next_split = sentence_word_next.flatMap(lambda x: split_suggestions(x))\n",
    "sentence_word_next_split.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ((u'ax', (u'ax', 0)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'ax', (u'a', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'ax', (u'as', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'ax', (u'at', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'ax', (u'an', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'ax', (u'am', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'ax', (u'ah', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'ax', (u'x', 1)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'ax', (u'and', 2)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (0,\n",
       "  ((u'ax', (u'was', 2)),\n",
       "   [([u'this', u'is'], -0.001838225822312142),\n",
       "    ([u'this', u'in'], -7.474238641144874),\n",
       "    ([u'this', u'it'], -8.921157624081198),\n",
       "    ([u'this', u'his'], -9.20883969653298),\n",
       "    ([u'this', u'as'], -7.417080227304924),\n",
       "    ([u'this', u'i'], -7.822545335413089),\n",
       "    ([u'that', u's'], -15.439796609526178),\n",
       "    ([u'this', u'if'], -10.307451985201089),\n",
       "    ([u'this', u'its'], -15.880533930581192),\n",
       "    ([u'this', u'us'], -15.880533930581192)])),\n",
       " (1,\n",
       "  ((u'set', (u'set', 0)),\n",
       "   [([u'her', u'the'], -0.12135666115371971),\n",
       "    ([u'her', u'see'], -2.8294068622559303),\n",
       "    ([u'her', u'ten'], -8.906225245412237),\n",
       "    ([u'her', u'tea'], -3.522554042815874),\n",
       "    ([u'her', u'to'], -4.1295235271347694),\n",
       "    ([u'her', u'he'], -5.642817579015966),\n",
       "    ([u'he', u'be'], -10.21949099950891),\n",
       "    ([u'her', u'her'], -8.127724228803967),\n",
       "    ([u'her', u'were'], -8.82087140936391),\n",
       "    ([u'her', u'she'], -5.209953496719689)])),\n",
       " (1,\n",
       "  ((u'set', (u'see', 1)),\n",
       "   [([u'her', u'the'], -0.12135666115371971),\n",
       "    ([u'her', u'see'], -2.8294068622559303),\n",
       "    ([u'her', u'ten'], -8.906225245412237),\n",
       "    ([u'her', u'tea'], -3.522554042815874),\n",
       "    ([u'her', u'to'], -4.1295235271347694),\n",
       "    ([u'her', u'he'], -5.642817579015966),\n",
       "    ([u'he', u'be'], -10.21949099950891),\n",
       "    ([u'her', u'her'], -8.127724228803967),\n",
       "    ([u'her', u'were'], -8.82087140936391),\n",
       "    ([u'her', u'she'], -5.209953496719689)])),\n",
       " (1,\n",
       "  ((u'set', (u'met', 1)),\n",
       "   [([u'her', u'the'], -0.12135666115371971),\n",
       "    ([u'her', u'see'], -2.8294068622559303),\n",
       "    ([u'her', u'ten'], -8.906225245412237),\n",
       "    ([u'her', u'tea'], -3.522554042815874),\n",
       "    ([u'her', u'to'], -4.1295235271347694),\n",
       "    ([u'her', u'he'], -5.642817579015966),\n",
       "    ([u'he', u'be'], -10.21949099950891),\n",
       "    ([u'her', u'her'], -8.127724228803967),\n",
       "    ([u'her', u'were'], -8.82087140936391),\n",
       "    ([u'her', u'she'], -5.209953496719689)])),\n",
       " (1,\n",
       "  ((u'set', (u'let', 1)),\n",
       "   [([u'her', u'the'], -0.12135666115371971),\n",
       "    ([u'her', u'see'], -2.8294068622559303),\n",
       "    ([u'her', u'ten'], -8.906225245412237),\n",
       "    ([u'her', u'tea'], -3.522554042815874),\n",
       "    ([u'her', u'to'], -4.1295235271347694),\n",
       "    ([u'her', u'he'], -5.642817579015966),\n",
       "    ([u'he', u'be'], -10.21949099950891),\n",
       "    ([u'her', u'her'], -8.127724228803967),\n",
       "    ([u'her', u'were'], -8.82087140936391),\n",
       "    ([u'her', u'she'], -5.209953496719689)])),\n",
       " (1,\n",
       "  ((u'set', (u'yet', 1)),\n",
       "   [([u'her', u'the'], -0.12135666115371971),\n",
       "    ([u'her', u'see'], -2.8294068622559303),\n",
       "    ([u'her', u'ten'], -8.906225245412237),\n",
       "    ([u'her', u'tea'], -3.522554042815874),\n",
       "    ([u'her', u'to'], -4.1295235271347694),\n",
       "    ([u'her', u'he'], -5.642817579015966),\n",
       "    ([u'he', u'be'], -10.21949099950891),\n",
       "    ([u'her', u'her'], -8.127724228803967),\n",
       "    ([u'her', u'were'], -8.82087140936391),\n",
       "    ([u'her', u'she'], -5.209953496719689)])),\n",
       " (1,\n",
       "  ((u'set', (u'get', 1)),\n",
       "   [([u'her', u'the'], -0.12135666115371971),\n",
       "    ([u'her', u'see'], -2.8294068622559303),\n",
       "    ([u'her', u'ten'], -8.906225245412237),\n",
       "    ([u'her', u'tea'], -3.522554042815874),\n",
       "    ([u'her', u'to'], -4.1295235271347694),\n",
       "    ([u'her', u'he'], -5.642817579015966),\n",
       "    ([u'he', u'be'], -10.21949099950891),\n",
       "    ([u'her', u'her'], -8.127724228803967),\n",
       "    ([u'her', u'were'], -8.82087140936391),\n",
       "    ([u'her', u'she'], -5.209953496719689)])),\n",
       " (1,\n",
       "  ((u'set', (u'sat', 1)),\n",
       "   [([u'her', u'the'], -0.12135666115371971),\n",
       "    ([u'her', u'see'], -2.8294068622559303),\n",
       "    ([u'her', u'ten'], -8.906225245412237),\n",
       "    ([u'her', u'tea'], -3.522554042815874),\n",
       "    ([u'her', u'to'], -4.1295235271347694),\n",
       "    ([u'her', u'he'], -5.642817579015966),\n",
       "    ([u'he', u'be'], -10.21949099950891),\n",
       "    ([u'her', u'her'], -8.127724228803967),\n",
       "    ([u'her', u'were'], -8.82087140936391),\n",
       "    ([u'her', u'she'], -5.209953496719689)])),\n",
       " (1,\n",
       "  ((u'set', (u'sent', 1)),\n",
       "   [([u'her', u'the'], -0.12135666115371971),\n",
       "    ([u'her', u'see'], -2.8294068622559303),\n",
       "    ([u'her', u'ten'], -8.906225245412237),\n",
       "    ([u'her', u'tea'], -3.522554042815874),\n",
       "    ([u'her', u'to'], -4.1295235271347694),\n",
       "    ([u'her', u'he'], -5.642817579015966),\n",
       "    ([u'he', u'be'], -10.21949099950891),\n",
       "    ([u'her', u'her'], -8.127724228803967),\n",
       "    ([u'her', u'were'], -8.82087140936391),\n",
       "    ([u'her', u'she'], -5.209953496719689)])),\n",
       " (1,\n",
       "  ((u'set', (u'seat', 1)),\n",
       "   [([u'her', u'the'], -0.12135666115371971),\n",
       "    ([u'her', u'see'], -2.8294068622559303),\n",
       "    ([u'her', u'ten'], -8.906225245412237),\n",
       "    ([u'her', u'tea'], -3.522554042815874),\n",
       "    ([u'her', u'to'], -4.1295235271347694),\n",
       "    ([u'her', u'he'], -5.642817579015966),\n",
       "    ([u'he', u'be'], -10.21949099950891),\n",
       "    ([u'her', u'her'], -8.127724228803967),\n",
       "    ([u'her', u'were'], -8.82087140936391),\n",
       "    ([u'her', u'she'], -5.209953496719689)])),\n",
       " (1,\n",
       "  ((u'set', (u'st', 1)),\n",
       "   [([u'her', u'the'], -0.12135666115371971),\n",
       "    ([u'her', u'see'], -2.8294068622559303),\n",
       "    ([u'her', u'ten'], -8.906225245412237),\n",
       "    ([u'her', u'tea'], -3.522554042815874),\n",
       "    ([u'her', u'to'], -4.1295235271347694),\n",
       "    ([u'her', u'he'], -5.642817579015966),\n",
       "    ([u'he', u'be'], -10.21949099950891),\n",
       "    ([u'her', u'her'], -8.127724228803967),\n",
       "    ([u'her', u'were'], -8.82087140936391),\n",
       "    ([u'her', u'she'], -5.209953496719689)]))]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join on previous path\n",
    "# format: (sentence id, ((current word, (current word suggestion, edit distance)), \n",
    "#         [(previous path-probability pairs)]))\n",
    "sentence_word_next_path = sentence_word_next_split.join(sentence_path)\n",
    "sentence_word_next_path.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([u'this', u'is', u'ax'], 1.0206768458569286e-06)),\n",
       " (0, ([u'this', u'is', u'a'], 0.0006417656065436952)),\n",
       " (0, ([u'this', u'is', u'as'], 4.482519568703837e-05)),\n",
       " (0, ([u'this', u'is', u'at'], 4.263860077547551e-05)),\n",
       " (0, ([u'this', u'is', u'an'], 0.00012026272013595663)),\n",
       " (0, ([u'this', u'i', u'am'], 3.3681971618496976e-07)),\n",
       " (0, ([u'this', u'is', u'ah'], 1.0932974557814245e-06)),\n",
       " (0, ([u'this', u'is', u'x'], 1.0206768458569282e-08)),\n",
       " (0, ([u'this', u'is', u'and'], 8.199730918360683e-08)),\n",
       " (0, ([u'this', u'is', u'was'], 1.093297455781426e-08)),\n",
       " (1, ([u'her', u'to', u'set'], 1.674747778516495e-05)),\n",
       " (1, ([u'her', u'to', u'see'], 2.0216598183520555e-06)),\n",
       " (1, ([u'her', u'she', u'met'], 1.4880518962107781e-07)),\n",
       " (1, ([u'her', u'to', u'let'], 3.708370081000812e-07)),\n",
       " (1, ([u'her', u'the', u'yet'], 9.056951996826441e-09)),\n",
       " (1, ([u'her', u'to', u'get'], 1.1304547504986358e-06)),\n",
       " (1, ([u'her', u'she', u'sat'], 4.315350499011254e-07)),\n",
       " (1, ([u'her', u'he', u'sent'], 4.922975217825312e-08)),\n",
       " (1, ([u'her', u'the', u'seat'], 1.1733516503040005e-05)),\n",
       " (1, ([u'her', u'the', u'st'], 1.330604964262271e-06))]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate path with max probability\n",
    "sentence_word_next_path_prob = sentence_word_next_path.map(lambda x:\n",
    "                                        subs_word_prob(x, bc_transition_prob.value, default_transition_prob))\n",
    "sentence_word_next_path_prob.collect() #filter(lambda x: x[0]==0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "   ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "   ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "   ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "   ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "   ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "   ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "   ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "   ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "   ([u'this', u'is', u'was'], -11.263612447240725)]),\n",
       " (1,\n",
       "  [([u'her', u'to', u'set'], -0.7073236573108231),\n",
       "   ([u'her', u'to', u'see'], -2.8216524579910995),\n",
       "   ([u'her', u'she', u'met'], -5.43068860495902),\n",
       "   ([u'her', u'to', u'let'], -4.517563968429027),\n",
       "   ([u'her', u'the', u'yet'], -8.229793963975347),\n",
       "   ([u'her', u'to', u'get'], -3.4029513384144745),\n",
       "   ([u'her', u'she', u'sat'], -4.365977867966592),\n",
       "   ([u'her', u'he', u'sent'], -6.536828443436029),\n",
       "   ([u'her', u'the', u'seat'], -1.0631219193848052),\n",
       "   ([u'her', u'the', u'st'], -3.2399376250898158)])]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize for numerical stability\n",
    "sentence_path = sentence_word_next_path_prob.groupByKey().mapValues(lambda v: normalize(v))\n",
    "sentence_path.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_num += 1\n",
    "word_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (u'test',\n",
       "   [(u'test', 0),\n",
       "    (u'west', 1),\n",
       "    (u'best', 1),\n",
       "    (u'rest', 1),\n",
       "    (u'that', 2),\n",
       "    (u'these', 2),\n",
       "    (u'went', 2),\n",
       "    (u'must', 2),\n",
       "    (u'most', 2),\n",
       "    (u'left', 2)]))]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_word_next = sentence_word_suggestions.filter(lambda (k, v): v[0]==word_num) \\\n",
    "                .mapValues(lambda v: (v[1], v[2]))\n",
    "sentence_word_next.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, (u'test', (u'test', 0))],\n",
       " [0, (u'test', (u'west', 1))],\n",
       " [0, (u'test', (u'best', 1))],\n",
       " [0, (u'test', (u'rest', 1))],\n",
       " [0, (u'test', (u'that', 2))],\n",
       " [0, (u'test', (u'these', 2))],\n",
       " [0, (u'test', (u'went', 2))],\n",
       " [0, (u'test', (u'must', 2))],\n",
       " [0, (u'test', (u'most', 2))],\n",
       " [0, (u'test', (u'left', 2))]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_word_next_split = sentence_word_next.flatMap(lambda x: split_suggestions(x))\n",
    "sentence_word_next_split.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ((u'test', (u'test', 0)),\n",
       "   [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "    ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "    ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "    ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "    ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "    ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "    ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "    ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "    ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "    ([u'this', u'is', u'was'], -11.263612447240725)])),\n",
       " (0,\n",
       "  ((u'test', (u'west', 1)),\n",
       "   [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "    ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "    ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "    ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "    ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "    ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "    ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "    ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "    ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "    ([u'this', u'is', u'was'], -11.263612447240725)])),\n",
       " (0,\n",
       "  ((u'test', (u'best', 1)),\n",
       "   [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "    ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "    ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "    ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "    ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "    ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "    ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "    ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "    ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "    ([u'this', u'is', u'was'], -11.263612447240725)])),\n",
       " (0,\n",
       "  ((u'test', (u'rest', 1)),\n",
       "   [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "    ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "    ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "    ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "    ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "    ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "    ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "    ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "    ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "    ([u'this', u'is', u'was'], -11.263612447240725)])),\n",
       " (0,\n",
       "  ((u'test', (u'that', 2)),\n",
       "   [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "    ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "    ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "    ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "    ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "    ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "    ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "    ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "    ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "    ([u'this', u'is', u'was'], -11.263612447240725)])),\n",
       " (0,\n",
       "  ((u'test', (u'these', 2)),\n",
       "   [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "    ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "    ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "    ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "    ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "    ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "    ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "    ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "    ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "    ([u'this', u'is', u'was'], -11.263612447240725)])),\n",
       " (0,\n",
       "  ((u'test', (u'went', 2)),\n",
       "   [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "    ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "    ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "    ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "    ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "    ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "    ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "    ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "    ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "    ([u'this', u'is', u'was'], -11.263612447240725)])),\n",
       " (0,\n",
       "  ((u'test', (u'must', 2)),\n",
       "   [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "    ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "    ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "    ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "    ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "    ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "    ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "    ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "    ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "    ([u'this', u'is', u'was'], -11.263612447240725)])),\n",
       " (0,\n",
       "  ((u'test', (u'most', 2)),\n",
       "   [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "    ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "    ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "    ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "    ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "    ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "    ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "    ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "    ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "    ([u'this', u'is', u'was'], -11.263612447240725)])),\n",
       " (0,\n",
       "  ((u'test', (u'left', 2)),\n",
       "   [([u'this', u'is', u'ax'], -6.727174598010501),\n",
       "    ([u'this', u'is', u'a'], -0.2834174414245401),\n",
       "    ([u'this', u'is', u'as'], -2.944870194548328),\n",
       "    ([u'this', u'is', u'at'], -2.9948806151229896),\n",
       "    ([u'this', u'is', u'an'], -1.9579618954602194),\n",
       "    ([u'this', u'i', u'am'], -7.8358480381938485),\n",
       "    ([u'this', u'is', u'ah'], -6.658442261252635),\n",
       "    ([u'this', u'is', u'x'], -11.332344783998593),\n",
       "    ([u'this', u'is', u'and'], -9.248709426698461),\n",
       "    ([u'this', u'is', u'was'], -11.263612447240725)]))]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_word_next_path = sentence_word_next_split.join(sentence_path)\n",
    "sentence_word_next_path.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([u'this', u'is', u'a', u'test'], 0.00015533214292411503)),\n",
       " (0, ([u'this', u'is', u'a', u'west'], 7.701937025599192e-09)),\n",
       " (0, ([u'this', u'is', u'at', u'best'], 4.6791753999290885e-07)),\n",
       " (0, ([u'this', u'is', u'a', u'rest'], 1.1649910719308636e-06)),\n",
       " (0, ([u'this', u'is', u'at', u'that'], 7.252721869890078e-08)),\n",
       " (0, ([u'this', u'is', u'at', u'these'], 5.848969249911342e-09)),\n",
       " (0, ([u'this', u'is', u'a', u'went'], 3.850968512799608e-11)),\n",
       " (0, ([u'this', u'is', u'a', u'must'], 3.850968512799608e-11)),\n",
       " (0, ([u'this', u'is', u'a', u'most'], 8.154937503516043e-08)),\n",
       " (0, ([u'this', u'is', u'a', u'left'], 3.883303573102879e-09))]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_word_next_path_prob = sentence_word_next_path.map(lambda x:\n",
    "                                                subs_word_prob(x, bc_transition_prob.value, default_transition_prob))\n",
    "sentence_word_next_path_prob.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [([u'this', u'is', u'a', u'test'], -0.011550059383306386),\n",
       "   ([u'this', u'is', u'a', u'west'], -9.923399161225545),\n",
       "   ([u'this', u'is', u'at', u'best'], -5.816578936418101),\n",
       "   ([u'this', u'is', u'a', u'rest'], -4.904402317823178),\n",
       "   ([u'this', u'is', u'at', u'that'], -7.680909098480993),\n",
       "   ([u'this', u'is', u'at', u'these'], -10.198605571091987),\n",
       "   ([u'this', u'is', u'a', u'went'], -15.221716527773578),\n",
       "   ([u'this', u'is', u'a', u'must'], -15.221716527773578),\n",
       "   ([u'this', u'is', u'a', u'most'], -7.563662354755956),\n",
       "   ([u'this', u'is', u'a', u'left'], -10.60818479247938)])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_path = sentence_word_next_path_prob.groupByKey().mapValues(lambda v: normalize(v))\n",
    "sentence_path.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_num += 1\n",
    "word_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_word_next = sentence_word_suggestions.filter(lambda (k, v): v[0]==word_num) \\\n",
    "                .mapValues(lambda v: (v[1], v[2]))\n",
    "sentence_word_next.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_max_path(final_paths):\n",
    "    max_path = max((p[1], p[0]) for p in final_paths)\n",
    "    return max_path[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [u'this', u'is', u'a', u'test'])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_suggestion = sentence_path.mapValues(lambda v: get_max_path(v))\n",
    "sentence_suggestion.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ([u'this', u'is', u'ax', u'test'], [u'this', u'is', u'a', u'test']))]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_max_prob = sentence_id.join(sentence_suggestion)\n",
    "sentence_max_prob.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
