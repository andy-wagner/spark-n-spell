{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_document_context_parallel_words(fname, dictionary, longest_word_length,\n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob,\n",
    "                             num_word_suggestions=5000, printlist=True):\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # broadcast Python dictionaries to workers\n",
    "    bc_dictionary = sc.broadcast(dictionary)\n",
    "    bc_start_prob = sc.broadcast(start_prob)\n",
    "    bc_transition_prob = sc.broadcast(transition_prob)\n",
    "    \n",
    "    make_all_lower = sc.textFile(fname).map(lambda line: line.lower()).filter(lambda x: x!='')\n",
    "    \n",
    "    regex = re.compile('[^a-z ]')\n",
    "    \n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()).cache()                        ###\n",
    "            \n",
    "    # use accumulator to count the number of words checked\n",
    "    accum_total_words = sc.accumulator(0)\n",
    "    split_words = split_sentence.flatMap(lambda x: x).foreach(lambda x: accum_total_words.add(1))\n",
    "    \n",
    "    sentence_id = split_sentence.zipWithIndex().map(lambda (k, v): (v, k)).partitionBy(n_partitions).cache()  ###\n",
    "    \n",
    "    sentence_words = sentence_id.mapValues(lambda v: \n",
    "                                map_sentence_words(v, bc_dictionary.value, longest_word_length))  ###\n",
    "    \n",
    "    sentence_word_sug = sentence_words.mapValues(lambda v: split_suggestions(v))  ###\n",
    "    \n",
    "    sentence_word_combos = sentence_word_sug.mapValues(lambda v: get_word_combos(v)) ###\n",
    "    \n",
    "    sentence_word_combos_split = sentence_word_combos.flatMap(lambda x: split_combos(x)).partitionBy(n_partitions).cache()  ###? lost partitioning,\n",
    "     \n",
    "    sentence_word_combos_prob = sentence_word_combos_split.mapValues(lambda v: \n",
    "                                get_combo_prob(v, bc_start_prob.value, default_start_prob, \n",
    "                                               bc_transition_prob.value, default_transition_prob))  ## only if we successfully repartition\n",
    "    \n",
    "    sentence_max_prob = sentence_word_combos_prob.reduceByKey(lambda a,b: a if a[2] > b[2] else b)\n",
    "\n",
    "    sentence_mismatch = sentence_max_prob.mapValues(lambda v: (v[0], v[1])) \\\n",
    "         .mapValues(lambda v: get_sentence_mismatches(v)) \\\n",
    "         .filter(lambda (k,v): v!=None)\n",
    "               \n",
    "    word_mismatch = sentence_mismatch.flatMap(lambda x: split_mismatches(x)).cache() ## or just 'collect' right away & change print statement below\n",
    "    \n",
    "#     # use accumulator to count the number of mismatches\n",
    "#     accum_total_mismatches = sc.accumulator(0)\n",
    "#     count_mismatches = word_mismatch.foreach(lambda x: accum_total_mismatches.add(1))\n",
    "    \n",
    "    if printlist:\n",
    "        print '    Words with suggested corrections (line number, word in text, top match):'\n",
    "        print word_mismatch.map(lambda x: (x[0], str(x[1]) + \" --> \" + str(x[2]))).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
