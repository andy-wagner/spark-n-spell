{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson\n",
    "import time\n",
    "import sys, getopt\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deletes_list(w, max_edit_distance):\n",
    "    '''\n",
    "    Given a word, derive strings with up to max_edit_distance\n",
    "    characters deleted. \n",
    "\n",
    "    The list is generally of the same magnitude as the number of\n",
    "    characters in a word, so it does not make sense to parallelize\n",
    "    this function. Instead, we use Python to create the list.\n",
    "    '''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(max_edit_distance):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes\n",
    "\n",
    "def get_transitions(sentence):\n",
    "    '''\n",
    "    Helper function: converts a sentence into all two-word pairs.\n",
    "    Output format is a list of tuples.\n",
    "    e.g. 'This is a test' >> ('this', 'is'), ('is', 'a'), ('a', 'test')\n",
    "    ''' \n",
    "    if len(sentence)<2:\n",
    "        return None\n",
    "    else:\n",
    "        return [((sentence[i], sentence[i+1]), 1) \n",
    "                for i in range(len(sentence)-1)]\n",
    "    \n",
    "def map_transition_prob(vals):\n",
    "    '''\n",
    "    Helper function: calculates conditional probabilities for all word\n",
    "    pairs, i.e. P(word|previous word)\n",
    "    '''\n",
    "    total = float(sum(vals.values()))\n",
    "    return {k: math.log(v/total) for k, v in vals.items()}\n",
    "\n",
    "def parallel_create_dictionary(fname, max_edit_distance=3, \n",
    "                                num_partitions=6):\n",
    "    '''\n",
    "    Load a text file and use it to create a dictionary and\n",
    "    to calculate start probabilities and transition probabilities. \n",
    "    '''\n",
    "    \n",
    "    # Note: this function makes use of multiple accumulators to keep\n",
    "    # track of the words that are being processed. An alternative \n",
    "    # implementation that wraps accumulators in helper functions was\n",
    "    # also tested, but did not yield any noticeable improvements.\n",
    "\n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "\n",
    "    # load file contents and convert into one long sequence of words\n",
    "    # RDD format: 'line 1', 'line 2', 'line 3', ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "            .map(lambda line: line.lower()) \\\n",
    "            .filter(lambda x: x!='').cache()\n",
    "    \n",
    "    # split into individual sentences and remove other punctuation\n",
    "    # RDD format: [words of sentence 1], [words of sentence 2], ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    split_sentence = make_all_lower.flatMap(lambda \n",
    "        line: line.replace('?','.').replace('!','.').split('.')) \\\n",
    "             .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "             .map(lambda sentence: sentence.split()) \\\n",
    "             .filter(lambda x: x!=[]).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate start probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # extract all words that are at the beginning of sentences\n",
    "    # RDD format: 'word1', 'word2', 'word3', ...\n",
    "    start_words = split_sentence.map(lambda sentence: sentence[0] \n",
    "        if len(sentence)>0 else None) \\\n",
    "            .filter(lambda word: word!=None)\n",
    "    \n",
    "    # add a count to each word\n",
    "    # RDD format: ('word1', 1), ('word2', 1), ('word3', 1), ...\n",
    "    # note: partition here because we are using words as keys for\n",
    "    # the first time - yields a small but consistent improvement in\n",
    "    # runtime (~2-3 sec for big.txt)\n",
    "    # cache because this RDD is used in multiple operations\n",
    "    count_start_words_once = start_words.map(lambda word: (word, 1)) \\\n",
    "            .partitionBy(num_partitions).cache()\n",
    "\n",
    "    # use accumulator to count the number of start words processed\n",
    "    accum_total_start_words = sc.accumulator(0)\n",
    "    count_start_words_once.foreach(lambda x: accum_total_start_words.add(1))\n",
    "    total_start_words = float(accum_total_start_words.value)\n",
    "    \n",
    "    # reduce into count of unique words at the start of sentences\n",
    "    # RDD format: ('word1', frequency), ('word2', frequency), ...\n",
    "    unique_start_words = count_start_words_once.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # convert counts to log-probabilities\n",
    "    # RDD format: ('word1', log-prob of word1), \n",
    "    #             ('word2', log-prob of word2), ...\n",
    "    start_prob_calc = unique_start_words.mapValues(lambda v: \n",
    "        math.log(v/total_start_words))\n",
    "    \n",
    "    # get default start probabilities (for words not in corpus)\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    \n",
    "    # store start probabilities as a dictionary (i.e. a lookup table)\n",
    "    # note: given the spell-checking algorithm, this cannot be maintained\n",
    "    # as an RDD as it is not possible to map within a map\n",
    "    start_prob = start_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate transition probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # note: various partitioning strategies were attempted for this\n",
    "    # portion of the function, but they failed to yield significant\n",
    "    # improvements in performance.\n",
    "\n",
    "    # focus on continuous word pairs within the sentence\n",
    "    # e.g. \"this is a test\" -> \"this is\", \"is a\", \"a test\"\n",
    "    # note: as the relevant probability is P(word|previous word)\n",
    "    # the tuples are ordered as (previous word, word)\n",
    "\n",
    "    # extract all word pairs within a sentence and add a count\n",
    "    # RDD format: (('word1', 'word2'), 1), (('word2', 'word3'), 1), ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    other_words = split_sentence.map(lambda sentence: \n",
    "        get_transitions(sentence)) \\\n",
    "            .filter(lambda x: x!=None) \\\n",
    "            .flatMap(lambda x: x).cache()\n",
    "\n",
    "    # use accumulator to count the number of transitions (word pairs)\n",
    "    accum_total_other_words = sc.accumulator(0)\n",
    "    other_words.foreach(lambda x: accum_total_other_words.add(1))\n",
    "    total_other_words = float(accum_total_other_words.value)\n",
    "    \n",
    "    # reduce into count of unique word pairs\n",
    "    # RDD format: (('word1', 'word2'), frequency), \n",
    "    #             (('word2', 'word3'), frequency), ...\n",
    "    unique_other_words = other_words.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # aggregate by (and change key to) previous word\n",
    "    # RDD format: ('previous word', {'word1': word pair count, \n",
    "    #                                'word2': word pair count}}), ...\n",
    "    other_words_collapsed = unique_other_words.map(lambda x: \n",
    "        (x[0][0], (x[0][1], x[1]))) \\\n",
    "            .groupByKey().mapValues(dict)\n",
    "\n",
    "    # note: the above line of code is the slowest in the function\n",
    "    # (8.6 MB shuffle read and 4.5 MB shuffle write for big.txt)\n",
    "    # An alternative approach that aggregates lists with reduceByKey was\n",
    "    # attempted, but did not yield noticeable improvements in runtime.\n",
    "    \n",
    "    # convert counts to log-probabilities\n",
    "    # RDD format: ('previous word', {'word1': log-prob of pair, \n",
    "    #                                 word2: log-prob of pair}}), ...\n",
    "    transition_prob_calc = other_words_collapsed.mapValues(lambda v: \n",
    "        map_transition_prob(v))\n",
    "\n",
    "    # get default transition probabilities (for word pairs not in corpus)\n",
    "    default_transition_prob = math.log(1/total_other_words)\n",
    "    \n",
    "    # store transition probabilities as a dictionary (i.e. a lookup table)\n",
    "    # note: given the spell-checking algorithm, this cannot be maintained\n",
    "    # as an RDD as it is not possible to map within a map\n",
    "    transition_prob = transition_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate dictionary\n",
    "    #\n",
    "    ############\n",
    "\n",
    "    # note: this approach is slightly different from the original SymSpell\n",
    "    # algorithm, but is more appropriate for a SPARK implementation\n",
    "    \n",
    "    # split into individual words (all)\n",
    "    # RDD format: 'word1', 'word2', 'word3', ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    all_words = make_all_lower.map(lambda line: regex.sub(' ', line)) \\\n",
    "            .flatMap(lambda line: line.split()).cache()\n",
    "\n",
    "    # use accumulator to count the number of words processed\n",
    "    accum_words_processed = sc.accumulator(0)\n",
    "    all_words.foreach(lambda x: accum_words_processed.add(1))\n",
    "\n",
    "    # add a count to each word\n",
    "    # RDD format: ('word1', 1), ('word2', 1), ('word3', 1), ...\n",
    "    count_once = all_words.map(lambda word: (word, 1))\n",
    "\n",
    "    # reduce into counts of unique words - this is the core corpus dictionary\n",
    "    # (i.e. only words appearing in the file, without 'deletes'))\n",
    "    # RDD format: ('word1', frequency), ('word2', frequency), ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    # note: imposing partitioning at this step yields a small \n",
    "    # improvement in runtime (~1 sec for big.txt) by equally\n",
    "    # balancing elements among workers for subsequent operations\n",
    "    unique_words_with_count = count_once.reduceByKey(lambda a, b: a + b, \n",
    "        numPartitions = num_partitions).cache()\n",
    "    \n",
    "    # use accumulator to count the number of unique words\n",
    "    accum_unique_words = sc.accumulator(0)\n",
    "    unique_words_with_count.foreach(lambda x: accum_unique_words.add(1))\n",
    "\n",
    "    # generate list of \"deletes\" for each word in the corpus\n",
    "    # RDD format: (word1, [deletes for word1]), (word2, [deletes for word2]), ...\n",
    "    generate_deletes = unique_words_with_count.map(lambda (parent, count): \n",
    "        (parent, get_deletes_list(parent, max_edit_distance)))\n",
    "    \n",
    "    # split into all key-value pairs\n",
    "    # RDD format: (word1, delete1), (word1, delete2), ...\n",
    "    expand_deletes = generate_deletes.flatMapValues(lambda x: x)\n",
    "    \n",
    "    # swap word order and add a zero count (because \"deletes\" were not\n",
    "    # present in the dictionary)\n",
    "    swap = expand_deletes.map(lambda (orig, delete): (delete, ([orig], 0)))\n",
    "    \n",
    "    # create a placeholder for each real word\n",
    "    # RDD format: ('word1', ([], frequency)), ('word2', ([], frequency)), ...\n",
    "    corpus = unique_words_with_count.mapValues(lambda count: ([], count))\n",
    "\n",
    "    # combine main dictionary and \"deletes\" (and eliminate duplicates)\n",
    "    # RDD format: ('word1', ([deletes for word1], frequency)), \n",
    "    #             ('word2', ([deletes for word2], frequency)), ...\n",
    "    combine = swap.union(corpus)\n",
    "    \n",
    "    # store dictionary items and deletes as a dictionary (i.e. a lookup table)\n",
    "    # note: given the spell-checking algorithm, this cannot be maintained\n",
    "    # as an RDD as it is not possible to map within a map\n",
    "    # note: use reduceByKeyLocally to avoid an extra shuffle from reduceByKey\n",
    "    dictionary = combine.reduceByKeyLocally(lambda a, b: (a[0]+b[0], a[1]+b[1])) \n",
    "    \n",
    "    # output stats\n",
    "    print 'Total words processed: %i' % accum_words_processed.value\n",
    "    print 'Total unique words in corpus: %i' % accum_unique_words.value \n",
    "    print 'Total items in dictionary (corpus words and deletions): %i' \\\n",
    "        % len(dictionary)\n",
    "    print '  Edit distance for deletions: %i' % max_edit_distance\n",
    "    print 'Total unique words at the start of a sentence: %i' \\\n",
    "        % len(start_prob)\n",
    "    print 'Total unique word transitions: %i' % len(transition_prob)\n",
    "    \n",
    "    return dictionary, start_prob, default_start_prob, \\\n",
    "            transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_file = 'testdata/big.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 2 µs, total: 8 µs\n",
      "Wall time: 12.9 µs\n",
      "Total words processed: 1105285\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary (corpus words and deletions): 2151998\n",
      "  Edit distance for deletions: 3\n",
      "Total unique words at the start of a sentence: 15356\n",
      "Total unique word transitions: 27086\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "    parallel_create_dictionary(dictionary_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    matrix. However, only the current and two previous rows are\n",
    "    needed at once, so we only store those.\n",
    "\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, max_edit_distance, \n",
    "                    longest_word_length=20, min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count and max_sug parameters.\n",
    "    - min_count: minimum number of times a word must have appeared\n",
    "    in the dictionary corpus to be considered a valid suggestion\n",
    "    - max_sug: number of suggestions that are returned (ranked by\n",
    "    frequency of appearance in dictionary corpus and edit distance\n",
    "    from word being checked)\n",
    "\n",
    "    These changes were imposed in order to ensure that the problem\n",
    "    remains tractable when checking very large documents. In practice,\n",
    "    the \"correct\" suggestion is almost always amongst the top ten.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > max_edit_distance:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=max_edit_distance:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<max_edit_distance \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # note: make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance between the observed word and the intended\n",
    "    word and l=0.01.\n",
    "    \n",
    "    Both the overall approach and the parameter of l=0.01 are based on\n",
    "    the 2015 lecture notes from AM207 Stochastic Optimization.\n",
    "    Various parameters for lambda between 0 and 1 were tested, which\n",
    "    confirmed that 0.01 yields the most accurate word suggestions.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    '''\n",
    "    P(word being at the beginning of a sentence)\n",
    "    '''\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, \n",
    "                        transition_prob, default_transition_prob):\n",
    "    '''\n",
    "    P(word|previous word)\n",
    "    '''\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob\n",
    "\n",
    "def map_sentence_words(sentence, tmp_dict, max_edit_distance):\n",
    "    '''\n",
    "    Helper function: returns all suggestions for all words\n",
    "    in a sentence.\n",
    "    '''\n",
    "    return [[word, get_suggestions(word, tmp_dict, max_edit_distance)] \n",
    "            for i, word in enumerate(sentence)]\n",
    "\n",
    "def split_suggestions(sentence):\n",
    "    '''\n",
    "    Helper function: create word-suggestion pairs for each\n",
    "    word in the sentence, and look up the emission probability\n",
    "    of each pair.\n",
    "    '''\n",
    "    result = []\n",
    "    for word in sentence:\n",
    "        result.append([(word[0], s[0], get_emission_prob(s[1])) \n",
    "                       for s in word[1]])\n",
    "    return result\n",
    "\n",
    "def get_word_combos(sug_lists):\n",
    "    '''\n",
    "    Helper function: returns all possible sentences that can be\n",
    "    created from the list of suggestions for each word in the\n",
    "    original sentence.\n",
    "    e.g. a sentence with 5 words and 10 suggestions per word\n",
    "    will result in 10^6 possible sentences.\n",
    "    '''\n",
    "    return list(itertools.product(*sug_lists))\n",
    "\n",
    "def get_combo_prob(combo, tmp_sp, d_sp, tmp_tp, d_tp):\n",
    "    \n",
    "    # first word in sentence\n",
    "    # emission prob * start prob\n",
    "    orig_path = [combo[0][0]]\n",
    "    sug_path = [combo[0][1]]\n",
    "    prob = combo[0][2] + get_start_prob(combo[0][1], tmp_sp, d_sp)\n",
    "    \n",
    "    # subsequent words\n",
    "    for i, w in enumerate(combo[1:]):\n",
    "        orig_path.append(w[0])\n",
    "        sug_path.append(w[1])\n",
    "        prob += w[2] + get_transition_prob(w[1], combo[i-1][1], tmp_tp, d_tp)\n",
    "    \n",
    "    return orig_path, sug_path, prob\n",
    "\n",
    "def get_count_mismatches(sentences):\n",
    "    '''\n",
    "    Helper function: compares the original sentence with the sentence\n",
    "    that has been suggested by the Viterbi algorithm, and calculates\n",
    "    the number of words that do not match.\n",
    "    '''\n",
    "    orig_sentence, sug_sentence, prob = sentences\n",
    "    count_mismatches = len([(orig_sentence[i], sug_sentence[i]) \n",
    "            for i in range(len(orig_sentence))\n",
    "            if orig_sentence[i]!=sug_sentence[i]])\n",
    "    return count_mismatches, orig_sentence, sug_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_document_context_parallel_approximate(fname, dictionary,\n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob,\n",
    "                             max_edit_distance=3, num_partitions=6,\n",
    "                             display_results=False):\n",
    "    \n",
    "    '''\n",
    "    Load a text file and spell-check each sentence using the\n",
    "    dictionary and probability tables that were created in the\n",
    "    pre-processing stage.\n",
    "\n",
    "    Suggested corrections are either printed to the screen or\n",
    "    saved in a log file, depending on the settings.\n",
    "    '''\n",
    "\n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "    \n",
    "    # broadcast Python dictionaries to workers (from pre-processing)\n",
    "    bc_dictionary = sc.broadcast(dictionary)\n",
    "    bc_start_prob = sc.broadcast(start_prob)\n",
    "    bc_transition_prob = sc.broadcast(transition_prob)\n",
    "    \n",
    "    # load file contents and convert into one long sequence of words\n",
    "    # RDD format: 'line 1', 'line 2', 'line 3', ...\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "            .map(lambda line: line.lower()) \\\n",
    "            .filter(lambda x: x!='')\n",
    "    \n",
    "    # split into individual sentences and remove other punctuation\n",
    "    # RDD format: [words of sentence1], [words of sentence2], ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    split_sentence = make_all_lower.flatMap(lambda line: line.split('.')) \\\n",
    "            .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "            .map(lambda sentence: sentence.split()) \\\n",
    "            .filter(lambda x: x!=[]).cache()\n",
    "    \n",
    "    # use accumulator to count the number of words checked\n",
    "    accum_total_words = sc.accumulator(0)\n",
    "    split_sentence.flatMap(lambda x: x) \\\n",
    "            .foreach(lambda x: accum_total_words.add(1))\n",
    "    \n",
    "    # assign a unique id to each sentence\n",
    "    # RDD format: (0, [words of sentence1]), (1, [words of sentence2]), ...\n",
    "    # cache here after completing transformations - results in \n",
    "    # improvements in runtime that scale with file size\n",
    "    # partition as sentence id will remain the key going forward\n",
    "    sentence_id = split_sentence.zipWithIndex().map(\n",
    "        lambda (k, v): (v, k)).partitionBy(num_partitions).cache()\n",
    "    \n",
    "    print sentence_id.count()\n",
    "    print sentence_id.getNumPartitions()\n",
    "    print sentence_id.collect()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # spell-checking\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # look up possible suggestions for each word in each sentence\n",
    "    # RDD format:\n",
    "    # (sentence id, [[word, [suggestions]], [word, [suggestions]], ... ]),\n",
    "    # (sentence id, [[word, [suggestions]], [word, [suggestions]], ... ]), ...\n",
    "    sentence_words = sentence_id.mapValues(lambda v: \n",
    "        map_sentence_words(v, bc_dictionary.value, max_edit_distance))\n",
    "    \n",
    "    # look up emission probabilities for each word\n",
    "    # i.e. P(observed word|intended word)\n",
    "    # RDD format: (one list of tuples per word in the sentence)\n",
    "    # (sentence id, [(original word, suggested word, P(original|suggested)),\n",
    "    #                (original word, suggested word, P(original|suggested)), ...]), ...\n",
    "    sentence_word_sug = sentence_words.mapValues(lambda v: \n",
    "        split_suggestions(v))\n",
    "    \n",
    "    # generate all possible corrected combinations (using Cartesian\n",
    "    # product) - i.e. a sentence with 4 word, each of which have 5 \n",
    "    # possible suggestions, will yield 5^4 possible combinations\n",
    "    # RDD format: (a tuple of tuples for each combination)\n",
    "    # (sentence id, [(original word1, suggested word1, P(original1|suggested1)),\n",
    "    #                (original word2, suggested word2, P(original2|suggested2)), ...]), ...\n",
    "    sentence_word_combos = sentence_word_sug.mapValues(lambda v:\n",
    "        get_word_combos(v))\n",
    "    \n",
    "    # flatmap into all possible combinations per sentence\n",
    "    # RDD format: (a tuple of tuples for one combination)\n",
    "    # (sentence id, [(original word1, suggested word1, P(original1|suggested1)),\n",
    "    #                (original word2, suggested word2, P(original2|suggested2)), ...]), ...\n",
    "    # partitioning after the flatmap results in a small improvement in runtime\n",
    "    # for the smaller texts on which we were able to run the function\n",
    "    sentence_word_combos_split = sentence_word_combos.flatMapValues(lambda x: x) \\\n",
    "            .partitionBy(num_partitions).cache()\n",
    "    \n",
    "    # calculate the probability of each word combination being the\n",
    "    # intended one, given what was actually typed\n",
    "    # note: the approach does not drop any word combinations, so may\n",
    "    # yield different results to the Viterbi algorithm\n",
    "    # RDD format: \n",
    "    # (sentence id, ([original sentence], [suggested sentence], probability)),\n",
    "    # (sentence id, ([original sentence], [suggested sentence], probability)), ...\n",
    "    sentence_word_combos_prob = sentence_word_combos_split.mapValues(\n",
    "        lambda v: get_combo_prob(v, bc_start_prob.value, default_start_prob, \n",
    "            bc_transition_prob.value, default_transition_prob))\n",
    "    \n",
    "    # identify the word combination with the highest probability for\n",
    "    # each sentence\n",
    "    # (sentence id, ([original sentence], [suggested sentence], probability)),\n",
    "    # (sentence id, ([original sentence], [suggested sentence], probability)), ...\n",
    "    sentence_max_prob = sentence_word_combos_prob.reduceByKey(\n",
    "        lambda a,b: a if a[2] > b[2] else b)\n",
    "\n",
    "    ############\n",
    "    #\n",
    "    # output results\n",
    "    #\n",
    "    ############\n",
    "\n",
    "    # count the number of errors per sentence and drop any sentences\n",
    "    # without errors\n",
    "    # RDD format: (sentence id, (# errors, [original sentence], [suggested sentence])),\n",
    "    #             (sentence id, (# errors, [original sentence], [suggested sentence])), ...\n",
    "    sentence_errors = sentence_max_prob.mapValues(\n",
    "        lambda v: get_count_mismatches(v)) \\\n",
    "            .filter(lambda (k, v): v[0]>0)\n",
    "\n",
    "    # collect all sentences with identified errors (as list)\n",
    "    sentence_errors_list = sentence_errors.collect()\n",
    "    \n",
    "    # count the number of potentially misspelled words\n",
    "    num_errors = sum([s[1][0] for s in sentence_errors_list])\n",
    "    \n",
    "    # print suggested corrections\n",
    "    if display_results:\n",
    "        for sentence in sentence_errors_list:\n",
    "            print 'Sentence %i: %s --> %s' % (sentence[0],\n",
    "                ' '.join(sentence[1][1]), ' '.join(sentence[1][2]))\n",
    "            print '-----'\n",
    "    \n",
    "    # output suggested corrections to file\n",
    "    else:\n",
    "        f = open('spell-log.txt', 'w')\n",
    "        for sentence in sentence_errors_list:\n",
    "            f.write('Sentence %i: %s --> %s\\n' % (sentence[0], \n",
    "                ' '.join(sentence[1][1]), ' '.join(sentence[1][2])))\n",
    "        f.close()\n",
    "    \n",
    "    print '-----'\n",
    "    print 'Total words checked: %i' % accum_total_words.value\n",
    "    print 'Total potential errors found: %i' % num_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "6\n",
      "[(0, [u'this', u'place', u'was', u'delicious', u'my', u'parents', u'saw', u'a', u'recommendation', u'to', u'visit', u'this', u'place', u'from', u'rick', u'sebak', u's', u'things', u'i', u'like', u'about', u'pittsburgh', u'and', u'he', u's', u'usually', u'pretty', u'accurate']), (6, [u'on', u'top', u'of', u'being', u'extremely', u'large', u'portions', u'it', u'was', u'incredibly', u'affordable']), (1, [u'his', u'recommendations', u'were', u'to', u'try', u'the', u'reuben', u'fish', u'sandwich', u'and', u'open', u'faced', u'steak', u'sandwich']), (7, [u'the', u'giant', u'fish', u'sandwich', u'was', u'and', u'the', u'giant', u'reuben', u'was']), (2, [u'we', u'went', u'early', u'afternoon', u'for', u'a', u'late', u'lunch', u'today', u'a', u'saturday', u'and', u'were', u'seated', u'right', u'away']), (8, [u'our', u'drinks', u'were', u'always', u'filled', u'and', u'we', u'were', u'checked', u'on', u'several', u'times', u'during', u'the', u'meal']), (3, [u'the', u'staff', u'is', u'extremely', u'friendly']), (9, [u'we', u'will', u'definitely', u'be', u'back', u'oh', u'and', u'a', u'bit', u'of', u'advice', u'ahead', u'of', u'time', u'they', u'take', u'cash', u'only']), (4, [u'my', u'mom', u'i', u'each', u'had', u'the', u'fish', u'sandwich', u'while', u'my', u'dad', u'brother', u'had', u'a', u'reuben', u'sandwich']), (10, [u'so', u'come', u'prepared', u'but', u'i', u'm', u'pretty', u'sure', u'i', u'saw', u'an', u'atm', u'there', u'as', u'well']), (5, [u'the', u'fish', u'was', u'very', u'good', u'but', u'the', u'reuben', u'was', u'to', u'die', u'for', u'both', u'dishes', u'were', u'massive', u'and', u'could', u'very', u'easily', u'be', u'shared', u'between', u'two', u'people']), (11, [u'and', u'i', u'do', u'believe', u'they', u'are', u'closed', u'on', u'sundays', u'mondays'])]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-30efc55170f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m correct_document_context_parallel_approximate(check_file, dictionary,\n\u001b[1;32m      6\u001b[0m                         \u001b[0mstart_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_start_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                         transition_prob, default_transition_prob)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-40451154c247>\u001b[0m in \u001b[0;36mcorrect_document_context_parallel_approximate\u001b[0;34m(fname, dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob, max_edit_distance, num_partitions, display_results)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;31m# collect all sentences with identified errors (as list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0msentence_errors_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# count the number of potentially misspelled words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \"\"\"\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m    538\u001b[0m                 self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    432\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                         \u001b[0;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m                             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "check_file = 'testdata/yelp1review.txt'\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "correct_document_context_parallel_approximate(check_file, dictionary,\n",
    "                        start_prob, default_start_prob, \n",
    "                        transition_prob, default_transition_prob)\n",
    "\n",
    "run_time = time.time() - start_time\n",
    "\n",
    "print '-----'\n",
    "print '%.2f seconds to run' % run_time\n",
    "print '-----'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
