{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from scipy.stats import poisson\n",
    "import time\n",
    "import sys, getopt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deletes_list(w, max_edit_distance):\n",
    "    '''\n",
    "    Given a word, derive strings with up to max_edit_distance\n",
    "    characters deleted. \n",
    "\n",
    "    The list is generally of the same magnitude as the number of\n",
    "    characters in a word, so it does not make sense to parallelize\n",
    "    this function. Instead, we use Python to create the list.\n",
    "    '''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(max_edit_distance):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes\n",
    "\n",
    "def get_transitions(sentence):\n",
    "    '''\n",
    "    Helper function: converts a sentence into all two-word pairs.\n",
    "    Output format is a list of tuples.\n",
    "    e.g. 'This is a test' >> ('this', 'is'), ('is', 'a'), ('a', 'test')\n",
    "    ''' \n",
    "    if len(sentence)<2:\n",
    "        return None\n",
    "    else:\n",
    "        return [((sentence[i], sentence[i+1]), 1) \n",
    "                for i in range(len(sentence)-1)]\n",
    "    \n",
    "def map_transition_prob(vals):\n",
    "    '''\n",
    "    Helper function: calculates conditional probabilities for all word\n",
    "    pairs, i.e. P(word|previous word)\n",
    "    '''\n",
    "    total = float(sum(vals.values()))\n",
    "    return {k: math.log(v/total) for k, v in vals.items()}\n",
    "\n",
    "def parallel_create_dictionary(fname, max_edit_distance=3, \n",
    "                                num_partitions=6):\n",
    "    '''\n",
    "    Load a text file and use it to create a dictionary and\n",
    "    to calculate start probabilities and transition probabilities. \n",
    "    '''\n",
    "    \n",
    "    # Note: this function makes use of multiple accumulators to keep\n",
    "    # track of the words that are being processed. An alternative \n",
    "    # implementation that wraps accumulators in helper functions was\n",
    "    # also tested, but did not yield any noticeable improvements.\n",
    "\n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "\n",
    "    # load file contents and convert into one long sequence of words\n",
    "    # RDD format: 'line 1', 'line 2', 'line 3', ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "            .map(lambda line: line.lower()) \\\n",
    "            .filter(lambda x: x!='').cache()\n",
    "    \n",
    "    # split into individual sentences and remove other punctuation\n",
    "    # RDD format: [words of sentence 1], [words of sentence 2], ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    split_sentence = make_all_lower.flatMap(lambda \n",
    "        line: line.replace('?','.').replace('!','.').split('.')) \\\n",
    "             .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "             .map(lambda sentence: sentence.split()) \\\n",
    "             .filter(lambda x: x!=[]).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate start probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # extract all words that are at the beginning of sentences\n",
    "    # RDD format: 'word1', 'word2', 'word3', ...\n",
    "    start_words = split_sentence.map(lambda sentence: sentence[0] \n",
    "        if len(sentence)>0 else None) \\\n",
    "            .filter(lambda word: word!=None)\n",
    "    \n",
    "    # add a count to each word\n",
    "    # RDD format: ('word1', 1), ('word2', 1), ('word3', 1), ...\n",
    "    # note: partition here because we are using words as keys for\n",
    "    # the first time - yields a small but consistent improvement in\n",
    "    # runtime (~2-3 sec for big.txt)\n",
    "    # cache because this RDD is used in multiple operations\n",
    "    count_start_words_once = start_words.map(lambda word: (word, 1)) \\\n",
    "            .partitionBy(num_partitions).cache()\n",
    "\n",
    "    # use accumulator to count the number of start words processed\n",
    "    accum_total_start_words = sc.accumulator(0)\n",
    "    count_start_words_once.foreach(lambda x: accum_total_start_words.add(1))\n",
    "    total_start_words = float(accum_total_start_words.value)\n",
    "    \n",
    "    # reduce into count of unique words at the start of sentences\n",
    "    # RDD format: ('word1', frequency), ('word2', frequency), ...\n",
    "    unique_start_words = count_start_words_once.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # convert counts to log-probabilities\n",
    "    # RDD format: ('word1', log-prob of word1), \n",
    "    #             ('word2', log-prob of word2), ...\n",
    "    start_prob_calc = unique_start_words.mapValues(lambda v: \n",
    "        math.log(v/total_start_words))\n",
    "    \n",
    "    # get default start probabilities (for words not in corpus)\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    \n",
    "    # store start probabilities as a dictionary (i.e. a lookup table)\n",
    "    # note: given the spell-checking algorithm, this cannot be maintained\n",
    "    # as an RDD as it is not possible to map within a map\n",
    "    start_prob = start_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate transition probabilities\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # note: various partitioning strategies were attempted for this\n",
    "    # portion of the function, but they failed to yield significant\n",
    "    # improvements in performance.\n",
    "\n",
    "    # focus on continuous word pairs within the sentence\n",
    "    # e.g. \"this is a test\" -> \"this is\", \"is a\", \"a test\"\n",
    "    # note: as the relevant probability is P(word|previous word)\n",
    "    # the tuples are ordered as (previous word, word)\n",
    "\n",
    "    # extract all word pairs within a sentence and add a count\n",
    "    # RDD format: (('word1', 'word2'), 1), (('word2', 'word3'), 1), ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    other_words = split_sentence.map(lambda sentence: \n",
    "        get_transitions(sentence)) \\\n",
    "            .filter(lambda x: x!=None) \\\n",
    "            .flatMap(lambda x: x).cache()\n",
    "\n",
    "    # use accumulator to count the number of transitions (word pairs)\n",
    "    accum_total_other_words = sc.accumulator(0)\n",
    "    other_words.foreach(lambda x: accum_total_other_words.add(1))\n",
    "    total_other_words = float(accum_total_other_words.value)\n",
    "    \n",
    "    # reduce into count of unique word pairs\n",
    "    # RDD format: (('word1', 'word2'), frequency), \n",
    "    #             (('word2', 'word3'), frequency), ...\n",
    "    unique_other_words = other_words.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # aggregate by (and change key to) previous word\n",
    "    # RDD format: ('previous word', {'word1': word pair count, \n",
    "    #                                'word2': word pair count}}), ...\n",
    "    other_words_collapsed = unique_other_words.map(lambda x: \n",
    "        (x[0][0], (x[0][1], x[1]))) \\\n",
    "            .groupByKey().mapValues(dict)\n",
    "\n",
    "    # note: the above line of code is the slowest in the function\n",
    "    # (8.6 MB shuffle read and 4.5 MB shuffle write for big.txt)\n",
    "    # An alternative approach that aggregates lists with reduceByKey was\n",
    "    # attempted, but did not yield noticeable improvements in runtime.\n",
    "    \n",
    "    # convert counts to log-probabilities\n",
    "    # RDD format: ('previous word', {'word1': log-prob of pair, \n",
    "    #                                 word2: log-prob of pair}}), ...\n",
    "    transition_prob_calc = other_words_collapsed.mapValues(lambda v: \n",
    "        map_transition_prob(v))\n",
    "\n",
    "    # get default transition probabilities (for word pairs not in corpus)\n",
    "    default_transition_prob = math.log(1/total_other_words)\n",
    "    \n",
    "    # store transition probabilities as a dictionary (i.e. a lookup table)\n",
    "    # note: given the spell-checking algorithm, this cannot be maintained\n",
    "    # as an RDD as it is not possible to map within a map\n",
    "    transition_prob = transition_prob_calc.collectAsMap()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # generate dictionary\n",
    "    #\n",
    "    ############\n",
    "\n",
    "    # note: this approach is slightly different from the original SymSpell\n",
    "    # algorithm, but is more appropriate for a SPARK implementation\n",
    "    \n",
    "    # split into individual words (all)\n",
    "    # RDD format: 'word1', 'word2', 'word3', ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    all_words = make_all_lower.map(lambda line: regex.sub(' ', line)) \\\n",
    "            .flatMap(lambda line: line.split()).cache()\n",
    "\n",
    "    # use accumulator to count the number of words processed\n",
    "    accum_words_processed = sc.accumulator(0)\n",
    "    all_words.foreach(lambda x: accum_words_processed.add(1))\n",
    "\n",
    "    # add a count to each word\n",
    "    # RDD format: ('word1', 1), ('word2', 1), ('word3', 1), ...\n",
    "    count_once = all_words.map(lambda word: (word, 1))\n",
    "\n",
    "    # reduce into counts of unique words - this is the core corpus dictionary\n",
    "    # (i.e. only words appearing in the file, without 'deletes'))\n",
    "    # RDD format: ('word1', frequency), ('word2', frequency), ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    # note: imposing partitioning at this step yields a small \n",
    "    # improvement in runtime (~1 sec for big.txt) by equally\n",
    "    # balancing elements among workers for subsequent operations\n",
    "    unique_words_with_count = count_once.reduceByKey(lambda a, b: a + b, \n",
    "        numPartitions = num_partitions).cache()\n",
    "    \n",
    "    # use accumulator to count the number of unique words\n",
    "    accum_unique_words = sc.accumulator(0)\n",
    "    unique_words_with_count.foreach(lambda x: accum_unique_words.add(1))\n",
    "\n",
    "    # generate list of \"deletes\" for each word in the corpus\n",
    "    # RDD format: (word1, [deletes for word1]), (word2, [deletes for word2]), ...\n",
    "    generate_deletes = unique_words_with_count.map(lambda (parent, count): \n",
    "        (parent, get_deletes_list(parent, max_edit_distance)))\n",
    "    \n",
    "    # split into all key-value pairs\n",
    "    # RDD format: (word1, delete1), (word1, delete2), ...\n",
    "    expand_deletes = generate_deletes.flatMapValues(lambda x: x)\n",
    "    \n",
    "    # swap word order and add a zero count (because \"deletes\" were not\n",
    "    # present in the dictionary)\n",
    "    swap = expand_deletes.map(lambda (orig, delete): (delete, ([orig], 0)))\n",
    "    \n",
    "    # create a placeholder for each real word\n",
    "    # RDD format: ('word1', ([], frequency)), ('word2', ([], frequency)), ...\n",
    "    corpus = unique_words_with_count.mapValues(lambda count: ([], count))\n",
    "\n",
    "    # combine main dictionary and \"deletes\" (and eliminate duplicates)\n",
    "    # RDD format: ('word1', ([deletes for word1], frequency)), \n",
    "    #             ('word2', ([deletes for word2], frequency)), ...\n",
    "    combine = swap.union(corpus)\n",
    "    \n",
    "    # store dictionary items and deletes as a dictionary (i.e. a lookup table)\n",
    "    # note: given the spell-checking algorithm, this cannot be maintained\n",
    "    # as an RDD as it is not possible to map within a map\n",
    "    # note: use reduceByKeyLocally to avoid an extra shuffle from reduceByKey\n",
    "    dictionary = combine.reduceByKeyLocally(lambda a, b: (a[0]+b[0], a[1]+b[1])) \n",
    "    \n",
    "    # output stats\n",
    "    print 'Total words processed: %i' % accum_words_processed.value\n",
    "    print 'Total unique words in corpus: %i' % accum_unique_words.value \n",
    "    print 'Total items in dictionary (corpus words and deletions): %i' \\\n",
    "        % len(dictionary)\n",
    "    print '  Edit distance for deletions: %i' % max_edit_distance\n",
    "    print 'Total unique words at the start of a sentence: %i' \\\n",
    "        % len(start_prob)\n",
    "    print 'Total unique word transitions: %i' % len(transition_prob)\n",
    "    \n",
    "    return dictionary, start_prob, default_start_prob, \\\n",
    "            transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_file = 'testdata/big.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 9.78 µs\n",
      "Total words processed: 1105285\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary (corpus words and deletions): 2151998\n",
      "  Edit distance for deletions: 3\n",
      "Total unique words at the start of a sentence: 15356\n",
      "Total unique word transitions: 27086\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "    parallel_create_dictionary(dictionary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    '''\n",
    "    Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1\n",
    "    matrix. However, only the current and two previous rows are\n",
    "    needed at once, so we only store those.\n",
    "\n",
    "    Same code as word-level checking.\n",
    "    '''\n",
    "    \n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    \n",
    "    for x in xrange(len(seq1)):\n",
    "        \n",
    "        twoago, oneago, thisrow = \\\n",
    "            oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        \n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "                \n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "def get_suggestions(string, dictionary, max_edit_distance, \n",
    "                    longest_word_length=20, min_count=100, max_sug=10):\n",
    "    '''\n",
    "    Return list of suggested corrections for potentially incorrectly\n",
    "    spelled word.\n",
    "\n",
    "    Code based on get_suggestions function from word-level checking,\n",
    "    with the addition of the min_count and max_sug parameters.\n",
    "    - min_count: minimum number of times a word must have appeared\n",
    "    in the dictionary corpus to be considered a valid suggestion\n",
    "    - max_sug: number of suggestions that are returned (ranked by\n",
    "    frequency of appearance in dictionary corpus and edit distance\n",
    "    from word being checked)\n",
    "\n",
    "    These changes were imposed in order to ensure that the problem\n",
    "    remains tractable when checking very large documents. In practice,\n",
    "    the \"correct\" suggestion is almost always amongst the top ten.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > max_edit_distance:\n",
    "        # to ensure Viterbi can keep running -- use the word itself\n",
    "        return [(string, 0)]\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>0):\n",
    "            # word is in dictionary, and is a word from the corpus,\n",
    "            # and not already in suggestion list so add to suggestion\n",
    "            # dictionary, indexed by the word with value (frequency\n",
    "            # in corpus, edit distance)\n",
    "            # note: q_items that are not the input string are shorter\n",
    "            # than input string since only deletes are added (unless\n",
    "            # manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = \\\n",
    "                    (dictionary[q_item][1], len(string) - len(q_item))\n",
    "            \n",
    "            # the suggested corrections for q_item as stored in\n",
    "            # dictionary (whether or not q_item itself is a valid\n",
    "            # word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    \n",
    "                    # compute edit distance\n",
    "                    # suggested items should always be longer (unless\n",
    "                    # manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter\n",
    "                    # than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                    # item in suggestions list should not be the same\n",
    "                    # as the string itself\n",
    "                    assert sc_item!=string           \n",
    "                    # calculate edit distance using Damerau-\n",
    "                    # Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "                    \n",
    "                    if item_dist<=max_edit_distance:\n",
    "                        # should already be in dictionary if in\n",
    "                        # suggestion list\n",
    "                        assert sc_item in dictionary  \n",
    "                        # trim list to contain state space\n",
    "                        if (dictionary[q_item][1]>0): \n",
    "                            suggest_dict[sc_item] = \\\n",
    "                                (dictionary[sc_item][1], item_dist)\n",
    "        \n",
    "        # now generate deletes (e.g. a substring of string or of a\n",
    "        # delete) from the queue item as additional items to check\n",
    "        # -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<max_edit_distance \\\n",
    "            and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    # arbitrary value to identify we checked this\n",
    "                    q_dictionary[word_minus_c] = None\n",
    "\n",
    "    # return list of suggestions: (correction, edit distance)\n",
    "    \n",
    "    # only include words that have appeared a minimum number of times\n",
    "    # note: make sure that we do not lose the original word\n",
    "    as_list = [i for i in suggest_dict.items() \n",
    "               if (i[1][0]>min_count or i[0]==string)]\n",
    "    \n",
    "    # only include the most likely suggestions (based on frequency\n",
    "    # and edit distance from original word)\n",
    "    trunc_as_list = sorted(as_list, \n",
    "            key = lambda (term, (freq, dist)): (dist, -freq))[:max_sug]\n",
    "    \n",
    "    if len(trunc_as_list)==0:\n",
    "        # to ensure Viterbi can keep running\n",
    "        # -- use the word itself if no corrections are found\n",
    "        return [(string, 0)]\n",
    "        \n",
    "    else:\n",
    "        # drop the word frequency - not needed beyond this point\n",
    "        return [(i[0], i[1][1]) for i in trunc_as_list]\n",
    "\n",
    "    '''\n",
    "    Output format:\n",
    "    get_suggestions('file', dictionary)\n",
    "    [('file', 0), ('five', 1), ('fire', 1), ('fine', 1), ('will', 2),\n",
    "    ('time', 2), ('face', 2), ('like', 2), ('life', 2), ('while', 2)]\n",
    "    '''\n",
    "    \n",
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    '''\n",
    "    The emission probability, i.e. P(observed word|intended word)\n",
    "    is approximated by a Poisson(k, l) distribution, where \n",
    "    k=edit distance between the observed word and the intended\n",
    "    word and l=0.01.\n",
    "    \n",
    "    Both the overall approach and the parameter of l=0.01 are based on\n",
    "    the 2015 lecture notes from AM207 Stochastic Optimization.\n",
    "    Various parameters for lambda between 0 and 1 were tested, which\n",
    "    confirmed that 0.01 yields the most accurate word suggestions.\n",
    "    '''\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))\n",
    "\n",
    "######################\n",
    "# Multiple helper functions are used to avoid KeyErrors when\n",
    "# attempting to access values that are not present in dictionaries,\n",
    "# in which case the previously specified default value is returned.\n",
    "######################\n",
    "\n",
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    '''\n",
    "    P(word being at the beginning of a sentence)\n",
    "    '''\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob\n",
    "    \n",
    "def get_transition_prob(cur_word, prev_word, \n",
    "                        transition_prob, default_transition_prob):\n",
    "    '''\n",
    "    P(word|previous word)\n",
    "    '''\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob \n",
    "\n",
    "def get_sentence_word_id(words):\n",
    "    '''\n",
    "    Helper function: numbers each word according to its position\n",
    "    in the sentence.\n",
    "    '''\n",
    "    return [(i, w) for i, w in enumerate(words)]\n",
    "\n",
    "def start_word_prob(words, tmp_sp, d_sp):\n",
    "    '''\n",
    "    Helper function: calculates the probability of all word \n",
    "    suggestions being at the beginning of the sentence, based on\n",
    "    the pre-processed start probabilities and the emission model.\n",
    "    i.e. start probability x emission probability\n",
    "    '''\n",
    "    orig_word, sug_words = words\n",
    "    probs = [(w[0], math.exp(\n",
    "                get_start_prob(w[0], tmp_sp, d_sp) \n",
    "                + get_emission_prob(w[1])\n",
    "            )) \n",
    "             for w in sug_words]\n",
    "    sum_probs = sum([p[1] for p in probs])\n",
    "    probs = [([p[0]], math.log(p[1]/sum_probs)) for p in probs]\n",
    "    return probs\n",
    "\n",
    "def split_suggestions(sentence):\n",
    "    '''\n",
    "    Helper function: Splits into all the suggestions for a given\n",
    "    word, while retaining the previous path for all elements.\n",
    "    '''\n",
    "    sent_id, (word, word_sug)  = sentence\n",
    "    return [[sent_id, (word, w)] for w in word_sug]\n",
    "\n",
    "def normalize(probs):\n",
    "    '''\n",
    "    Helper function: normalizes probability so they add to 1.\n",
    "    Note: this is especially necessary given the small\n",
    "    probabilities that apply to this problem.\n",
    "    '''\n",
    "    sum_probs = sum([p[1] for p in probs])\n",
    "    return [(p[0], math.log(p[1]/sum_probs)) for p in probs]\n",
    "\n",
    "def get_max_prev_path(words, tmp_tp, d_tp):\n",
    "    '''\n",
    "    Helper function: Calculates the previous path that maximizes\n",
    "    the probability of the current word suggestion.\n",
    "    '''\n",
    "\n",
    "    # unpack values\n",
    "    cur_word = words[0][0]\n",
    "    cur_sug = words[0][1][0]\n",
    "    cur_sug_ed = words[0][1][1]\n",
    "    prev_sug = words[1]\n",
    "    \n",
    "    # belief + transition probability + emission probability\n",
    "    (prob, word) = max((p[1]\n",
    "                 + get_transition_prob(cur_sug, p[0][-1], tmp_tp, d_tp)\n",
    "                 + get_emission_prob(cur_sug_ed), p[0])\n",
    "                     for p in prev_sug)\n",
    "    \n",
    "    return word + [cur_sug], math.exp(prob)\n",
    "\n",
    "def get_max_path(final_paths):\n",
    "    '''\n",
    "    Helper function: at the final step, identifies the full path\n",
    "    (i.e. sentence correction) with the highest probability.\n",
    "    '''\n",
    "    return max((p[1], p[0]) for p in final_paths)[1]\n",
    "\n",
    "def get_count_mismatches(sentences):\n",
    "    '''\n",
    "    Helper function: compares the original sentence with the sentence\n",
    "    that has been suggested by the Viterbi algorithm, and calculates\n",
    "    the number of words that do not match.\n",
    "    '''\n",
    "    orig_sentence, sug_sentence = sentences\n",
    "    count_mismatches = len([(orig_sentence[i], sug_sentence[i]) \n",
    "            for i in range(len(orig_sentence))\n",
    "            if orig_sentence[i]!=sug_sentence[i]])\n",
    "    return count_mismatches, orig_sentence, sug_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_document_context_parallel_full(fname, dictionary,\n",
    "                             start_prob, default_start_prob,\n",
    "                             transition_prob, default_transition_prob,\n",
    "                             max_edit_distance=3, num_partitions=6,\n",
    "                             display_results=False):\n",
    "    \n",
    "    '''\n",
    "    Load a text file and spell-check each sentence using the\n",
    "    dictionary and probability tables that were created in the\n",
    "    pre-processing stage.\n",
    "\n",
    "    Suggested corrections are either printed to the screen or\n",
    "    saved in a log file, depending on the settings.\n",
    "    '''\n",
    "\n",
    "    ############\n",
    "    #\n",
    "    # load file & initial processing\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # http://stackoverflow.com/questions/22520932/python-remove-all-non-alphabet-chars-from-string\n",
    "    regex = re.compile('[^a-z ]')\n",
    "    \n",
    "    # broadcast Python dictionaries to workers (from pre-processing)\n",
    "    bc_dictionary = sc.broadcast(dictionary)\n",
    "    bc_start_prob = sc.broadcast(start_prob)\n",
    "    bc_transition_prob = sc.broadcast(transition_prob)\n",
    "    \n",
    "    # load file contents and convert into one long sequence of words\n",
    "    # RDD format: 'line 1', 'line 2', 'line 3', ...\n",
    "    make_all_lower = sc.textFile(fname) \\\n",
    "            .map(lambda line: line.lower()) \\\n",
    "            .filter(lambda x: x!='')\n",
    "    \n",
    "    # split into individual sentences and remove other punctuation\n",
    "    # RDD format: [words of sentence1], [words of sentence2], ...\n",
    "    # cache because this RDD is used in multiple operations \n",
    "    split_sentence = make_all_lower.flatMap(lambda \n",
    "        line: line.replace('?','.').replace('!','.').split('.')) \\\n",
    "             .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "             .map(lambda sentence: sentence.split()) \\\n",
    "             .filter(lambda x: x!=[]).cache()\n",
    "    \n",
    "    # use accumulator to count the number of words checked\n",
    "    accum_total_words = sc.accumulator(0)\n",
    "    split_sentence.flatMap(lambda x: x) \\\n",
    "            .foreach(lambda x: accum_total_words.add(1))\n",
    "    \n",
    "    # assign a unique id to each sentence\n",
    "    # RDD format: (0, [words of sentence1]), (1, [words of sentence2]), ...\n",
    "    # partition and cache here after completing transformations - this\n",
    "    # RDD is used in multiple operations and the sentence id will\n",
    "    # remain the key from this point forward\n",
    "    sentence_id = split_sentence.zipWithIndex().map(\n",
    "        lambda (k, v): (v, k)).partitionBy(num_partitions).cache()\n",
    "    \n",
    "    # count the number of words in each sentence - this is used to\n",
    "    # determine when each sentence is done processing\n",
    "    # RDD format: (0, words in sentence1), (1, words in sentence2), ...\n",
    "    # cache as this RDD is called at every iteration\n",
    "    sentence_word_count = sentence_id.mapValues(lambda v: len(v)).cache()\n",
    "    \n",
    "    ############\n",
    "    #\n",
    "    # spell-checking\n",
    "    #\n",
    "    ############\n",
    "\n",
    "    # number each word in a sentence, and split into individual words\n",
    "    # RDD format: (sentence1 id, (word1 id, word1)), \n",
    "    #             (sentence1 id, (word2 id, word2), ...\n",
    "    sentence_word_id = sentence_id.mapValues(lambda v: get_sentence_word_id(v)) \\\n",
    "            .flatMapValues(lambda x: x)\n",
    "\n",
    "    print 'sentence_word_id'\n",
    "    \n",
    "    # get suggestions for each word\n",
    "    # RDD format: (sentence1 id, (word1 id, word1, [suggestions for word1])), \n",
    "    #             (sentence1 id, (word2 id, word2, [suggestions for word2]), ...\n",
    "    # cache as this RDD is called at each iteration\n",
    "    sentence_word_suggestions = sentence_word_id.mapValues(\n",
    "        lambda v: (v[0], v[1], get_suggestions(v[1], bc_dictionary.value,\n",
    "            max_edit_distance))).cache()\n",
    "\n",
    "    print 'sentence_word_suggestions'\n",
    "    \n",
    "    # filter for all the first words in sentences\n",
    "    # RDD format: (sentence id, (0, word, [suggestions for word])), \n",
    "    #             (sentence id, (0, word, [suggestions for word]), ...\n",
    "    sentence_word_1 = sentence_word_suggestions.filter(lambda (k, v): v[0]==0) \\\n",
    "            .mapValues(lambda v: (v[1], v[2]))\n",
    "\n",
    "    print 'sentence_word_1'\n",
    "    \n",
    "    # calculate probability for each suggestion\n",
    "    # RDD format: (sentence id, [([word], P(word)), ([word], P(word)), ...]), \n",
    "    #             (sentence id, [([word], P(word)), ([word], P(word)), ...]), ...\n",
    "    sentence_path = sentence_word_1.mapValues(lambda v: \n",
    "            start_word_prob(v, bc_start_prob.value, default_start_prob))\n",
    "\n",
    "    print 'sentence_path'\n",
    "\n",
    "    # start loop from second word (zero-indexed)\n",
    "    word_num = 1\n",
    "    \n",
    "    # extract any sentences that have been fully processed\n",
    "    # RDD format: (sentence id, [([path], P(path)), ([path], P(path)), ...]), \n",
    "    #             (sentence id, [([path], P(path)), ([path], P(path)), ...]), ...\n",
    "    completed = sentence_word_count.filter(lambda (k, v): v==word_num) \\\n",
    "            .join(sentence_path).mapValues(lambda v: v[1]).cache()\n",
    "    \n",
    "    print 'completed'\n",
    "\n",
    "    # filter for the next words in sentences\n",
    "    # RDD format: (sentence id, (word, [suggestions for word])), \n",
    "    #             (sentence id, (word, [suggestions for word]), ...\n",
    "    sentence_word_next = sentence_word_suggestions.filter(lambda \n",
    "        (k,v): v[0]==word_num) \\\n",
    "            .mapValues(lambda v: (v[1], v[2])).cache()\n",
    "    \n",
    "    print 'Starting loop'\n",
    "\n",
    "    # check whether there are any words left to process\n",
    "    while not sentence_word_next.isEmpty():\n",
    "\n",
    "        print word_num\n",
    "\n",
    "        # split by suggestions, while retaining previous path\n",
    "        # RDD format: (sentence id, (word, (suggested word, edit distance)), \n",
    "        #                    [previous path]), ...\n",
    "        # use preservesPartitioning to signal that the sentence id\n",
    "        # continues to be the key\n",
    "        sentence_word_next_split = sentence_word_next.flatMap(lambda x: \n",
    "            split_suggestions(x), preservesPartitioning=True)\n",
    "        \n",
    "        print 'sentence_word_next_split'\n",
    "\n",
    "        # join each suggestion with the previous path\n",
    "        # RDD format:\n",
    "        # (sentence id, ((current word, \n",
    "        #          (current word suggestion, edit distance)), \n",
    "        #               [(previous path-probability pairs)])), ...\n",
    "        sentence_word_next_path = sentence_word_next_split.join(sentence_path)\n",
    "\n",
    "        print 'sentence_word_next_path'\n",
    "        \n",
    "        # identify previous path that maximizes the probability \n",
    "        # of each suggested word correction\n",
    "        # RDD format: (sentence id, ([path], path probability)),\n",
    "        #             (sentence id, ([path], path probability)), ...\n",
    "        sentence_word_next_path_prob = sentence_word_next_path \\\n",
    "            .mapValues(lambda v: get_max_prev_path(v, \n",
    "                bc_transition_prob.value, default_transition_prob))\n",
    "\n",
    "        print 'sentence_word_next_path_prob'\n",
    "        \n",
    "        # group all the new paths for each sentence and normalize\n",
    "        # for numerical stability\n",
    "        # RDD format: (sentence id, [([path], P(path)), ([path], P(path)), ...]), \n",
    "        #             (sentence id, [([path], P(path)), ([path], P(path)), ...]), ...\n",
    "        sentence_path = sentence_word_next_path_prob.groupByKey() \\\n",
    "                .mapValues(lambda v: normalize(v))\n",
    "\n",
    "        print 'sentence_path'\n",
    "        \n",
    "        # move on to next word\n",
    "        word_num += 1\n",
    "        \n",
    "        # extract any sentences that have been fully processed\n",
    "        # RDD format: (sentence id, [([path], P(path)), ([path], P(path)), ...]), \n",
    "        #             (sentence id, [([path], P(path)), ([path], P(path)), ...]), ...\n",
    "        # cache as this is carried over to the next iteration\n",
    "        completed = completed \\\n",
    "            .union(sentence_word_count.filter(lambda (k, v): v==word_num) \\\n",
    "            .join(sentence_path) \\\n",
    "            .mapValues(lambda v: v[1])).cache()\n",
    "\n",
    "        print 'completed'\n",
    "        \n",
    "        # filter for the next words in sentences\n",
    "        # RDD format: (sentence id, (word, [suggestions for word])), \n",
    "        #             (sentence id, (word, [suggestions for word]), ...\n",
    "        # cache as this is carried over to the next iteration\n",
    "        sentence_word_next = sentence_word_suggestions.filter(\n",
    "            lambda (k, v): v[0]==word_num) \\\n",
    "                .mapValues(lambda v: (v[1], v[2])).cache()\n",
    "\n",
    "        print 'sentence_word_next'\n",
    "        \n",
    "    # get most likely path (sentence)\n",
    "    # RDD format: (sentence id, [suggested sentence]),\n",
    "    #             (sentence id, [suggested sentence]), ...\n",
    "    sentence_suggestion = completed.mapValues(lambda v: get_max_path(v))\n",
    "\n",
    "    print 'sentence_suggestion'\n",
    "\n",
    "    # join with original path (sentence)\n",
    "    # RDD format: (sentence id, ([original sentence], [suggested sentence])),\n",
    "    #             (sentence id, ([original sentence], [suggested sentence])), ...\n",
    "    sentence_max_prob = sentence_id.join(sentence_suggestion)\n",
    "\n",
    "    print 'sentence_max_prob'\n",
    "        \n",
    "    ############\n",
    "    #\n",
    "    # output results\n",
    "    #\n",
    "    ############\n",
    "    \n",
    "    # count the number of errors per sentence and drop any sentences\n",
    "    # without errors\n",
    "    # RDD format: (sentence id, (# errors, [original sentence], [suggested sentence])),\n",
    "    #             (sentence id, (# errors, [original sentence], [suggested sentence])), ...\n",
    "    sentence_errors = sentence_max_prob.mapValues(\n",
    "        lambda v: get_count_mismatches(v)) \\\n",
    "            .filter(lambda (k, v): v[0]>0)\n",
    "\n",
    "    print 'sentence_errors'\n",
    "               \n",
    "    # collect all sentences with identified errors (as list)\n",
    "    sentence_errors_list = sentence_errors.collect()\n",
    "\n",
    "    print 'COLLECT COMPLETED'\n",
    "    \n",
    "    # count the number of potentially misspelled words\n",
    "    num_errors = sum([s[1][0] for s in sentence_errors_list])\n",
    "    \n",
    "    # print suggested corrections\n",
    "    if display_results:\n",
    "        for sentence in sentence_errors_list:\n",
    "            print 'Sentence %i: %s --> %s' % (sentence[0],\n",
    "                ' '.join(sentence[1][1]), ' '.join(sentence[1][2]))\n",
    "            print '-----'\n",
    "    \n",
    "    # output suggested corrections to file\n",
    "    else:\n",
    "        f = open('spell-log.txt', 'w')\n",
    "        for sentence in sentence_errors_list:\n",
    "            f.write('Sentence %i: %s --> %s\\n' % (sentence[0], \n",
    "                ' '.join(sentence[1][1]), ' '.join(sentence[1][2])))\n",
    "        f.close()\n",
    "    \n",
    "    print '-----'\n",
    "    print 'Total words checked: %i' % accum_total_words.value\n",
    "    print 'Total potential errors found: %i' % num_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_file = 'testdata/yelp1review.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 2 µs, total: 4 µs\n",
      "Wall time: 5.01 µs\n",
      "sentence_word_id\n",
      "sentence_word_suggestions\n",
      "sentence_word_1\n",
      "sentence_path\n",
      "completed\n",
      "Starting loop\n",
      "1\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "2\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "3\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "4\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "5\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "6\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "7\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "8\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "9\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "10\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "11\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "12\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "13\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "14\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "15\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "16\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "17\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "18\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "19\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "20\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "21\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "22\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "23\n",
      "sentence_word_next_split\n",
      "sentence_word_next_path\n",
      "sentence_word_next_path_prob\n",
      "sentence_path\n",
      "completed\n",
      "sentence_word_next\n",
      "sentence_suggestion\n",
      "sentence_max_prob\n",
      "sentence_errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-195102f803c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m correct_document_context_parallel_full(check_file, dictionary,\n\u001b[1;32m      3\u001b[0m                         \u001b[0mstart_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_start_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                         transition_prob, default_transition_prob)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-e72e9a8d3ddd>\u001b[0m in \u001b[0;36mcorrect_document_context_parallel_full\u001b[0;34m(fname, dictionary, start_prob, default_start_prob, transition_prob, default_transition_prob, max_edit_distance, num_partitions, display_results)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# collect all sentences with identified errors (as list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0msentence_errors_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'COLLECT COMPLETED'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \"\"\"\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m    538\u001b[0m                 self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_give_back_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JNetworkError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/opt/apache-spark/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    432\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                         \u001b[0;32mwhile\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m                             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time\n",
    "correct_document_context_parallel_full(check_file, dictionary,\n",
    "                        start_prob, default_start_prob, \n",
    "                        transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile('[^a-z ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bc_dictionary = sc.broadcast(dictionary)\n",
    "bc_start_prob = sc.broadcast(start_prob)\n",
    "bc_transition_prob = sc.broadcast(transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'this place was delicious!!  my parents saw a recommendation to visit this place from rick sebak\\'s \"25 things i like about pittsburgh\" and he\\'s usually pretty accurate.  his recommendations were to try the reuben, fish sandwich and open-faced steak sandwich.  we went early afternoon for a late lunch today (a saturday) and were seated right away.  the staff is extremely friendly.  my mom & i each had the fish sandwich, while my dad & brother had a reuben sandwich.  the fish was very good, but the reuben was to die for!  both dishes were massive, and could very easily be shared between two people.  on top of being extremely large portions, it was incredibly affordable.  the giant fish sandwich was $8 and the giant reuben was $7.50.  our drinks were always filled and we were checked on several times during the meal.  we will definitely be back!!!  oh and a bit of advice ahead of time - they take cash only.  so come prepared, but i\\'m pretty sure i saw an atm there as well.  and i do believe they are closed on sundays & mondays.,can\\'t miss stop for the best fish sandwich in pittsburgh.,this place should have a lot more reviews - but i\\'m glad it doesn\\'t, they don\\'t need to get any busier.',\n",
       " u\"its been there ages, and looks it. if you're all about ambiance, don't bother. if you pretend you're in a movie set in pittsburgh 30 years ago it works pretty well. the service is sometimes hit or miss. most of girls are good, one is very slow, one is amazing. they are all friendly and usually a few different people will check in to make sure that you're happy. everything is made fresh so be prepared that nothing comes flying out of that kitchen - busy times it can take a good while to get food. \",\n",
       " u'the food is awesome! worth any little complaints i might think up before it gets there. once its on the table, i forget them all.',\n",
       " u'-fish sandwiich',\n",
       " u'-salmon (huge and delicious)']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_all_lower = sc.textFile(check_file) \\\n",
    "        .map(lambda line: line.lower()) \\\n",
    "        .filter(lambda x: x!='')\n",
    "make_all_lower.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'this', u'place', u'was', u'delicious'],\n",
       " [u'my',\n",
       "  u'parents',\n",
       "  u'saw',\n",
       "  u'a',\n",
       "  u'recommendation',\n",
       "  u'to',\n",
       "  u'visit',\n",
       "  u'this',\n",
       "  u'place',\n",
       "  u'from',\n",
       "  u'rick',\n",
       "  u'sebak',\n",
       "  u's',\n",
       "  u'things',\n",
       "  u'i',\n",
       "  u'like',\n",
       "  u'about',\n",
       "  u'pittsburgh',\n",
       "  u'and',\n",
       "  u'he',\n",
       "  u's',\n",
       "  u'usually',\n",
       "  u'pretty',\n",
       "  u'accurate'],\n",
       " [u'his',\n",
       "  u'recommendations',\n",
       "  u'were',\n",
       "  u'to',\n",
       "  u'try',\n",
       "  u'the',\n",
       "  u'reuben',\n",
       "  u'fish',\n",
       "  u'sandwich',\n",
       "  u'and',\n",
       "  u'open',\n",
       "  u'faced',\n",
       "  u'steak',\n",
       "  u'sandwich'],\n",
       " [u'we',\n",
       "  u'went',\n",
       "  u'early',\n",
       "  u'afternoon',\n",
       "  u'for',\n",
       "  u'a',\n",
       "  u'late',\n",
       "  u'lunch',\n",
       "  u'today',\n",
       "  u'a',\n",
       "  u'saturday',\n",
       "  u'and',\n",
       "  u'were',\n",
       "  u'seated',\n",
       "  u'right',\n",
       "  u'away'],\n",
       " [u'the', u'staff', u'is', u'extremely', u'friendly']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_sentence = make_all_lower.flatMap(lambda \n",
    "    line: line.replace('?','.').replace('!','.').split('.')) \\\n",
    "         .map(lambda sentence: regex.sub(' ', sentence)) \\\n",
    "         .map(lambda sentence: sentence.split()) \\\n",
    "         .filter(lambda x: x!=[]).cache()\n",
    "split_sentence.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accum_total_words = sc.accumulator(0)\n",
    "split_sentence.flatMap(lambda x: x) \\\n",
    "        .foreach(lambda x: accum_total_words.add(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
