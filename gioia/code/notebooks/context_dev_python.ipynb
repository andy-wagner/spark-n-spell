{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Based on SymSpell:\n",
    "\n",
    "Originally written in C#:\n",
    "\n",
    "// SymSpell: 1 million times faster through Symmetric Delete spelling correction algorithm\n",
    "//\n",
    "// The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup \n",
    "// for a given Damerau-Levenshtein distance. It is six orders of magnitude faster and language independent.\n",
    "// Opposite to other algorithms only deletes are required, no transposes + replaces + inserts.\n",
    "// Transposes + replaces + inserts of the input term are transformed into deletes of the dictionary term.\n",
    "// Replaces and inserts are expensive and language dependent: e.g. Chinese has 70,000 Unicode Han characters!\n",
    "//\n",
    "// Copyright (C) 2015 Wolf Garbe\n",
    "// Version: 3.0\n",
    "// Author: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// Maintainer: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// URL: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "// Description: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "//\n",
    "// License:\n",
    "// This program is free software; you can redistribute it and/or modify\n",
    "// it under the terms of the GNU Lesser General Public License, \n",
    "// version 3.0 (LGPL-3.0) as published by the Free Software Foundation.\n",
    "// http://www.opensource.org/licenses/LGPL-3.0\n",
    "//\n",
    "// Usage: single word + Enter:  Display spelling suggestions\n",
    "//        Enter without input:  Terminate the program\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math # GD: needed to calculate logs below\n",
    "from scipy.stats import poisson # GD: needed to calculate emission probability\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_edit_distance = 3\n",
    "not_found_str = '<not found>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deletes_list(w):\n",
    "    '''given a word, derive strings with up to max_edit_distance characters deleted'''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(max_edit_distance):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary_entry(w, dictionary, longest_word_length):\n",
    "    '''add word and its derived deletions to dictionary'''\n",
    "    # check if word is already in dictionary\n",
    "    # dictionary entries are in the form: (list of suggested corrections, frequency of word in corpus)\n",
    "\n",
    "    new_real_word_added = False\n",
    "    if w in dictionary:\n",
    "        dictionary[w] = (dictionary[w][0], dictionary[w][1] + 1)  # increment count of word in corpus\n",
    "    else:\n",
    "        dictionary[w] = ([], 1)  \n",
    "        longest_word_length = max(longest_word_length, len(w))\n",
    "        \n",
    "    if dictionary[w][1]==1:\n",
    "        # first appearance of word in corpus\n",
    "        # n.b. word may already be in dictionary as a derived word (deleting character from a real word)\n",
    "        # but counter of frequency of word in corpus is not incremented in those cases)\n",
    "        \n",
    "        new_real_word_added = True\n",
    "        deletes = get_deletes_list(w)\n",
    "        \n",
    "        for item in deletes:\n",
    "            if item in dictionary:\n",
    "                # add (correct) word to delete's suggested correction list if not already there\n",
    "                if item not in dictionary[item][0]:\n",
    "                    dictionary[item][0].append(w)\n",
    "            else:\n",
    "                dictionary[item] = ([w], 0)  # note frequency of word in corpus is not incremented\n",
    "        \n",
    "    return new_real_word_added, longest_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dictionary(fname):\n",
    "    '''\n",
    "    Loads a text file and uses it to create a dictionary and\n",
    "    to calculate start probabilities and transition probabilities. \n",
    "    Please refer to the text above for a full description.\n",
    "    '''\n",
    "    \n",
    "    print 'Creating dictionary...'\n",
    "\n",
    "    dictionary = dict()\n",
    "    longest_word_length = 0\n",
    "    start_prob = dict()\n",
    "    transition_prob = dict()\n",
    "    word_count = 0\n",
    "    transitions = 0\n",
    "    words_processed = 0\n",
    "    test = []\n",
    "    \n",
    "    with open(fname) as file:    \n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            # process each sentence separately\n",
    "            for sentence in line.split('.'):\n",
    "                \n",
    "                # separate by words by non-alphabetical characters\n",
    "                words = re.findall('[a-z]+', sentence.lower())      \n",
    "                \n",
    "                for w, word in enumerate(words):\n",
    "                    \n",
    "                    new_word, longest_word_length = \\\n",
    "                        create_dictionary_entry(word, dictionary,\n",
    "                                                longest_word_length)\n",
    "                    \n",
    "                    if new_word:\n",
    "                        word_count += 1\n",
    "                    words_processed += 1\n",
    "                        \n",
    "                    # update probabilities for Hidden Markov Model\n",
    "                    if w == 0:\n",
    "\n",
    "                        # probability of a word being at the\n",
    "                        # beginning of a sentence\n",
    "                        if word in start_prob:\n",
    "                            start_prob[word] += 1\n",
    "                        else:\n",
    "                            start_prob[word] = 1\n",
    "                    else:\n",
    "                        \n",
    "                        # probability of transitionining from one\n",
    "                        # word to another\n",
    "                        # dictionary format:\n",
    "                        # {previous word: {word1 : P(word1|prevous\n",
    "                        # word), word2 : P(word2|prevous word)}}\n",
    "                        \n",
    "                        # check that prior word is present\n",
    "                        # - create if not\n",
    "                        test.append(word)\n",
    "                        \n",
    "                        if words[w - 1] not in transition_prob:\n",
    "                            transition_prob[words[w - 1]] = dict()\n",
    "                            \n",
    "                        # check that current word is present\n",
    "                        # - create if not\n",
    "                        if word not in transition_prob[words[w - 1]]:\n",
    "                            transition_prob[words[w - 1]][word] = 0\n",
    "                            \n",
    "                        # update value\n",
    "                        transition_prob[words[w - 1]][word] += 1\n",
    "                        transitions += 1\n",
    "                    \n",
    "    # convert counts to log-probabilities, to avoid underflow in\n",
    "    # later calculations (note: natural logarithm, not base-10)\n",
    "    \n",
    "    total_start_words = float(sum(start_prob.values()))\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    start_prob.update( \n",
    "        {k: math.log(v/total_start_words)\n",
    "         for k, v in start_prob.items()})\n",
    "\n",
    "    default_transition_prob = math.log(1./transitions)\n",
    "    transition_prob.update(\n",
    "        {k: {k1: math.log(float(v1)/sum(v.values()))\n",
    "             for k1, v1 in v.items()} \n",
    "         for k, v in transition_prob.items()})\n",
    "\n",
    "    print 'Total words processed: %i' % words_processed\n",
    "    print 'Total unique words in corpus: %i' % word_count\n",
    "    print 'Total items in dictionary: %i' \\\n",
    "        % len(dictionary)\n",
    "    print '  Edit distance for deletions: %i' % max_edit_distance\n",
    "    print '  Length of longest word: %i' % longest_word_length\n",
    "    print 'Total unique words at the start of a sentence: %i' \\\n",
    "        % len(start_prob)\n",
    "    print 'Total unique word transitions: %i' % len(transition_prob)\n",
    "        \n",
    "    return dictionary, longest_word_length, start_prob, \\\n",
    "        default_start_prob, transition_prob, default_transition_prob, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating dictionary...\n",
      "Total words processed: 1105285\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary: 2151998\n",
      "  Edit distance for deletions: 3\n",
      "  Length of longest word: 18\n",
      "Total unique words at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n",
      "CPU times: user 48.8 s, sys: 1.25 s, total: 50.1 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob, test = \\\n",
    "    create_dictionary(\"testdata/big.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Can look up a specific entry in the dictionary below. <br>\n",
    "shows (possible corrections, and frequency that entry itself is in corpus - 0 if not a real word) <br>\n",
    "Note: will return key error if there are no corrections (because we are accessing dictionary directly here)\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['essentially', 'essentials'], 92)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print dictionary[\"essential\"]\n",
    "except KeyError:\n",
    "    print 'Not in dictionary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['wrack'], 0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print dictionary[\"wack\"]\n",
    "except KeyError:\n",
    "    print 'Not in dictionary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Word-level correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    \n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    for x in xrange(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suggestions(string, dictionary, longest_word_length, silent=False, min_count=0):\n",
    "    '''\n",
    "    return list of suggested corrections for potentially incorrectly spelled word\n",
    "    '''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > max_edit_distance:\n",
    "        if not silent:\n",
    "            print \"no items in dictionary within maximum edit distance\"\n",
    "        return []\n",
    "    \n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        # print \"processing '%s'\" % q_item\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>=min_count):# GD added to trim list for context checking\n",
    "            # word is in dictionary, and is a word from the corpus, and not already in suggestion list\n",
    "            # so add to suggestion dictionary, indexed by the word with value (frequency in corpus, edit distance)\n",
    "            # note q_items that are not the input string are shorter than input string \n",
    "            # since only deletes are added (unless manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = (dictionary[q_item][1], len(string) - len(q_item))\n",
    "\n",
    "            ## the suggested corrections for q_item as stored in dictionary (whether or not\n",
    "            ## q_item itself is a valid word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    # compute edit distance\n",
    "                    # if len(sc_item)==len(q_item):\n",
    "                    #    item_dist = len(string) - len(q_item)\n",
    "                    # suggested items should always be longer (unless manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "                    #elif len(q_item)==len(string):\n",
    "                        # a suggestion could be longer or shorter than original string (bug in original FAROO?)\n",
    "                        # if suggestion is from string's suggestion list, sc_item will be longer\n",
    "                        # if suggestion is from a delete's suggestion list, sc_item may be shorter\n",
    "                    #   item_dist = abs(len(sc_item) - len(q_item))\n",
    "                    #else:\n",
    "                    # check in original code, but probably not necessary because string has already checked\n",
    "                    assert sc_item!=string\n",
    "\n",
    "                    # calculate edit distance using, for example, Damerau-Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "\n",
    "                    if item_dist<=max_edit_distance:\n",
    "                        assert sc_item in dictionary  # should already be in dictionary if in suggestion list\n",
    "                        if (dictionary[q_item][1]>=min_count):# GD added to trim list for context checking\n",
    "                            suggest_dict[sc_item] = (dictionary[sc_item][1], item_dist)\n",
    "\n",
    "        # now generate deletes (e.g. a substring of string or of a delete) from the queue item\n",
    "        # as additional items to check -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<max_edit_distance and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
    "             \n",
    "    # queue is now empty: convert suggestions in dictionary to list for output\n",
    "    \n",
    "    if not silent:\n",
    "        print \"number of possible corrections: %i\" %len(suggest_dict)\n",
    "        print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    \n",
    "    # output option 1\n",
    "    # sort results by ascending order of edit distance and descending order of frequency\n",
    "    #     and return list of suggested corrections only:\n",
    "    # return sorted(suggest_dict, key = lambda x: (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "    # output option 2\n",
    "    # return list of suggestions with (correction, (frequency in corpus, edit distance)):\n",
    "    as_list = suggest_dict.items()\n",
    "    return sorted(as_list, key = lambda (term, (freq, dist)): (dist, -freq))\n",
    "\n",
    "    '''\n",
    "    Option 1:\n",
    "    get_suggestions(\"file\")\n",
    "    ['file', 'five', 'fire', 'fine', ...]\n",
    "    \n",
    "    Option 2:\n",
    "    get_suggestions(\"file\")\n",
    "    [('file', (5, 0)),\n",
    "     ('five', (67, 1)),\n",
    "     ('fire', (54, 1)),\n",
    "     ('fine', (17, 1))...]  \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Type in word to correct below, to test and get whole list of possible suggestions.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible corrections: 142\n",
      "  edit distance for deletions: 3\n",
      "CPU times: user 16.6 ms, sys: 4.11 ms, total: 20.7 ms\n",
      "Wall time: 17.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mitten', (0, 1)),\n",
       " ('mittes', (0, 1)),\n",
       " ('ittens', (0, 1)),\n",
       " ('matters', (136, 2)),\n",
       " ('bitten', (13, 2)),\n",
       " ('kitten', (7, 2)),\n",
       " ('listens', (2, 2)),\n",
       " ('battens', (1, 2)),\n",
       " ('smitten', (1, 2)),\n",
       " ('itten', (0, 2)),\n",
       " ('mites', (0, 2)),\n",
       " ('miten', (0, 2)),\n",
       " ('ittes', (0, 2)),\n",
       " ('mtten', (0, 2)),\n",
       " ('mttes', (0, 2)),\n",
       " ('ttens', (0, 2)),\n",
       " ('itens', (0, 2)),\n",
       " ('miens', (0, 2)),\n",
       " ('mittn', (0, 2)),\n",
       " ('mitte', (0, 2)),\n",
       " ('ittns', (0, 2)),\n",
       " ('mitts', (0, 2)),\n",
       " ('matter', (365, 3)),\n",
       " ('sitting', (269, 3)),\n",
       " ('minutes', (146, 3)),\n",
       " ('written', (117, 3)),\n",
       " ('miles', (110, 3)),\n",
       " ('citizens', (109, 3)),\n",
       " ('letters', (108, 3)),\n",
       " ('listen', (100, 3)),\n",
       " ('cities', (77, 3)),\n",
       " ('bitter', (47, 3)),\n",
       " ('masters', (37, 3)),\n",
       " ('intense', (34, 3)),\n",
       " ('witness', (33, 3)),\n",
       " ('attend', (29, 3)),\n",
       " ('mistress', (24, 3)),\n",
       " ('fitted', (23, 3)),\n",
       " ('mines', (22, 3)),\n",
       " ('fitting', (21, 3)),\n",
       " ('miners', (19, 3)),\n",
       " ('mitenka', (16, 3)),\n",
       " ('tens', (16, 3)),\n",
       " ('sisters', (16, 3)),\n",
       " ('intent', (13, 3)),\n",
       " ('mothers', (12, 3)),\n",
       " ('mutton', (11, 3)),\n",
       " ('bites', (10, 3)),\n",
       " ('sites', (9, 3)),\n",
       " ('omitted', (9, 3)),\n",
       " ('buttons', (8, 3)),\n",
       " ('fitness', (8, 3)),\n",
       " ('rotten', (8, 3)),\n",
       " ('attends', (8, 3)),\n",
       " ('matrena', (8, 3)),\n",
       " ('intend', (8, 3)),\n",
       " ('titles', (7, 3)),\n",
       " ('pitted', (6, 3)),\n",
       " ('matted', (6, 3)),\n",
       " ('omitting', (5, 3)),\n",
       " ('witted', (5, 3)),\n",
       " ('kitchens', (5, 3)),\n",
       " ('hitting', (5, 3)),\n",
       " ('intends', (5, 3)),\n",
       " ('emitted', (4, 3)),\n",
       " ('items', (4, 3)),\n",
       " ('mutter', (4, 3)),\n",
       " ('pitting', (4, 3)),\n",
       " ('mates', (4, 3)),\n",
       " ('emitting', (3, 3)),\n",
       " ('litter', (3, 3)),\n",
       " ('dites', (3, 3)),\n",
       " ('mister', (2, 3)),\n",
       " ('distend', (2, 3)),\n",
       " ('withers', (2, 3)),\n",
       " ('mints', (2, 3)),\n",
       " ('motions', (2, 3)),\n",
       " ('cutters', (2, 3)),\n",
       " ('mien', (2, 3)),\n",
       " ('linens', (2, 3)),\n",
       " ('hastens', (2, 3)),\n",
       " ('softens', (2, 3)),\n",
       " ('gotten', (2, 3)),\n",
       " ('misses', (2, 3)),\n",
       " ('dickens', (2, 3)),\n",
       " ('viens', (2, 3)),\n",
       " ('winters', (2, 3)),\n",
       " ('matins', (2, 3)),\n",
       " ('potters', (2, 3)),\n",
       " ('amiens', (1, 3)),\n",
       " ('metes', (1, 3)),\n",
       " ('filters', (1, 3)),\n",
       " ('mattress', (1, 3)),\n",
       " ('pickens', (1, 3)),\n",
       " ('distends', (1, 3)),\n",
       " ('kittenish', (1, 3)),\n",
       " ('gutters', (1, 3)),\n",
       " ('rioters', (1, 3)),\n",
       " ('hatters', (1, 3)),\n",
       " ('sittings', (1, 3)),\n",
       " ('fetters', (1, 3)),\n",
       " ('fatten', (1, 3)),\n",
       " ('remittent', (1, 3)),\n",
       " ('imitates', (1, 3)),\n",
       " ('pities', (1, 3)),\n",
       " ('utters', (1, 3)),\n",
       " ('athens', (1, 3)),\n",
       " ('tiens', (1, 3)),\n",
       " ('maidens', (1, 3)),\n",
       " ('moyens', (1, 3)),\n",
       " ('matting', (1, 3)),\n",
       " ('milton', (1, 3)),\n",
       " ('mists', (1, 3)),\n",
       " ('hittel', (1, 3)),\n",
       " ('cottons', (1, 3)),\n",
       " ('intents', (1, 3)),\n",
       " ('midges', (1, 3)),\n",
       " ('wotten', (1, 3)),\n",
       " ('patterns', (1, 3)),\n",
       " ('mtns', (0, 3)),\n",
       " ('ites', (0, 3)),\n",
       " ('iten', (0, 3)),\n",
       " ('mten', (0, 3)),\n",
       " ('mies', (0, 3)),\n",
       " ('itns', (0, 3)),\n",
       " ('mitt', (0, 3)),\n",
       " ('mits', (0, 3)),\n",
       " ('mitn', (0, 3)),\n",
       " ('ittn', (0, 3)),\n",
       " ('itte', (0, 3)),\n",
       " ('mtes', (0, 3)),\n",
       " ('mite', (0, 3)),\n",
       " ('mins', (0, 3)),\n",
       " ('mens', (0, 3)),\n",
       " ('ttns', (0, 3)),\n",
       " ('ttes', (0, 3)),\n",
       " ('mtte', (0, 3)),\n",
       " ('itts', (0, 3)),\n",
       " ('iens', (0, 3)),\n",
       " ('mttn', (0, 3)),\n",
       " ('mtts', (0, 3)),\n",
       " ('tten', (0, 3))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_suggestions(\"mittens\", dictionary, longest_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.98 s, sys: 15.9 ms, total: 2 s\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"acamodation\", dictionary, longest_word_length, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 15.8 ms, total: 2.28 s\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"acomodation\", dictionary, longest_word_length, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.7 s, sys: 329 ms, total: 43.1 s\n",
      "Wall time: 44.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"hous\", dictionary, longest_word_length, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get best word\n",
    "def best_word(s, dictionary, silent=False):\n",
    "    try:\n",
    "        return get_suggestions(s, dictionary, longest_word_length, silent)[0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Type in word to correct below, to test and get most suggested word.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible corrections: 349\n",
      "  edit distance for deletions: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('hello', (1, 0))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_word(\"hello\", dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_document(fname, dictionary):\n",
    "    with open(fname) as file:\n",
    "        doc_word_count = 0\n",
    "        corrected_word_count = 0\n",
    "        unknown_word_count = 0\n",
    "        print \"Finding misspelled words in your document...\" \n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            doc_words = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for doc_word in doc_words:\n",
    "                doc_word_count += 1\n",
    "                suggestion = best_word(doc_word, dictionary, silent=True)\n",
    "                if suggestion is None:\n",
    "                    print \"In line %i, the word < %s > was not found (no suggested correction)\" % (i, doc_word)\n",
    "                    unknown_word_count += 1\n",
    "                elif suggestion[0]!=doc_word:\n",
    "                    print \"In line %i, %s: suggested correction is < %s >\" % (i, doc_word, suggestion[0])\n",
    "                    corrected_word_count += 1\n",
    "        \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % doc_word_count\n",
    "    print \"total unknown words: %i\" % unknown_word_count\n",
    "    print \"total potential errors found: %i\" % corrected_word_count\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Provide text file to correct, and give all best word suggestions (word level only) for errors.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "-----\n",
      "total words checked: 27\n",
      "total unknown words: 0\n",
      "total potential errors found: 0\n"
     ]
    }
   ],
   "source": [
    "correct_document(\"testdata/test.txt\", dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "In line 3, taiths: suggested correction is < taits >\n",
      "In line 11, the word < oonipiittee > was not found (no suggested correction)\n",
      "In line 13, tj: suggested correction is < to >\n",
      "In line 13, mnnff: suggested correction is < snuff >\n",
      "In line 13, gjpt: suggested correction is < gpt >\n",
      "In line 15, unuer: suggested correction is < under >\n",
      "In line 20, mthiitt: suggested correction is < miitt >\n",
      "In line 22, pythian: suggested correction is < ythian >\n",
      "In line 28, debbs: suggested correction is < debts >\n",
      "In line 29, nericans: suggested correction is < ericans >\n",
      "In line 33, unorthodox: suggested correction is < orthodox >\n",
      "In line 33, nenance: suggested correction is < penance >\n",
      "In line 38, williaij: suggested correction is < william >\n",
      "In line 40, fcsf: suggested correction is < fcs >\n",
      "In line 42, unorthodoxy: suggested correction is < orthodox >\n",
      "In line 42, thpt: suggested correction is < that >\n",
      "In line 42, the word < senbrnrgs > was not found (no suggested correction)\n",
      "In line 44, fascism: suggested correction is < fascia >\n",
      "In line 65, ththn: suggested correction is < hthn >\n",
      "In line 65, scbell: suggested correction is < cbell >\n",
      "In line 65, yktcn: suggested correction is < ktcn >\n",
      "In line 68, saij: suggested correction is < said >\n",
      "In line 69, defendants: suggested correction is < defendant >\n",
      "In line 77, korea: suggested correction is < orea >\n",
      "In line 78, doatli: suggested correction is < doati >\n",
      "In line 82, the word < ghmhvestigat > was not found (no suggested correction)\n",
      "In line 83, rosenbt: suggested correction is < rodent >\n",
      "In line 84, coriritted: suggested correction is < committed >\n",
      "In line 88, tchf: suggested correction is < chf >\n",
      "In line 91, ijb: suggested correction is < ij >\n",
      "In line 92, telegrnm: suggested correction is < telegram >\n",
      "In line 92, jillia: suggested correction is < illia >\n",
      "In line 93, ecretdry: suggested correction is < ecretry >\n",
      "In line 95, purview: suggested correction is < purves >\n",
      "In line 99, dthethg: suggested correction is < teeth >\n",
      "In line 100, sacc: suggested correction is < sac >\n",
      "In line 100, vthnz: suggested correction is < vtn >\n",
      "In line 103, fascist: suggested correction is < fascit >\n",
      "-----\n",
      "total words checked: 700\n",
      "total unknown words: 3\n",
      "total potential errors found: 35\n"
     ]
    }
   ],
   "source": [
    "# from http://www.columbia.edu/acis/cria/rosenberg/sample/\n",
    "correct_document(\"testdata/OCRsample.txt\", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Context-level correction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setup:**  \n",
    "Each sentence is modeled as a hidden Markov model. Prior probabilities (for first word in the sentence) and transition probabilities (for all subsequent words) are calculated when generating the main dictionary, using the same corpus. Emission probabilities are generated on the fly by parameterizing a Poisson distribution with the edit distance. The state space of possible corrections is based on the suggested words from the word-level correction.  \n",
    "All probabilities are stored in log-space to avoid underflow. Pre-defined minimum values are used for words that are not present in the dictionary and/or probability tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    \n",
    "    # Poisson(k, l), where k = edit distance and l=0.01\n",
    "    # TODO - validate lambda parameter (taken from Verena's code)\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.) # TODO - confirm default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modified from AM207 lecture notes\n",
    "def viterbi(words, dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                transition_prob, default_transition_prob, \\\n",
    "                num_word_suggestions=5000):\n",
    "    \n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    path_context = []\n",
    "    path_word = []\n",
    "    \n",
    "    # FOR TESTING - DELETE EVENTUALLY\n",
    "    if type(words) != list:\n",
    "        words = re.findall('[a-z]+', words.lower())  # separate by words by non-alphabetical characters\n",
    "        \n",
    "    # Character level correction\n",
    "    corrections = get_suggestions(words[0], dictionary, longest_word_length, \\\n",
    "                                  silent=True, min_count=1)\n",
    "\n",
    "    # To ensure Viterbi can keep running\n",
    "    if len(corrections) == 0:\n",
    "        corrections = [(words[0], (1, 0))]\n",
    "        path_word.append(not_found_str)\n",
    "    else:    \n",
    "        if len(corrections) > num_word_suggestions:\n",
    "            corrections = corrections[0:num_word_suggestions]\n",
    "        if len(corrections) > 0:\n",
    "            path_word.append(corrections[0][0])  # string of most frequent word tuple\n",
    "        \n",
    "    # Initialize base cases (t == 0)\n",
    "    for sug_word in corrections:\n",
    "        \n",
    "        # compute the value for all possible starting states\n",
    "        V[0][sug_word[0]] = math.exp(get_start_prob(sug_word[0], start_prob, default_start_prob) \\\n",
    "                                     + get_emission_prob(sug_word[1][1]))\n",
    "        \n",
    "        # remember all the different paths (here its only one state so far)\n",
    "        path[sug_word[0]] = [sug_word[0]]\n",
    "    \n",
    "    # normalize for numerical stability\n",
    "    path_temp_sum = sum(V[0].values())\n",
    "    V[0].update({k: math.log(v/path_temp_sum) for k, v in V[0].items()})\n",
    "    prev_corrections = [i[0] for i in corrections]\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        path_context = [max(V[0], key=lambda i: V[0][i])]\n",
    "        return path_word, path_context\n",
    "\n",
    "    # Run Viterbi for t > 0\n",
    "    for t in range(1, len(words)):\n",
    "\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        \n",
    "        # Character level correction\n",
    "        corrections = get_suggestions(words[t], dictionary, longest_word_length, \\\n",
    "                        silent=True, min_count=1)\n",
    "        \n",
    "        # To ensure Viterbi can keep running\n",
    "        if len(corrections) == 0:\n",
    "            corrections = [(words[t], (1, 0))]\n",
    "            path_word.append(not_found_str)\n",
    "        else:\n",
    "            if len(corrections) > num_word_suggestions:\n",
    "                corrections = corrections[0:num_word_suggestions]\n",
    "            if len(corrections) > 0:\n",
    "                path_word.append(corrections[0][0])  # string of most frequent word tuple\n",
    "\n",
    "        for sug_word in corrections:\n",
    "        \n",
    "            sug_word_emission_prob = get_emission_prob(sug_word[1][1])\n",
    "            \n",
    "            # compute the values coming from all possible previous states, only keep the maximum\n",
    "            (prob, word) = max((get_belief(prev_word, V[t-1])\n",
    "                                + get_transition_prob(sug_word[0], prev_word, transition_prob, default_transition_prob)\n",
    "                                + sug_word_emission_prob, prev_word) for prev_word in prev_corrections)       \n",
    "            # save the maximum value for each state\n",
    "            V[t][sug_word[0]] = math.exp(prob)\n",
    "            # remember the path we came from to get this maximum value\n",
    "            new_path[sug_word[0]] = path[word] + [sug_word[0]]\n",
    "            \n",
    "        # normalize for numerical stability\n",
    "        path_temp_sum = sum(V[t].values())\n",
    "        V[t].update({k: math.log(v/path_temp_sum) for k, v in V[t].items()})\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "        # Don't need to remember the old paths\n",
    "        path = new_path\n",
    "     \n",
    "    (prob, word) = max((V[t][sug_word[0]], sug_word[0]) for sug_word in corrections)\n",
    "    path_context = path[word]\n",
    "    \n",
    "    assert len(path_word) == len(path_context)\n",
    "\n",
    "    return path_word, path_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correct_document_context(fname, dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                             transition_prob, default_transition_prob, num_word_suggestions=5000):\n",
    "    \n",
    "    doc_word_count = 0\n",
    "    unknown_word_count = 0\n",
    "    corrected_word_count = 0\n",
    "    mismatches = 0\n",
    "    \n",
    "    with open(fname) as file:\n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            \n",
    "            for sentence in line.split('.'):\n",
    "                \n",
    "                words = re.findall('[a-z]+', sentence.lower())  # separate by words by non-alphabetical characters\n",
    "                doc_word_count += len(words)\n",
    "                \n",
    "                if len(words) > 0:\n",
    "                \n",
    "                    suggestion_w, suggestion_c = viterbi(words, dictionary, longest_word_length, \\\n",
    "                                                start_prob, default_start_prob, \\\n",
    "                                                transition_prob, default_transition_prob)\n",
    "\n",
    "                    # Display sentences where errors have been identified\n",
    "                    if (words != suggestion_w) or (words != suggestion_c):\n",
    "                        \n",
    "                        # Check for unknown words\n",
    "                        unknown_word_count += sum([w==not_found_str for w in suggestion_w])\n",
    "                        \n",
    "                        # Most users will expect to see 1-indexing.\n",
    "                        print '\\nErrors found in line %i. \\nOriginal sentence: %s' % (i+1, \" \".join(words))\n",
    "\n",
    "                        # Word-checker and context-checker output match\n",
    "                        if suggestion_w == suggestion_c:\n",
    "                            print 'Word & context-level correction: %s' % (\" \".join(suggestion_w))\n",
    "                            corrected_word_count += sum([words[j]!=suggestion_w[j] for j in range(len(words))])\n",
    "                        \n",
    "                        # Word-checker and context-checker output don't match\n",
    "                        else:\n",
    "                            print 'Word-level correction: %s' % (\" \".join(suggestion_w))\n",
    "                            print 'Context-level correction: %s' % (\" \".join(suggestion_c))\n",
    "                            corrected_word_count += \\\n",
    "                                sum([(words[j]!=suggestion_w[j]) or (words[j]!=suggestion_c[j]) for j in range(len(words))])\n",
    "                            mismatches += sum([suggestion_w[j] != suggestion_c[j] for j in range(len(words))])\n",
    "  \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % doc_word_count\n",
    "    print \"total unknown words: %i\" % unknown_word_count\n",
    "    print \"total potential errors found: %i\" % corrected_word_count\n",
    "    print \"total mismatches (word-level vs. context-level): %i\" % mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Provide string to correct, and give all best word suggestions (word level & context level).\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  this is ax test\n",
      "Word-level check:  this is ax test\n",
      "Context-level check:  this is a test\n",
      "CPU times: user 7.52 s, sys: 73 ms, total: 7.59 s\n",
      "Wall time: 7.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentence = \"this is ax test\"\n",
    "word_check, context_check = viterbi(sentence, \\\n",
    "            dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob)\n",
    "print 'Original sentence: ', sentence\n",
    "print 'Word-level check: ', \" \".join(word_check)\n",
    "print 'Context-level check: ', \" \".join(context_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Provide text file to correct, and give all best word suggestions (word level & context level).\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Errors found in line 4. \n",
      "Original sentence: this is ax test\n",
      "Word-level correction: this is ax test\n",
      "Context-level correction: this is a test\n",
      "\n",
      "Errors found in line 5. \n",
      "Original sentence: this is za test\n",
      "Word & context-level correction: this is a test\n",
      "\n",
      "Errors found in line 6. \n",
      "Original sentence: thee is a test\n",
      "Word-level correction: thee is a test\n",
      "Context-level correction: there is a test\n",
      "\n",
      "Errors found in line 7. \n",
      "Original sentence: her tee set\n",
      "Word & context-level correction: her the set\n",
      "-----\n",
      "total words checked: 27\n",
      "total unknown words: 0\n",
      "total potential errors found: 4\n",
      "total mismatches (word-level vs. context-level): 2\n",
      "CPU times: user 49.4 s, sys: 340 ms, total: 49.7 s\n",
      "Wall time: 50.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context(\"testdata/test.txt\", \\\n",
    "                         dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                         transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# correct_document_context(\"testdata/tiny.txt\", \\\n",
    "#                          dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "#                          transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# correct_document_context(\"testdata/OCRsample.txt\", \\\n",
    "#                          dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "#                          transition_prob, default_transition_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
