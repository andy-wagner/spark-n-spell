{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Based on SymSpell:\n",
    "\n",
    "Originally written in C#:\n",
    "\n",
    "// SymSpell: 1 million times faster through Symmetric Delete spelling correction algorithm\n",
    "//\n",
    "// The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup \n",
    "// for a given Damerau-Levenshtein distance. It is six orders of magnitude faster and language independent.\n",
    "// Opposite to other algorithms only deletes are required, no transposes + replaces + inserts.\n",
    "// Transposes + replaces + inserts of the input term are transformed into deletes of the dictionary term.\n",
    "// Replaces and inserts are expensive and language dependent: e.g. Chinese has 70,000 Unicode Han characters!\n",
    "//\n",
    "// Copyright (C) 2015 Wolf Garbe\n",
    "// Version: 3.0\n",
    "// Author: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// Maintainer: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// URL: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "// Description: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "//\n",
    "// License:\n",
    "// This program is free software; you can redistribute it and/or modify\n",
    "// it under the terms of the GNU Lesser General Public License, \n",
    "// version 3.0 (LGPL-3.0) as published by the Free Software Foundation.\n",
    "// http://www.opensource.org/licenses/LGPL-3.0\n",
    "//\n",
    "// Usage: single word + Enter:  Display spelling suggestions\n",
    "//        Enter without input:  Terminate the program\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math # GD: needed to calculate logs below\n",
    "from scipy.stats import poisson # GD: needed to calculate emission probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_edit_distance = 3\n",
    "not_found_str = '<not found>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deletes_list(w):\n",
    "    '''given a word, derive strings with up to max_edit_distance characters deleted'''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(max_edit_distance):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary_entry(w, dictionary, longest_word_length):\n",
    "    '''add word and its derived deletions to dictionary'''\n",
    "    # check if word is already in dictionary\n",
    "    # dictionary entries are in the form: (list of suggested corrections, frequency of word in corpus)\n",
    "\n",
    "    new_real_word_added = False\n",
    "    if w in dictionary:\n",
    "        dictionary[w] = (dictionary[w][0], dictionary[w][1] + 1)  # increment count of word in corpus\n",
    "    else:\n",
    "        dictionary[w] = ([], 1)  \n",
    "        longest_word_length = max(longest_word_length, len(w))\n",
    "        \n",
    "    if dictionary[w][1]==1:\n",
    "        # first appearance of word in corpus\n",
    "        # n.b. word may already be in dictionary as a derived word (deleting character from a real word)\n",
    "        # but counter of frequency of word in corpus is not incremented in those cases)\n",
    "        \n",
    "        new_real_word_added = True\n",
    "        deletes = get_deletes_list(w)\n",
    "        \n",
    "        for item in deletes:\n",
    "            if item in dictionary:\n",
    "                # add (correct) word to delete's suggested correction list if not already there\n",
    "                if item not in dictionary[item][0]:\n",
    "                    dictionary[item][0].append(w)\n",
    "            else:\n",
    "                dictionary[item] = ([w], 0)  # note frequency of word in corpus is not incremented\n",
    "        \n",
    "    return new_real_word_added, longest_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary(fname):\n",
    "    \n",
    "    print \"Creating dictionary...\" \n",
    "\n",
    "    dictionary = dict() # GD: moved here to ensure that dictionary is re-initialized.\n",
    "    longest_word_length = 0 # GD: moved here to ensure that it is re-initialized.\n",
    "    start_prob = dict()\n",
    "    transition_prob = dict()\n",
    "    word_count = 0\n",
    "    \n",
    "    with open(fname) as file:    \n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            for sentence in line.split('.'): # GD: added to ensure split at sentence level\n",
    "                \n",
    "                words = re.findall('[a-z]+', sentence.lower())  # separate by words by non-alphabetical characters      \n",
    "                \n",
    "                for w, word in enumerate(words):\n",
    "                    \n",
    "                    new_word, longest_word_length = \\\n",
    "                        create_dictionary_entry(word, dictionary, longest_word_length)\n",
    "                    \n",
    "                    if new_word:\n",
    "                        word_count += 1\n",
    "                        \n",
    "                    # GD: added to calculate probabilities for Hidden Markov Model\n",
    "                    if w == 0:\n",
    "\n",
    "                        # Probability of a word being at the beginning of a sentence\n",
    "                        if word in start_prob:\n",
    "                            start_prob[word] += 1\n",
    "                        else:\n",
    "                            start_prob[word] = 1\n",
    "                    else:\n",
    "                        \n",
    "                        # Probability of transitionining from one word to another\n",
    "                        # dictionary format: {previous word: {word1 : P(word1|prevous word), word2 : P(word2|prevous word)}}\n",
    "                        # Check that prior word is present - create if not\n",
    "                        if words[w - 1] not in transition_prob:\n",
    "                            transition_prob[words[w - 1]] = dict()\n",
    "                            \n",
    "                        # Check that current word is present - create if not\n",
    "                        if word not in transition_prob[words[w - 1]]:\n",
    "                            transition_prob[words[w - 1]][word] = 0\n",
    "                            \n",
    "                        # Update value\n",
    "                        transition_prob[words[w - 1]][word] += 1\n",
    "                              \n",
    "    # GD: added to convert counts to log-probabilities (to avoid underflow)\n",
    "    # Note: natural logarithm, not base-10\n",
    "    \n",
    "    total_start_words = float(sum(start_prob.values()))\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    start_prob.update({k: math.log(v/total_start_words) for k, v in start_prob.items()})\n",
    "    \n",
    "    default_transition_prob = math.log(1./word_count)    \n",
    "    transition_prob.update({k: {k1: math.log(float(v1)/sum(v.values())) for k1, v1 in v.items()} \\\n",
    "                            for k, v in transition_prob.items()})\n",
    "\n",
    "    print \"Total unique words in corpus: %i\" % word_count\n",
    "    print \"Total items in dictionary (corpus words and deletions): %i\" % len(dictionary)\n",
    "    print \"  Edit distance for deletions: %i\" % max_edit_distance\n",
    "    print \"  Length of longest word in corpus: %i\" % longest_word_length\n",
    "    print \"Total unique words appearing at the start of a sentence: %i\" % len(start_prob)\n",
    "    print \"Total unique word transitions: %i\" % len(transition_prob)\n",
    "        \n",
    "    return dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "            transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary (corpus words and deletions): 2151998\n",
      "  Edit distance for deletions: 3\n",
      "  Length of longest word in corpus: 18\n",
      "Total unique words appearing at the start of a sentence: 15297\n",
      "Total unique word transitions: 27224\n",
      "CPU times: user 33.7 s, sys: 815 ms, total: 34.5 s\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "    create_dictionary(\"testdata/big.txt\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Can look up a specific entry in the dictionary below. <br>\n",
    "shows (possible corrections, and frequency that entry itself is in corpus - 0 if not a real word) <br>\n",
    "Note: will return key error if there are no corrections (because we are accessing dictionary directly here)\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['essentially', 'essentials'], 92)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print dictionary[\"essential\"]\n",
    "except KeyError:\n",
    "    print 'Not in dictionary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['wrack'], 0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print dictionary[\"wack\"]\n",
    "except KeyError:\n",
    "    print 'Not in dictionary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Word-level correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    \n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    for x in xrange(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suggestions(string, dictionary, longest_word_length, silent=False, min_count=0):\n",
    "    '''return list of suggested corrections for potentially incorrectly spelled word'''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > max_edit_distance:\n",
    "        if not silent:\n",
    "            print \"no items in dictionary within maximum edit distance\"\n",
    "        return []\n",
    "    \n",
    "    # suggestions = []\n",
    "    # s_dictionary = {}\n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        # print \"processing '%s'\" % q_item\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>=min_count):# GD added to trim list for context checking\n",
    "            # word is in dictionary, and is a word from the corpus, and not already in suggestion list\n",
    "            # so add to suggestion dictionary, indexed by the word with value (frequency in corpus, edit distance)\n",
    "            # note q_items that are not the input string are shorter than input string \n",
    "            # since only deletes are added (unless manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = (dictionary[q_item][1], len(string) - len(q_item))\n",
    "\n",
    "            ## the suggested corrections for q_item as stored in dictionary (whether or not\n",
    "            ## q_item itself is a valid word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    # compute edit distance\n",
    "                    # if len(sc_item)==len(q_item):\n",
    "                    #    item_dist = len(string) - len(q_item)\n",
    "                    # suggested items should always be longer (unless manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "                    #elif len(q_item)==len(string):\n",
    "                        # a suggestion could be longer or shorter than original string (bug in original FAROO?)\n",
    "                        # if suggestion is from string's suggestion list, sc_item will be longer\n",
    "                        # if suggestion is from a delete's suggestion list, sc_item may be shorter\n",
    "                    #   item_dist = abs(len(sc_item) - len(q_item))\n",
    "                    #else:\n",
    "                    # check in original code, but probably not necessary because string has already checked\n",
    "                    assert sc_item!=string\n",
    "\n",
    "                    # calculate edit distance using, for example, Damerau-Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "\n",
    "                    if item_dist<=max_edit_distance:\n",
    "                        assert sc_item in dictionary  # should already be in dictionary if in suggestion list\n",
    "                        if (dictionary[q_item][1]>=min_count):# GD added to trim list for context checking\n",
    "                            suggest_dict[sc_item] = (dictionary[sc_item][1], item_dist)\n",
    "\n",
    "        # now generate deletes (e.g. a substring of string or of a delete) from the queue item\n",
    "        # as additional items to check -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<max_edit_distance and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
    "             \n",
    "    # queue is now empty: convert suggestions in dictionary to list for output\n",
    "    \n",
    "    if not silent:\n",
    "        print \"number of possible corrections: %i\" %len(suggest_dict)\n",
    "        print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    \n",
    "    # output option 1\n",
    "    # sort results by ascending order of edit distance and descending order of frequency\n",
    "    #     and return list of suggested corrections only:\n",
    "    # return sorted(suggest_dict, key = lambda x: (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "    # output option 2\n",
    "    # return list of suggestions with (correction, (frequency in corpus, edit distance)):\n",
    "    as_list = suggest_dict.items()\n",
    "    return sorted(as_list, key = lambda (term, (freq, dist)): (dist, -freq))\n",
    "\n",
    "    '''\n",
    "    Option 1:\n",
    "    get_suggestions(\"file\")\n",
    "    ['file', 'five', 'fire', 'fine', ...]\n",
    "    \n",
    "    Option 2:\n",
    "    get_suggestions(\"file\")\n",
    "    [('file', (5, 0)),\n",
    "     ('five', (67, 1)),\n",
    "     ('fire', (54, 1)),\n",
    "     ('fine', (17, 1))...]  \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Type in word to correct below, to test and get whole list of possible suggestions.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible corrections: 142\n",
      "  edit distance for deletions: 3\n",
      "CPU times: user 11.4 ms, sys: 966 µs, total: 12.3 ms\n",
      "Wall time: 11.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mitten', (0, 1)),\n",
       " ('mittes', (0, 1)),\n",
       " ('ittens', (0, 1)),\n",
       " ('matters', (136, 2)),\n",
       " ('bitten', (13, 2)),\n",
       " ('kitten', (7, 2)),\n",
       " ('listens', (2, 2)),\n",
       " ('battens', (1, 2)),\n",
       " ('smitten', (1, 2)),\n",
       " ('itten', (0, 2)),\n",
       " ('mites', (0, 2)),\n",
       " ('miten', (0, 2)),\n",
       " ('ittes', (0, 2)),\n",
       " ('mtten', (0, 2)),\n",
       " ('mttes', (0, 2)),\n",
       " ('ttens', (0, 2)),\n",
       " ('itens', (0, 2)),\n",
       " ('miens', (0, 2)),\n",
       " ('mittn', (0, 2)),\n",
       " ('mitte', (0, 2)),\n",
       " ('ittns', (0, 2)),\n",
       " ('mitts', (0, 2)),\n",
       " ('matter', (365, 3)),\n",
       " ('sitting', (269, 3)),\n",
       " ('minutes', (146, 3)),\n",
       " ('written', (117, 3)),\n",
       " ('miles', (110, 3)),\n",
       " ('citizens', (109, 3)),\n",
       " ('letters', (108, 3)),\n",
       " ('listen', (100, 3)),\n",
       " ('cities', (77, 3)),\n",
       " ('bitter', (47, 3)),\n",
       " ('masters', (37, 3)),\n",
       " ('intense', (34, 3)),\n",
       " ('witness', (33, 3)),\n",
       " ('attend', (29, 3)),\n",
       " ('mistress', (24, 3)),\n",
       " ('fitted', (23, 3)),\n",
       " ('mines', (22, 3)),\n",
       " ('fitting', (21, 3)),\n",
       " ('miners', (19, 3)),\n",
       " ('mitenka', (16, 3)),\n",
       " ('tens', (16, 3)),\n",
       " ('sisters', (16, 3)),\n",
       " ('intent', (13, 3)),\n",
       " ('mothers', (12, 3)),\n",
       " ('mutton', (11, 3)),\n",
       " ('bites', (10, 3)),\n",
       " ('sites', (9, 3)),\n",
       " ('omitted', (9, 3)),\n",
       " ('buttons', (8, 3)),\n",
       " ('fitness', (8, 3)),\n",
       " ('rotten', (8, 3)),\n",
       " ('attends', (8, 3)),\n",
       " ('matrena', (8, 3)),\n",
       " ('intend', (8, 3)),\n",
       " ('titles', (7, 3)),\n",
       " ('pitted', (6, 3)),\n",
       " ('matted', (6, 3)),\n",
       " ('omitting', (5, 3)),\n",
       " ('witted', (5, 3)),\n",
       " ('kitchens', (5, 3)),\n",
       " ('hitting', (5, 3)),\n",
       " ('intends', (5, 3)),\n",
       " ('emitted', (4, 3)),\n",
       " ('items', (4, 3)),\n",
       " ('mutter', (4, 3)),\n",
       " ('pitting', (4, 3)),\n",
       " ('mates', (4, 3)),\n",
       " ('emitting', (3, 3)),\n",
       " ('litter', (3, 3)),\n",
       " ('dites', (3, 3)),\n",
       " ('mister', (2, 3)),\n",
       " ('distend', (2, 3)),\n",
       " ('withers', (2, 3)),\n",
       " ('mints', (2, 3)),\n",
       " ('motions', (2, 3)),\n",
       " ('cutters', (2, 3)),\n",
       " ('mien', (2, 3)),\n",
       " ('linens', (2, 3)),\n",
       " ('hastens', (2, 3)),\n",
       " ('softens', (2, 3)),\n",
       " ('gotten', (2, 3)),\n",
       " ('misses', (2, 3)),\n",
       " ('dickens', (2, 3)),\n",
       " ('viens', (2, 3)),\n",
       " ('winters', (2, 3)),\n",
       " ('matins', (2, 3)),\n",
       " ('potters', (2, 3)),\n",
       " ('amiens', (1, 3)),\n",
       " ('metes', (1, 3)),\n",
       " ('filters', (1, 3)),\n",
       " ('mattress', (1, 3)),\n",
       " ('pickens', (1, 3)),\n",
       " ('distends', (1, 3)),\n",
       " ('kittenish', (1, 3)),\n",
       " ('gutters', (1, 3)),\n",
       " ('rioters', (1, 3)),\n",
       " ('hatters', (1, 3)),\n",
       " ('sittings', (1, 3)),\n",
       " ('fetters', (1, 3)),\n",
       " ('fatten', (1, 3)),\n",
       " ('remittent', (1, 3)),\n",
       " ('imitates', (1, 3)),\n",
       " ('pities', (1, 3)),\n",
       " ('utters', (1, 3)),\n",
       " ('athens', (1, 3)),\n",
       " ('tiens', (1, 3)),\n",
       " ('maidens', (1, 3)),\n",
       " ('moyens', (1, 3)),\n",
       " ('matting', (1, 3)),\n",
       " ('milton', (1, 3)),\n",
       " ('mists', (1, 3)),\n",
       " ('hittel', (1, 3)),\n",
       " ('cottons', (1, 3)),\n",
       " ('intents', (1, 3)),\n",
       " ('midges', (1, 3)),\n",
       " ('wotten', (1, 3)),\n",
       " ('patterns', (1, 3)),\n",
       " ('mtns', (0, 3)),\n",
       " ('ites', (0, 3)),\n",
       " ('iten', (0, 3)),\n",
       " ('mten', (0, 3)),\n",
       " ('mies', (0, 3)),\n",
       " ('itns', (0, 3)),\n",
       " ('mitt', (0, 3)),\n",
       " ('mits', (0, 3)),\n",
       " ('mitn', (0, 3)),\n",
       " ('ittn', (0, 3)),\n",
       " ('itte', (0, 3)),\n",
       " ('mtes', (0, 3)),\n",
       " ('mite', (0, 3)),\n",
       " ('mins', (0, 3)),\n",
       " ('mens', (0, 3)),\n",
       " ('ttns', (0, 3)),\n",
       " ('ttes', (0, 3)),\n",
       " ('mtte', (0, 3)),\n",
       " ('itts', (0, 3)),\n",
       " ('iens', (0, 3)),\n",
       " ('mttn', (0, 3)),\n",
       " ('mtts', (0, 3)),\n",
       " ('tten', (0, 3))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_suggestions(\"mittens\", dictionary, longest_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 5.75 ms, total: 1.83 s\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"acamodation\", dictionary, longest_word_length, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.07 s, sys: 3.79 ms, total: 2.08 s\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"acomodation\", dictionary, longest_word_length, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.4 s, sys: 213 ms, total: 39.6 s\n",
      "Wall time: 39.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"hous\", dictionary, longest_word_length, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get best word\n",
    "def best_word(s, dictionary, silent=False):\n",
    "    try:\n",
    "        return get_suggestions(s, dictionary, longest_word_length, silent)[0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Type in word to correct below, to test and get most suggested word.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible corrections: 349\n",
      "  edit distance for deletions: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('hello', (1, 0))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_word(\"hello\", dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_document(fname, dictionary):\n",
    "    with open(fname) as file:\n",
    "        doc_word_count = 0\n",
    "        corrected_word_count = 0\n",
    "        unknown_word_count = 0\n",
    "        print \"Finding misspelled words in your document...\" \n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            doc_words = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for doc_word in doc_words:\n",
    "                doc_word_count += 1\n",
    "                suggestion = best_word(doc_word, dictionary, silent=True)\n",
    "                if suggestion is None:\n",
    "                    print \"In line %i, the word < %s > was not found (no suggested correction)\" % (i, doc_word)\n",
    "                    unknown_word_count += 1\n",
    "                elif suggestion[0]!=doc_word:\n",
    "                    print \"In line %i, %s: suggested correction is < %s >\" % (i, doc_word, suggestion[0])\n",
    "                    corrected_word_count += 1\n",
    "        \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % doc_word_count\n",
    "    print \"total unknown words: %i\" % unknown_word_count\n",
    "    print \"total potential errors found: %i\" % corrected_word_count\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Provide text file to correct, and give all best word suggestions (word level only) for errors.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "-----\n",
      "total words checked: 27\n",
      "total unknown words: 0\n",
      "total potential errors found: 0\n"
     ]
    }
   ],
   "source": [
    "correct_document(\"testdata/test.txt\", dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "In line 3, taiths: suggested correction is < taits >\n",
      "In line 11, the word < oonipiittee > was not found (no suggested correction)\n",
      "In line 13, tj: suggested correction is < to >\n",
      "In line 13, mnnff: suggested correction is < snuff >\n",
      "In line 13, gjpt: suggested correction is < gpt >\n",
      "In line 15, unuer: suggested correction is < under >\n",
      "In line 20, mthiitt: suggested correction is < miitt >\n",
      "In line 22, pythian: suggested correction is < ythian >\n",
      "In line 28, debbs: suggested correction is < debts >\n",
      "In line 29, nericans: suggested correction is < ericans >\n",
      "In line 33, unorthodox: suggested correction is < orthodox >\n",
      "In line 33, nenance: suggested correction is < penance >\n",
      "In line 38, williaij: suggested correction is < william >\n",
      "In line 40, fcsf: suggested correction is < fcs >\n",
      "In line 42, unorthodoxy: suggested correction is < orthodox >\n",
      "In line 42, thpt: suggested correction is < that >\n",
      "In line 42, the word < senbrnrgs > was not found (no suggested correction)\n",
      "In line 44, fascism: suggested correction is < fascia >\n",
      "In line 65, ththn: suggested correction is < hthn >\n",
      "In line 65, scbell: suggested correction is < cbell >\n",
      "In line 65, yktcn: suggested correction is < ktcn >\n",
      "In line 68, saij: suggested correction is < said >\n",
      "In line 69, defendants: suggested correction is < defendant >\n",
      "In line 77, korea: suggested correction is < orea >\n",
      "In line 78, doatli: suggested correction is < doati >\n",
      "In line 82, the word < ghmhvestigat > was not found (no suggested correction)\n",
      "In line 83, rosenbt: suggested correction is < rodent >\n",
      "In line 84, coriritted: suggested correction is < committed >\n",
      "In line 88, tchf: suggested correction is < chf >\n",
      "In line 91, ijb: suggested correction is < ij >\n",
      "In line 92, telegrnm: suggested correction is < telegram >\n",
      "In line 92, jillia: suggested correction is < illia >\n",
      "In line 93, ecretdry: suggested correction is < ecretry >\n",
      "In line 95, purview: suggested correction is < purves >\n",
      "In line 99, dthethg: suggested correction is < teeth >\n",
      "In line 100, sacc: suggested correction is < sac >\n",
      "In line 100, vthnz: suggested correction is < vtn >\n",
      "In line 103, fascist: suggested correction is < fascit >\n",
      "-----\n",
      "total words checked: 700\n",
      "total unknown words: 3\n",
      "total potential errors found: 35\n"
     ]
    }
   ],
   "source": [
    "# from http://www.columbia.edu/acis/cria/rosenberg/sample/\n",
    "correct_document(\"testdata/OCRsample.txt\", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Context-level correction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model setup:**  \n",
    "Each sentence is modeled as a hidden Markov model. Prior probabilities (for first word in the sentence) and transition probabilities (for all subsequent words) are calculated when generating the main dictionary, using the same corpus. Emission probabilities are generated on the fly by parameterizing a Poisson distribution with the edit distance. The state space of possible corrections is based on the suggested words from the word-level correction.  \n",
    "All probabilities are stored in log-space to avoid underflow. Pre-defined minimum values are used for words that are not present in the dictionary and/or probability tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    \n",
    "    # Poisson(k, l), where k = edit distance and l=0.01\n",
    "    # TODO - validate lambda parameter (taken from Verena's code)\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    try:\n",
    "        return start_prob[word]\n",
    "    except KeyError:\n",
    "        return default_start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    try:\n",
    "        return transition_prob[prev_word][cur_word]\n",
    "    except KeyError:\n",
    "        return default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_belief(prev_word, prev_belief):\n",
    "    try:\n",
    "        return prev_belief[prev_word]\n",
    "    except KeyError:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.) # TODO - confirm default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modified from AM207 lecture notes\n",
    "def viterbi(words, dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                transition_prob, default_transition_prob, \\\n",
    "                num_word_suggestions=5000):\n",
    "    \n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    path_context = []\n",
    "    path_word = []\n",
    "    \n",
    "    # FOR TESTING - DELETE EVENTUALLY\n",
    "    if type(words) != list:\n",
    "        words = re.findall('[a-z]+', words.lower())  # separate by words by non-alphabetical characters\n",
    "        \n",
    "    # Character level correction\n",
    "    corrections = get_suggestions(words[0], dictionary, longest_word_length, \\\n",
    "                                  silent=True, min_count=1)\n",
    "\n",
    "    # To ensure Viterbi can keep running\n",
    "    if len(corrections) == 0:\n",
    "        corrections = [(words[0], (1, 0))]\n",
    "        path_word.append(not_found_str)\n",
    "    else:    \n",
    "        if len(corrections) > num_word_suggestions:\n",
    "            corrections = corrections[0:num_word_suggestions]\n",
    "        if len(corrections) > 0:\n",
    "            path_word.append(corrections[0][0])  # string of most frequent word tuple\n",
    "        \n",
    "    # Initialize base cases (t == 0)\n",
    "    for sug_word in corrections:\n",
    "        \n",
    "        # compute the value for all possible starting states\n",
    "        V[0][sug_word[0]] = math.exp(get_start_prob(sug_word[0], start_prob, default_start_prob) \\\n",
    "                                     + get_emission_prob(sug_word[1][1]))\n",
    "        \n",
    "        # remember all the different paths (here its only one state so far)\n",
    "        path[sug_word[0]] = [sug_word[0]]\n",
    " \n",
    "    # normalize for numerical stability\n",
    "    path_temp_sum = sum(V[0].values())\n",
    "    V[0].update({k: math.log(v/path_temp_sum) for k, v in V[0].items()})\n",
    "    prev_corrections = [i[0] for i in corrections]\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        path_context = [max(V[0], key=lambda i: V[0][i])]\n",
    "        return path_word, path_context\n",
    "\n",
    "    # Run Viterbi for t > 0\n",
    "    for t in range(1, len(words)):\n",
    "\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        \n",
    "        # Character level correction\n",
    "        corrections = get_suggestions(words[t], dictionary, longest_word_length, \\\n",
    "                        silent=True, min_count=1)\n",
    "        \n",
    "        # To ensure Viterbi can keep running\n",
    "        if len(corrections) == 0:\n",
    "            corrections = [(words[t], (1, 0))]\n",
    "            path_word.append(not_found_str)\n",
    "        else:\n",
    "            if len(corrections) > num_word_suggestions:\n",
    "                corrections = corrections[0:num_word_suggestions]\n",
    "            if len(corrections) > 0:\n",
    "                path_word.append(corrections[0][0])  # string of most frequent word tuple\n",
    " \n",
    "        for sug_word in corrections:\n",
    "        \n",
    "            sug_word_emission_prob = get_emission_prob(sug_word[1][1])\n",
    "            \n",
    "            # compute the values coming from all possible previous states, only keep the maximum\n",
    "            (prob, word) = max((get_belief(prev_word, V[t-1]) \\\n",
    "                            + get_transition_prob(sug_word[0], prev_word, transition_prob, default_transition_prob) \\\n",
    "                            + sug_word_emission_prob, prev_word) for prev_word in prev_corrections)\n",
    "\n",
    "            # save the maximum value for each state\n",
    "            V[t][sug_word[0]] = math.exp(prob)\n",
    "            # remember the path we came from to get this maximum value\n",
    "            new_path[sug_word[0]] = path[word] + [sug_word[0]]\n",
    "            \n",
    "        # normalize for numerical stability\n",
    "        path_temp_sum = sum(V[t].values())\n",
    "        V[t].update({k: math.log(v/path_temp_sum) for k, v in V[t].items()})\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "        # Don't need to remember the old paths\n",
    "        path = new_path\n",
    "     \n",
    "    (prob, word) = max((V[t][sug_word[0]], sug_word[0]) for sug_word in corrections)\n",
    "    path_context = path[word]\n",
    "    \n",
    "    assert len(path_word) == len(path_context)\n",
    "\n",
    "    return path_word, path_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correct_document_context(fname, dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                             transition_prob, default_transition_prob, num_word_suggestions=5000):\n",
    "    \n",
    "    doc_word_count = 0\n",
    "    unknown_word_count = 0\n",
    "    corrected_word_count = 0\n",
    "    mismatches = 0\n",
    "    \n",
    "    with open(fname) as file:\n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            \n",
    "            for sentence in line.split('.'):\n",
    "                \n",
    "                words = re.findall('[a-z]+', sentence.lower())  # separate by words by non-alphabetical characters\n",
    "                doc_word_count += len(words)\n",
    "                \n",
    "                if len(words) > 0:\n",
    "                \n",
    "                    suggestion_w, suggestion_c = viterbi(words, dictionary, longest_word_length, \\\n",
    "                                                start_prob, default_start_prob, \\\n",
    "                                                transition_prob, default_transition_prob)\n",
    "\n",
    "                    # Display sentences where errors have been identified\n",
    "                    if (words != suggestion_w) or (words != suggestion_c):\n",
    "                        \n",
    "                        # Check for unknown words\n",
    "                        unknown_word_count += sum([w==not_found_str for w in suggestion_w])\n",
    "                        \n",
    "                        # Most users will expect to see 1-indexing.\n",
    "                        print '\\nErrors found in line %i. \\nOriginal sentence: %s' % (i+1, \" \".join(words))\n",
    "\n",
    "                        # Word-checker and context-checker output match\n",
    "                        if suggestion_w == suggestion_c:\n",
    "                            print 'Word & context-level correction: %s' % (\" \".join(suggestion_w))\n",
    "                            corrected_word_count += sum([words[j]!=suggestion_w[j] for j in range(len(words))])\n",
    "                        \n",
    "                        # Word-checker and context-checker output don't match\n",
    "                        else:\n",
    "                            print 'Word-level correction: %s' % (\" \".join(suggestion_w))\n",
    "                            print 'Context-level correction: %s' % (\" \".join(suggestion_c))\n",
    "                            corrected_word_count += \\\n",
    "                                sum([(words[j]!=suggestion_w[j]) or (words[j]!=suggestion_c[j]) for j in range(len(words))])\n",
    "                            mismatches += sum([suggestion_w[j] != suggestion_c[j] for j in range(len(words))])\n",
    "  \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % doc_word_count\n",
    "    print \"total unknown words: %i\" % unknown_word_count\n",
    "    print \"total potential errors found: %i\" % corrected_word_count\n",
    "    print \"total mismatches (word-level vs. context-level): %i\" % mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Provide string to correct, and give all best word suggestions (word level & context level).\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = \"ther sa pile of clothsing on the side of thee train treks\"\n",
    "# sentence = \"is a very\"\n",
    "# sentence = \"is a test\"\n",
    "# sentence = \"a test this\"\n",
    "# sentence = \"a test tube\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi algorithm:\n",
      "Original sentence:  ther sa pile of clothsing on the side of thee train treks\n",
      "Word-level check:  the sa pile of clothing on the side of thee train trees\n",
      "Context-level check:  then a pile of clothing of the side of the train tres\n",
      "CPU times: user 19.4 s, sys: 181 ms, total: 19.6 s\n",
      "Wall time: 20 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print 'Viterbi algorithm:'\n",
    "word_check, context_check = viterbi(sentence, \\\n",
    "            dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob)\n",
    "print 'Original sentence: ', sentence\n",
    "print 'Word-level check: ', \" \".join(word_check)\n",
    "print 'Context-level check: ', \" \".join(context_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<p><b>For testing:</b></p>\n",
    "<p>\n",
    "Provide text file to correct, and give all best word suggestions (word level & context level).\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Errors found in line 4. \n",
      "Original sentence: this is ax test\n",
      "Word-level correction: this is ax test\n",
      "Context-level correction: this is a test\n",
      "\n",
      "Errors found in line 5. \n",
      "Original sentence: this is za test\n",
      "Word & context-level correction: this is a test\n",
      "\n",
      "Errors found in line 6. \n",
      "Original sentence: thee is a test\n",
      "Word-level correction: thee is a test\n",
      "Context-level correction: there is a test\n",
      "\n",
      "Errors found in line 7. \n",
      "Original sentence: her tee set\n",
      "Word & context-level correction: her the set\n",
      "-----\n",
      "total words checked: 27\n",
      "total unknown words: 0\n",
      "total potential errors found: 4\n",
      "total mismatches (word-level vs. context-level): 2\n",
      "CPU times: user 1min 7s, sys: 2 s, total: 1min 9s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context(\"testdata/test.txt\", \\\n",
    "                         dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                         transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Errors found in line 1. \n",
      "Original sentence: ny\n",
      "Word-level correction: ny\n",
      "Context-level correction: no\n",
      "\n",
      "Errors found in line 4. \n",
      "Original sentence: di taiths\n",
      "Word-level correction: di tastes\n",
      "Context-level correction: i waits\n",
      "\n",
      "Errors found in line 7. \n",
      "Original sentence: cord n\n",
      "Word-level correction: cord n\n",
      "Context-level correction: cord in\n",
      "\n",
      "Errors found in line 12. \n",
      "Original sentence: en raised by the oonipiittee to pay the expenses and\n",
      "Word-level correction: en raised by the <not found> to pay the expenses and\n",
      "Context-level correction: in raised by the oonipiittee to pay the expense and\n",
      "\n",
      "Errors found in line 13. \n",
      "Original sentence: charges for preparing and filing the printed record and\n",
      "Word-level correction: charges for preparing and filing the printed record and\n",
      "Context-level correction: charge for preparing and filling the printed records and\n",
      "\n",
      "Errors found in line 14. \n",
      "Original sentence: brief for julius and t tj f d th t\n",
      "Word-level correction: brief for julius and t to f d th t\n",
      "Context-level correction: grief for julius and it to f d th to\n",
      "\n",
      "Errors found in line 14. \n",
      "Original sentence: n mnnff t gjpt n p t t\n",
      "Word-level correction: n snuff t get n p t t\n",
      "Context-level correction: in snuff t get on p t it\n",
      "\n",
      "Errors found in line 16. \n",
      "Original sentence: meetings have been held bh c snc uth c country unuer\n",
      "Word-level correction: meetings have been held by c sac th c country under\n",
      "Context-level correction: meetings have been held by c in the c country under\n",
      "\n",
      "Errors found in line 18. \n",
      "Original sentence: raise money for the defense of the os i r g and to rouse\n",
      "Word-level correction: raise money for the defense of the os i r g and to rouse\n",
      "Context-level correction: raise money for the defense of the so i or go and to rouse\n",
      "\n",
      "Errors found in line 21. \n",
      "Original sentence: on march l the national c mthiitt to secure\n",
      "Word-level correction: on march l the national c <not found> to secure\n",
      "Context-level correction: on march all the national c mthiitt to secure\n",
      "\n",
      "Errors found in line 22. \n",
      "Original sentence: justice in the r s j b f cas held a meeting at the\n",
      "Word-level correction: justice in the r s j b f was held a meeting at the\n",
      "Context-level correction: justice in the r s j mb f as held a meeting at the\n",
      "\n",
      "Errors found in line 23. \n",
      "Original sentence: pythian hall l west th street new york city which\n",
      "Word-level correction: than hall l west th street new york city which\n",
      "Context-level correction: than half l west the street new york city which\n",
      "\n",
      "Errors found in line 24. \n",
      "Original sentence: was attended by approximately oe to people\n",
      "Word-level correction: was attended by approximately oe to people\n",
      "Context-level correction: was attended by approximately one to people\n",
      "\n",
      "Errors found in line 26. \n",
      "Original sentence: confidential informant t l of unknown reliability\n",
      "Word-level correction: confidential informant t l of unknown reliability\n",
      "Context-level correction: confidential informant tu l or unknown reliability\n",
      "\n",
      "Errors found in line 27. \n",
      "Original sentence: attended the above meeting and stated that joseph brainin\n",
      "Word-level correction: attended the above meeting and stated that joseph brain\n",
      "Context-level correction: attended the above meeting and stated that joseph raining\n",
      "\n",
      "Errors found in line 28. \n",
      "Original sentence: was chairman and opened the meeting with greetings jfl the\n",
      "Word & context-level correction: was chairman and opened the meeting with greetings of the\n",
      "\n",
      "Errors found in line 29. \n",
      "Original sentence: names of ji stice blaci justice douglas eug n debbs and\n",
      "Word-level correction: names of i stick black justice douglas dug n debts and\n",
      "Context-level correction: names of jim silk black justice douglas tug in debts and\n",
      "\n",
      "Errors found in line 30. \n",
      "Original sentence: other great a nericans to whom liberty and justice i not a\n",
      "Word & context-level correction: other great a americans to whom liberty and justice i not a\n",
      "\n",
      "Errors found in line 31. \n",
      "Original sentence: b ainin commented that the r se ergs\n",
      "Word-level correction: b again commented that the r se eggs\n",
      "Context-level correction: b in commented that the r s eyes\n",
      "\n",
      "Errors found in line 32. \n",
      "Original sentence: were convicted on trumped up evidence and that the main\n",
      "Word-level correction: were convicted on trumpet up evidence and that the main\n",
      "Context-level correction: were convicted of trumpet up evidence and that the main\n",
      "\n",
      "Errors found in line 33. \n",
      "Original sentence: aim of their conviction was to warn the erican people\n",
      "Word-level correction: aim of their conviction was to warn the brian people\n",
      "Context-level correction: aim of their conviction was to warn the german people\n",
      "\n",
      "Errors found in line 34. \n",
      "Original sentence: that all holders of unorthodox views are a nenance to thg\n",
      "Word-level correction: that all holders of orthodox views are a <not found> to the\n",
      "Context-level correction: that all holders of orthodox view are a nenance to the\n",
      "\n",
      "Errors found in line 35. \n",
      "Original sentence: he claimed that the conviction of the r sln rgs\n",
      "Word-level correction: he claimed that the conviction of the r son rags\n",
      "Context-level correction: he claimed that the conviction of the r s rugs\n",
      "\n",
      "Errors found in line 36. \n",
      "Original sentence: and their sentence too death is an eternal shame on american\n",
      "Word-level correction: and their sentence too death is an eternal shame on american\n",
      "Context-level correction: and their sentence to death is an eternal shame on american\n",
      "\n",
      "Errors found in line 39. \n",
      "Original sentence: williaij eu i n writer for the national\n",
      "Word-level correction: <not found> e i n writer for the national\n",
      "Context-level correction: williaij qu il n writer or the national\n",
      "\n",
      "Errors found in line 41. \n",
      "Original sentence: next spoke and said that the fcsf ber and s b ll were\n",
      "Word-level correction: next spoke and said that the ff be and s b ll were\n",
      "Context-level correction: next spoke and said that the use her and so by all were\n",
      "\n",
      "Errors found in line 42. \n",
      "Original sentence: convicted not because of espionage but f r political\n",
      "Word-level correction: convicted not because of espionage but f r political\n",
      "Context-level correction: convicted not because of espionage act of or political\n",
      "\n",
      "Errors found in line 43. \n",
      "Original sentence: unorthodoxy\n",
      "Word & context-level correction: orthodox\n",
      "\n",
      "Errors found in line 43. \n",
      "Original sentence: he claimed thpt the r senbrnrgs wore victims\n",
      "Word-level correction: he claimed that the r <not found> wore victims\n",
      "Context-level correction: he claimed that the r senbrnrgs wore victims\n",
      "\n",
      "Errors found in line 44. \n",
      "Original sentence: of the cold war of the forces which are trying to plunge\n",
      "Word-level correction: of the cold war of the forces which are trying to plunge\n",
      "Context-level correction: of the old war of the forces which are trying to plunge\n",
      "\n",
      "Errors found in line 45. \n",
      "Original sentence: humanity into chaos and fascism\n",
      "Word-level correction: humanity into chaos and <not found>\n",
      "Context-level correction: humanity into chaos and fascism\n",
      "\n",
      "Errors found in line 63. \n",
      "Original sentence: ny loo\n",
      "Word-level correction: ny look\n",
      "Context-level correction: ni loi\n",
      "\n",
      "Errors found in line 66. \n",
      "Original sentence: h ththn scbell the i ife of i yktcn sob thl next\n",
      "Word-level correction: h then bell the i if of i <not found> sob the next\n",
      "Context-level correction: he then sell the i rife of it yktcn sow the next\n",
      "\n",
      "Errors found in line 67. \n",
      "Original sentence: addressed thi ing nd stated th t her husband and the\n",
      "Word-level correction: addressed the ing nd stated th t her husband and the\n",
      "Context-level correction: addressed the ing and stated th to her husband and the\n",
      "\n",
      "Errors found in line 68. \n",
      "Original sentence: os b f c are innocent and that they are victims of red\n",
      "Word-level correction: os b f c are innocent and that they are victims of red\n",
      "Context-level correction: of mb f c are innocent and that they are victims of red\n",
      "\n",
      "Errors found in line 69. \n",
      "Original sentence: hysteria she saij that as soon as it was decided that\n",
      "Word & context-level correction: hysteria she said that as soon as it was decided that\n",
      "\n",
      "Errors found in line 70. \n",
      "Original sentence: the defendants were cornr nists the trial became a massacre\n",
      "Word-level correction: the defendant were corner fists the trial became a massacre\n",
      "Context-level correction: the defendant were torn exists the trial became a massacre\n",
      "\n",
      "Errors found in line 72. \n",
      "Original sentence: the fight for a better world flow you must help us to free\n",
      "Word-level correction: the fight for a better world flow you must help us to free\n",
      "Context-level correction: the fight for a better world flog you must help us to free\n",
      "\n",
      "Errors found in line 73. \n",
      "Original sentence: my husband and the ro\n",
      "Word-level correction: my husband and the to\n",
      "Context-level correction: my husband and the so\n",
      "\n",
      "Errors found in line 73. \n",
      "Original sentence: bl g\n",
      "Word-level correction: bl g\n",
      "Context-level correction: by g\n",
      "\n",
      "Errors found in line 75. \n",
      "Original sentence: ath t k n made q collection speech in which\n",
      "Word-level correction: at t k n made q collection speech in which\n",
      "Context-level correction: ah it k n made q collection speech in which\n",
      "\n",
      "Errors found in line 76. \n",
      "Original sentence: he st ed tti t the r s n rg were being acrific on\n",
      "Word-level correction: he st ed ti t the r s n re were being <not found> on\n",
      "Context-level correction: he s ed tate at the r s no rugs were being acrific on\n",
      "\n",
      "Errors found in line 78. \n",
      "Original sentence: killer in korea by stepping the operation killer of the\n",
      "Word-level correction: killer in more by stepping the operation killer of the\n",
      "Context-level correction: killed in area by stopping the operation tiller of the\n",
      "\n",
      "Errors found in line 79. \n",
      "Original sentence: ro e g\n",
      "Word & context-level correction: to e g\n",
      "\n",
      "Errors found in line 79. \n",
      "Original sentence: they face doatli because they fought for u\n",
      "Word-level correction: they face death because they fought for u\n",
      "Context-level correction: the face doute because they fought for us\n",
      "\n",
      "Errors found in line 82. \n",
      "Original sentence: ith ry van kl ech next spoke and stated that she\n",
      "Word-level correction: it by van ll each next spoke and stated that she\n",
      "Context-level correction: it by an ke e next spoke and stated that she\n",
      "\n",
      "Errors found in line 83. \n",
      "Original sentence: made a th rb ghmhvestigat of the rd nb rg case and\n",
      "Word-level correction: made a th re <not found> of the rd no re case and\n",
      "Context-level correction: made a tr r ghmhvestigat of the rd and re case and\n",
      "\n",
      "Errors found in line 84. \n",
      "Original sentence: that she came to the conclusion that the rosenbt rgs were\n",
      "Word-level correction: that she came to the conclusion that the rodent rags were\n",
      "Context-level correction: that she came to the conclusion that the rodent rugs were\n",
      "\n",
      "Errors found in line 85. \n",
      "Original sentence: condemned to death not because they coriritted crime\n",
      "Word-level correction: condemned to death not because they <not found> crime\n",
      "Context-level correction: condemned to death not because they coriritted crime\n",
      "\n",
      "Errors found in line 87. \n",
      "Original sentence: fighti g for progress and a better world\n",
      "Word-level correction: fight g for progress and a better world\n",
      "Context-level correction: fight go for progress and a better world\n",
      "\n",
      "Errors found in line 89. \n",
      "Original sentence: bths i\n",
      "Word-level correction: baths i\n",
      "Context-level correction: this i\n",
      "\n",
      "Errors found in line 89. \n",
      "Original sentence: j tchf next spoke and told the people\n",
      "Word & context-level correction: j the next spoke and told the people\n",
      "\n",
      "Errors found in line 92. \n",
      "Original sentence: about the ro ijb g c se and fight for their lives and\n",
      "Word-level correction: about the to in g c se and fight for their lives and\n",
      "Context-level correction: about the rob i g c es and fight for their lives and\n",
      "\n",
      "Errors found in line 93. \n",
      "Original sentence: she read a telegrnm from jillia i patt rson\n",
      "Word-level correction: she read a <not found> from villa i part son\n",
      "Context-level correction: she read a telegrnm from axilla in part son\n",
      "\n",
      "Errors found in line 94. \n",
      "Original sentence: national executive ecretdry of the civil rights congress\n",
      "Word-level correction: national executive <not found> of the civil rights congress\n",
      "Context-level correction: national executive ecretdry of the civil rights congress\n",
      "\n",
      "Errors found in line 95. \n",
      "Original sentence: an organization listed by the attorney general as coming\n",
      "Word-level correction: an organization listed by the attorney general as coming\n",
      "Context-level correction: an organization lifted by the attorney general s coming\n",
      "\n",
      "Errors found in line 96. \n",
      "Original sentence: within the purview of executive rder in which he\n",
      "Word-level correction: within the purves of executive order in which he\n",
      "Context-level correction: within the view of executive order in which he\n",
      "\n",
      "Errors found in line 98. \n",
      "Original sentence: congress in the fight for justice in the i q v berg case\n",
      "Word-level correction: congress in the fight for justice in the i q v berg case\n",
      "Context-level correction: congress in the fight for justice in the i q v berg came\n",
      "\n",
      "Errors found in line 100. \n",
      "Original sentence: gor dthethg cor ared the ro enb rg case\n",
      "Word-level correction: for <not found> cor are the to end re case\n",
      "Context-level correction: for dthethg cord are the so esq pg case\n",
      "\n",
      "Errors found in line 101. \n",
      "Original sentence: to the case of sacc and vthnz ti and the dri yfu s case\n",
      "Word-level correction: to the case of sac and the ti and the dry you s case\n",
      "Context-level correction: to the case of sacs and the ti and the dry u s case\n",
      "\n",
      "Errors found in line 102. \n",
      "Original sentence: ile stated the way this case was conducted the rosi t rg\n",
      "Word-level correction: ill stated the way this case was conducted the rose t re\n",
      "Context-level correction: the stated the way this case was conducted the rosy tr r\n",
      "\n",
      "Errors found in line 103. \n",
      "Original sentence: could not get a fnir trial\n",
      "Word & context-level correction: could not get a fair trial\n",
      "\n",
      "Errors found in line 103. \n",
      "Original sentence: jhy is it that the azi and\n",
      "Word-level correction: why is it that the ami and\n",
      "Context-level correction: why is it that the arm and\n",
      "\n",
      "Errors found in line 104. \n",
      "Original sentence: fascist spies were net given a death sentence in time of\n",
      "Word-level correction: fact spies were net given a death sentence in time of\n",
      "Context-level correction: fact spies were not given a death sentence in time of\n",
      "\n",
      "Errors found in line 105. \n",
      "Original sentence: war and the o nb g wore given a death sentence in time\n",
      "Word-level correction: war and the o no g wore given a death sentence in time\n",
      "Context-level correction: war and the o ne g wore given a death sentence in time\n",
      "-----\n",
      "total words checked: 700\n",
      "total unknown words: 13\n",
      "total potential errors found: 165\n",
      "total mismatches (word-level vs. context-level): 128\n",
      "CPU times: user 12min 11s, sys: 16.9 s, total: 12min 28s\n",
      "Wall time: 12min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context(\"testdata/OCRsample.txt\", \\\n",
    "                         dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                         transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "total words checked: 209\n",
      "total unknown words: 0\n",
      "total potential errors found: 0\n",
      "total mismatches (word-level vs. context-level): 0\n",
      "CPU times: user 4min 43s, sys: 6.96 s, total: 4min 50s\n",
      "Wall time: 5min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "correct_document_context(\"testdata/tiny.txt\", \\\n",
    "                         dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                         transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
