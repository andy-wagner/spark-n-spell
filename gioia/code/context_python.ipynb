{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Based on SymSpell:\n",
    "\n",
    "Originally written in C#:\n",
    "\n",
    "// SymSpell: 1 million times faster through Symmetric Delete spelling correction algorithm\n",
    "//\n",
    "// The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup \n",
    "// for a given Damerau-Levenshtein distance. It is six orders of magnitude faster and language independent.\n",
    "// Opposite to other algorithms only deletes are required, no transposes + replaces + inserts.\n",
    "// Transposes + replaces + inserts of the input term are transformed into deletes of the dictionary term.\n",
    "// Replaces and inserts are expensive and language dependent: e.g. Chinese has 70,000 Unicode Han characters!\n",
    "//\n",
    "// Copyright (C) 2015 Wolf Garbe\n",
    "// Version: 3.0\n",
    "// Author: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// Maintainer: Wolf Garbe <wolf.garbe@faroo.com>\n",
    "// URL: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "// Description: http://blog.faroo.com/2012/06/07/improved-edit-distance-based-spelling-correction/\n",
    "//\n",
    "// License:\n",
    "// This program is free software; you can redistribute it and/or modify\n",
    "// it under the terms of the GNU Lesser General Public License, \n",
    "// version 3.0 (LGPL-3.0) as published by the Free Software Foundation.\n",
    "// http://www.opensource.org/licenses/LGPL-3.0\n",
    "//\n",
    "// Usage: single word + Enter:  Display spelling suggestions\n",
    "//        Enter without input:  Terminate the program\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math # GD: needed to calculate logs below\n",
    "from scipy.stats import poisson # GD: needed to calculate emission probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_edit_distance = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_deletes_list(w):\n",
    "    '''given a word, derive strings with up to max_edit_distance characters deleted'''\n",
    "    deletes = []\n",
    "    queue = [w]\n",
    "    for d in range(max_edit_distance):\n",
    "        temp_queue = []\n",
    "        for word in queue:\n",
    "            if len(word)>1:\n",
    "                for c in range(len(word)):  # character index\n",
    "                    word_minus_c = word[:c] + word[c+1:]\n",
    "                    if word_minus_c not in deletes:\n",
    "                        deletes.append(word_minus_c)\n",
    "                    if word_minus_c not in temp_queue:\n",
    "                        temp_queue.append(word_minus_c)\n",
    "        queue = temp_queue\n",
    "        \n",
    "    return deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get_deletes_list(\"tomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary_entry(w, dictionary, longest_word_length):\n",
    "    '''add word and its derived deletions to dictionary'''\n",
    "    # check if word is already in dictionary\n",
    "    # dictionary entries are in the form: (list of suggested corrections, frequency of word in corpus)\n",
    "\n",
    "    new_real_word_added = False\n",
    "    if w in dictionary:\n",
    "        dictionary[w] = (dictionary[w][0], dictionary[w][1] + 1)  # increment count of word in corpus\n",
    "    else:\n",
    "        dictionary[w] = ([], 1)  \n",
    "        longest_word_length = max(longest_word_length, len(w))\n",
    "        \n",
    "    if dictionary[w][1]==1:\n",
    "        # first appearance of word in corpus\n",
    "        # n.b. word may already be in dictionary as a derived word (deleting character from a real word)\n",
    "        # but counter of frequency of word in corpus is not incremented in those cases)\n",
    "        \n",
    "        new_real_word_added = True\n",
    "        deletes = get_deletes_list(w)\n",
    "        \n",
    "        for item in deletes:\n",
    "            if item in dictionary:\n",
    "                # add (correct) word to delete's suggested correction list if not already there\n",
    "                if item not in dictionary[item][0]:\n",
    "                    dictionary[item][0].append(w)\n",
    "            else:\n",
    "                dictionary[item] = ([w], 0)  # note frequency of word in corpus is not incremented\n",
    "        \n",
    "    return new_real_word_added, longest_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dictionary(fname):\n",
    "    \n",
    "    print \"Creating dictionary...\" \n",
    "\n",
    "    dictionary = dict() # GD: moved here to ensure that dictionary is re-initialized.\n",
    "    longest_word_length = 0 # GD: moved here to ensure that it is re-initialized.\n",
    "    start_prob = dict()\n",
    "    transition_prob = dict()\n",
    "    transition_prob_norm = dict()    \n",
    "    word_count = 0\n",
    "    \n",
    "    with open(fname) as file:    \n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            for sentence in line.split('.'): # GD: added to ensure split at sentence level\n",
    "                \n",
    "                words = re.findall('[a-z]+', sentence.lower())  # separate by words by non-alphabetical characters      \n",
    "                \n",
    "                for num_word in xrange(len(words)):\n",
    "                    \n",
    "                    new_word, longest_word_length = \\\n",
    "                        create_dictionary_entry(words[num_word], dictionary, longest_word_length)\n",
    "                    \n",
    "                    if new_word:\n",
    "                        word_count += 1\n",
    "                        \n",
    "                    # GD: added to calculate probabilities for Hidden Markov Model\n",
    "                    if num_word == 0:\n",
    "\n",
    "                        # Probability of a word being at the beginning of a sentence\n",
    "                        if words[num_word] in start_prob:\n",
    "                            start_prob[words[num_word]] += 1\n",
    "                        else:\n",
    "                            start_prob[words[num_word]] = 1\n",
    "                    else:\n",
    "                        \n",
    "                        # Probability of transitionining from one word to another\n",
    "                        # Key format (word, previous word)\n",
    "                        if (words[num_word], words[num_word - 1]) in transition_prob:\n",
    "                            transition_prob[(words[num_word], words[num_word - 1])] += 1\n",
    "                        else:\n",
    "                            transition_prob[(words[num_word], words[num_word - 1])] = 1\n",
    "                            \n",
    "                        # Used to normalize probabilities\n",
    "                        if words[num_word] in transition_prob_norm:\n",
    "                            transition_prob_norm[words[num_word]] += 1\n",
    "                        else:\n",
    "                            transition_prob_norm[words[num_word]] = 1\n",
    "                              \n",
    "    # GD: added to convert counts to log-probabilities (to avoid underflow)\n",
    "    # Note: natural logarithm, not base-10\n",
    "    total_start_words = float(sum(start_prob.values()))\n",
    "    default_start_prob = math.log(1/total_start_words)\n",
    "    start_prob.update({k: math.log(v/total_start_words) for k, v in start_prob.items()})\n",
    "    default_transition_prob = math.log(1./word_count)\n",
    "    transition_prob.update({k: math.log(v/float(transition_prob_norm[k[0]])) for k, v in transition_prob.items()})\n",
    "                    \n",
    "    print \"Total unique words in corpus: %i\" % word_count\n",
    "    print \"Total items in dictionary (corpus words and deletions): %i\" % len(dictionary)\n",
    "    print \"  Edit distance for deletions: %i\" % max_edit_distance\n",
    "    print \"  Length of longest word in corpus: %i\" % longest_word_length\n",
    "    print \"Total unique words appearing at the start of a sentence: %i\" % len(start_prob)\n",
    "    print \"Total unique word transitions: %i\" % len(transition_prob)\n",
    "        \n",
    "    return dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n",
      "Total unique words in corpus: 29157\n",
      "Total items in dictionary (corpus words and deletions): 2151998\n",
      "  Edit distance for deletions: 3\n",
      "  Length of longest word in corpus: 18\n",
      "Total unique words appearing at the start of a sentence: 15297\n",
      "Total unique word transitions: 319665\n",
      "CPU times: user 33.2 s, sys: 1.07 s, total: 34.2 s\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob = \\\n",
    "    create_dictionary(\"testdata/big.txt\")\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b> <p>\n",
    "Can look up a specific entry in the dictionary below. <br>\n",
    "shows (possible corrections, and frequency that entry itself is in corpus - 0 if not a real word) <br>\n",
    "Note: will return key error if there are no corrections (because we are accessing dictionary directly here)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['essentially', 'essentials'], 92)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print dictionary[\"essential\"]\n",
    "except KeyError:\n",
    "    print 'Not in dictionary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['wrack'], 0)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print dictionary[\"wack\"]\n",
    "except KeyError:\n",
    "    print 'Not in dictionary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dameraulevenshtein(seq1, seq2):\n",
    "    \"\"\"Calculate the Damerau-Levenshtein distance between sequences.\n",
    "\n",
    "    Source: http://mwh.geek.nz/2009/04/26/python-damerau-levenshtein-distance/\n",
    "    \n",
    "    This distance is the number of additions, deletions, substitutions,\n",
    "    and transpositions needed to transform the first sequence into the\n",
    "    second. Although generally used with strings, any sequences of\n",
    "    comparable objects will work.\n",
    "\n",
    "    Transpositions are exchanges of *consecutive* characters; all other\n",
    "    operations are self-explanatory.\n",
    "\n",
    "    This implementation is O(N*M) time and O(M) space, for N and M the\n",
    "    lengths of the two sequences.\n",
    "\n",
    "    >>> dameraulevenshtein('ba', 'abc')\n",
    "    2\n",
    "    >>> dameraulevenshtein('fee', 'deed')\n",
    "    2\n",
    "\n",
    "    It works with arbitrary sequences too:\n",
    "    >>> dameraulevenshtein('abcd', ['b', 'a', 'c', 'd', 'e'])\n",
    "    2\n",
    "    \"\"\"\n",
    "    # codesnippet:D0DE4716-B6E6-4161-9219-2903BF8F547F\n",
    "    # Conceptually, this is based on a len(seq1) + 1 * len(seq2) + 1 matrix.\n",
    "    # However, only the current and two previous rows are needed at once,\n",
    "    # so we only store those.\n",
    "    oneago = None\n",
    "    thisrow = range(1, len(seq2) + 1) + [0]\n",
    "    for x in xrange(len(seq1)):\n",
    "        # Python lists wrap around for negative indices, so put the\n",
    "        # leftmost column at the *end* of the list. This matches with\n",
    "        # the zero-indexed strings and saves extra calculation.\n",
    "        twoago, oneago, thisrow = oneago, thisrow, [0] * len(seq2) + [x + 1]\n",
    "        for y in xrange(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            # This block deals with transpositions\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1]\n",
    "                and seq1[x-1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_suggestions(string, dictionary, longest_word_length, silent=False, min_count=0):\n",
    "    '''return list of suggested corrections for potentially incorrectly spelled word'''\n",
    "    \n",
    "    if (len(string) - longest_word_length) > max_edit_distance:\n",
    "        if not silent:\n",
    "            print \"no items in dictionary within maximum edit distance\"\n",
    "        return []\n",
    "    \n",
    "    # suggestions = []\n",
    "    # s_dictionary = {}\n",
    "    suggest_dict = {}\n",
    "    \n",
    "    queue = [string]\n",
    "    q_dictionary = {}  # items other than string that we've checked\n",
    "    \n",
    "    while len(queue)>0:\n",
    "        q_item = queue[0]  # pop\n",
    "        # print \"processing '%s'\" % q_item\n",
    "        queue = queue[1:]\n",
    "        \n",
    "        # process queue item\n",
    "        if (q_item in dictionary) and (q_item not in suggest_dict):\n",
    "            if (dictionary[q_item][1]>=min_count):# GD added to trim list for context checking\n",
    "            # word is in dictionary, and is a word from the corpus, and not already in suggestion list\n",
    "            # so add to suggestion dictionary, indexed by the word with value (frequency in corpus, edit distance)\n",
    "            # note q_items that are not the input string are shorter than input string \n",
    "            # since only deletes are added (unless manual dictionary corrections are added)\n",
    "                assert len(string)>=len(q_item)\n",
    "                suggest_dict[q_item] = (dictionary[q_item][1], len(string) - len(q_item))\n",
    "\n",
    "            ## the suggested corrections for q_item as stored in dictionary (whether or not\n",
    "            ## q_item itself is a valid word or merely a delete) can be valid corrections\n",
    "            for sc_item in dictionary[q_item][0]:\n",
    "                if (sc_item not in suggest_dict):\n",
    "                    # compute edit distance\n",
    "                    # if len(sc_item)==len(q_item):\n",
    "                    #    item_dist = len(string) - len(q_item)\n",
    "                    # suggested items should always be longer (unless manual corrections are added)\n",
    "                    assert len(sc_item)>len(q_item)\n",
    "                    # q_items that are not input should be shorter than original string \n",
    "                    # (unless manual corrections added)\n",
    "                    assert len(q_item)<=len(string)\n",
    "                    if len(q_item)==len(string):\n",
    "                        assert q_item==string\n",
    "                        item_dist = len(sc_item) - len(q_item)\n",
    "                    #elif len(q_item)==len(string):\n",
    "                        # a suggestion could be longer or shorter than original string (bug in original FAROO?)\n",
    "                        # if suggestion is from string's suggestion list, sc_item will be longer\n",
    "                        # if suggestion is from a delete's suggestion list, sc_item may be shorter\n",
    "                    #   item_dist = abs(len(sc_item) - len(q_item))\n",
    "                    #else:\n",
    "                    # check in original code, but probably not necessary because string has already checked\n",
    "                    assert sc_item!=string\n",
    "\n",
    "                    # calculate edit distance using, for example, Damerau-Levenshtein distance\n",
    "                    item_dist = dameraulevenshtein(sc_item, string)\n",
    "\n",
    "                    if item_dist<=max_edit_distance:\n",
    "                        assert sc_item in dictionary  # should already be in dictionary if in suggestion list\n",
    "                        if (dictionary[q_item][1]>=min_count):# GD added to trim list for context checking\n",
    "                            suggest_dict[sc_item] = (dictionary[sc_item][1], item_dist)\n",
    "\n",
    "        # now generate deletes (e.g. a substring of string or of a delete) from the queue item\n",
    "        # as additional items to check -- add to end of queue\n",
    "        assert len(string)>=len(q_item)\n",
    "        if (len(string)-len(q_item))<max_edit_distance and len(q_item)>1:\n",
    "            for c in range(len(q_item)): # character index        \n",
    "                word_minus_c = q_item[:c] + q_item[c+1:]\n",
    "                if word_minus_c not in q_dictionary:\n",
    "                    queue.append(word_minus_c)\n",
    "                    q_dictionary[word_minus_c] = None  # arbitrary value, just to identify we checked this\n",
    "             \n",
    "    # queue is now empty: convert suggestions in dictionary to list for output\n",
    "    \n",
    "    if not silent:\n",
    "        print \"number of possible corrections: %i\" %len(suggest_dict)\n",
    "        print \"  edit distance for deletions: %i\" % max_edit_distance\n",
    "    \n",
    "    # output option 1\n",
    "    # sort results by ascending order of edit distance and descending order of frequency\n",
    "    #     and return list of suggested corrections only:\n",
    "    # return sorted(suggest_dict, key = lambda x: (suggest_dict[x][1], -suggest_dict[x][0]))\n",
    "\n",
    "    # output option 2\n",
    "    # return list of suggestions with (correction, (frequency in corpus, edit distance)):\n",
    "    as_list = suggest_dict.items()\n",
    "    return sorted(as_list, key = lambda (term, (freq, dist)): (dist, -freq))\n",
    "\n",
    "    '''\n",
    "    Option 1:\n",
    "    get_suggestions(\"file\")\n",
    "    ['file', 'five', 'fire', 'fine', ...]\n",
    "    \n",
    "    Option 2:\n",
    "    get_suggestions(\"file\")\n",
    "    [('file', (5, 0)),\n",
    "     ('five', (67, 1)),\n",
    "     ('fire', (54, 1)),\n",
    "     ('fine', (17, 1))...]  \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Type in word to correct below, to test and get whole list of possible suggestions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible corrections: 142\n",
      "  edit distance for deletions: 3\n",
      "CPU times: user 16.6 ms, sys: 6.92 ms, total: 23.5 ms\n",
      "Wall time: 17.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mitten', (0, 1)),\n",
       " ('mittes', (0, 1)),\n",
       " ('ittens', (0, 1)),\n",
       " ('matters', (136, 2)),\n",
       " ('bitten', (13, 2)),\n",
       " ('kitten', (7, 2)),\n",
       " ('listens', (2, 2)),\n",
       " ('battens', (1, 2)),\n",
       " ('smitten', (1, 2)),\n",
       " ('itten', (0, 2)),\n",
       " ('mites', (0, 2)),\n",
       " ('miten', (0, 2)),\n",
       " ('ittes', (0, 2)),\n",
       " ('mtten', (0, 2)),\n",
       " ('mttes', (0, 2)),\n",
       " ('ttens', (0, 2)),\n",
       " ('itens', (0, 2)),\n",
       " ('miens', (0, 2)),\n",
       " ('mittn', (0, 2)),\n",
       " ('mitte', (0, 2)),\n",
       " ('ittns', (0, 2)),\n",
       " ('mitts', (0, 2)),\n",
       " ('matter', (365, 3)),\n",
       " ('sitting', (269, 3)),\n",
       " ('minutes', (146, 3)),\n",
       " ('written', (117, 3)),\n",
       " ('miles', (110, 3)),\n",
       " ('citizens', (109, 3)),\n",
       " ('letters', (108, 3)),\n",
       " ('listen', (100, 3)),\n",
       " ('cities', (77, 3)),\n",
       " ('bitter', (47, 3)),\n",
       " ('masters', (37, 3)),\n",
       " ('intense', (34, 3)),\n",
       " ('witness', (33, 3)),\n",
       " ('attend', (29, 3)),\n",
       " ('mistress', (24, 3)),\n",
       " ('fitted', (23, 3)),\n",
       " ('mines', (22, 3)),\n",
       " ('fitting', (21, 3)),\n",
       " ('miners', (19, 3)),\n",
       " ('mitenka', (16, 3)),\n",
       " ('tens', (16, 3)),\n",
       " ('sisters', (16, 3)),\n",
       " ('intent', (13, 3)),\n",
       " ('mothers', (12, 3)),\n",
       " ('mutton', (11, 3)),\n",
       " ('bites', (10, 3)),\n",
       " ('sites', (9, 3)),\n",
       " ('omitted', (9, 3)),\n",
       " ('buttons', (8, 3)),\n",
       " ('fitness', (8, 3)),\n",
       " ('rotten', (8, 3)),\n",
       " ('attends', (8, 3)),\n",
       " ('matrena', (8, 3)),\n",
       " ('intend', (8, 3)),\n",
       " ('titles', (7, 3)),\n",
       " ('pitted', (6, 3)),\n",
       " ('matted', (6, 3)),\n",
       " ('omitting', (5, 3)),\n",
       " ('witted', (5, 3)),\n",
       " ('kitchens', (5, 3)),\n",
       " ('hitting', (5, 3)),\n",
       " ('intends', (5, 3)),\n",
       " ('emitted', (4, 3)),\n",
       " ('items', (4, 3)),\n",
       " ('mutter', (4, 3)),\n",
       " ('pitting', (4, 3)),\n",
       " ('mates', (4, 3)),\n",
       " ('emitting', (3, 3)),\n",
       " ('litter', (3, 3)),\n",
       " ('dites', (3, 3)),\n",
       " ('mister', (2, 3)),\n",
       " ('distend', (2, 3)),\n",
       " ('withers', (2, 3)),\n",
       " ('mints', (2, 3)),\n",
       " ('motions', (2, 3)),\n",
       " ('cutters', (2, 3)),\n",
       " ('mien', (2, 3)),\n",
       " ('linens', (2, 3)),\n",
       " ('hastens', (2, 3)),\n",
       " ('softens', (2, 3)),\n",
       " ('gotten', (2, 3)),\n",
       " ('misses', (2, 3)),\n",
       " ('dickens', (2, 3)),\n",
       " ('viens', (2, 3)),\n",
       " ('winters', (2, 3)),\n",
       " ('matins', (2, 3)),\n",
       " ('potters', (2, 3)),\n",
       " ('amiens', (1, 3)),\n",
       " ('metes', (1, 3)),\n",
       " ('filters', (1, 3)),\n",
       " ('mattress', (1, 3)),\n",
       " ('pickens', (1, 3)),\n",
       " ('distends', (1, 3)),\n",
       " ('kittenish', (1, 3)),\n",
       " ('gutters', (1, 3)),\n",
       " ('rioters', (1, 3)),\n",
       " ('hatters', (1, 3)),\n",
       " ('sittings', (1, 3)),\n",
       " ('fetters', (1, 3)),\n",
       " ('fatten', (1, 3)),\n",
       " ('remittent', (1, 3)),\n",
       " ('imitates', (1, 3)),\n",
       " ('pities', (1, 3)),\n",
       " ('utters', (1, 3)),\n",
       " ('athens', (1, 3)),\n",
       " ('tiens', (1, 3)),\n",
       " ('maidens', (1, 3)),\n",
       " ('moyens', (1, 3)),\n",
       " ('matting', (1, 3)),\n",
       " ('milton', (1, 3)),\n",
       " ('mists', (1, 3)),\n",
       " ('hittel', (1, 3)),\n",
       " ('cottons', (1, 3)),\n",
       " ('intents', (1, 3)),\n",
       " ('midges', (1, 3)),\n",
       " ('wotten', (1, 3)),\n",
       " ('patterns', (1, 3)),\n",
       " ('mtns', (0, 3)),\n",
       " ('ites', (0, 3)),\n",
       " ('iten', (0, 3)),\n",
       " ('mten', (0, 3)),\n",
       " ('mies', (0, 3)),\n",
       " ('itns', (0, 3)),\n",
       " ('mitt', (0, 3)),\n",
       " ('mits', (0, 3)),\n",
       " ('mitn', (0, 3)),\n",
       " ('ittn', (0, 3)),\n",
       " ('itte', (0, 3)),\n",
       " ('mtes', (0, 3)),\n",
       " ('mite', (0, 3)),\n",
       " ('mins', (0, 3)),\n",
       " ('mens', (0, 3)),\n",
       " ('ttns', (0, 3)),\n",
       " ('ttes', (0, 3)),\n",
       " ('mtte', (0, 3)),\n",
       " ('itts', (0, 3)),\n",
       " ('iens', (0, 3)),\n",
       " ('mttn', (0, 3)),\n",
       " ('mtts', (0, 3)),\n",
       " ('tten', (0, 3))]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "get_suggestions(\"mittens\", dictionary, longest_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.19 s, sys: 33.3 ms, total: 2.23 s\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"acamodation\", dictionary, longest_word_length, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 14.4 ms, total: 2.27 s\n",
      "Wall time: 2.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"acomodation\", dictionary, longest_word_length, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.21 s, sys: 35.8 ms, total: 6.25 s\n",
      "Wall time: 6.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#benchmark timing\n",
    "for i in range(1000):\n",
    "    get_suggestions(\"hous\", dictionary, longest_word_length, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get best word\n",
    "def best_word(s, dictionary, silent=False):\n",
    "    try:\n",
    "        return get_suggestions(s, dictionary, longest_word_length, silent)[0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Type in word to correct below, to test and get most suggested word.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible corrections: 163\n",
      "  edit distance for deletions: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('hello', (1, 0))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_word(\"hello\", dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correct_document(fname, dictionary):\n",
    "    with open(fname) as file:\n",
    "        doc_word_count = 0\n",
    "        corrected_word_count = 0\n",
    "        unknown_word_count = 0\n",
    "        print \"Finding misspelled words in your document...\" \n",
    "        \n",
    "        for i, line in enumerate(file):\n",
    "            doc_words = re.findall('[a-z]+', line.lower())  # separate by words by non-alphabetical characters      \n",
    "            for doc_word in doc_words:\n",
    "                doc_word_count += 1\n",
    "                suggestion = best_word(doc_word, dictionary, silent=True)\n",
    "                if suggestion is None:\n",
    "                    print \"In line %i, the word < %s > was not found (no suggested correction)\" % (i, doc_word)\n",
    "                    unknown_word_count += 1\n",
    "                elif suggestion[0]!=doc_word:\n",
    "                    print \"In line %i, %s: suggested correction is < %s >\" % (i, doc_word, suggestion[0])\n",
    "                    corrected_word_count += 1\n",
    "        \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % doc_word_count\n",
    "    print \"total unknown words: %i\" % unknown_word_count\n",
    "    print \"total potential errors found: %i\" % corrected_word_count\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Provide text file to correct, and give all best word suggestions (word level only) for errors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "In line 4, za: suggested correction is < a >\n",
      "In line 6, tee: suggested correction is < see >\n",
      "-----\n",
      "total words checked: 27\n",
      "total unknown words: 0\n",
      "total potential errors found: 2\n"
     ]
    }
   ],
   "source": [
    "correct_document(\"testdata/test.txt\", dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding misspelled words in your document...\n",
      "In line 3, taiths: suggested correction is < faith >\n",
      "In line 11, the word < oonipiittee > was not found (no suggested correction)\n",
      "In line 13, tj: suggested correction is < t >\n",
      "In line 13, mnnff: suggested correction is < snuff >\n",
      "In line 13, gjpt: suggested correction is < get >\n",
      "In line 15, bh: suggested correction is < b >\n",
      "In line 15, snc: suggested correction is < sac >\n",
      "In line 15, uth: suggested correction is < th >\n",
      "In line 15, unuer: suggested correction is < under >\n",
      "In line 20, mthiitt: suggested correction is < thirty >\n",
      "In line 21, cas: suggested correction is < as >\n",
      "In line 22, pythian: suggested correction is < scythian >\n",
      "In line 26, brainin: suggested correction is < brain >\n",
      "In line 27, jfl: suggested correction is < f >\n",
      "In line 28, ji: suggested correction is < i >\n",
      "In line 28, stice: suggested correction is < stick >\n",
      "In line 28, blaci: suggested correction is < black >\n",
      "In line 28, eug: suggested correction is < dug >\n",
      "In line 28, debbs: suggested correction is < debts >\n",
      "In line 29, nericans: suggested correction is < americans >\n",
      "In line 30, ainin: suggested correction is < dining >\n",
      "In line 30, ergs: suggested correction is < eggs >\n",
      "In line 31, trumped: suggested correction is < trumpet >\n",
      "In line 32, erican: suggested correction is < american >\n",
      "In line 33, unorthodox: suggested correction is < orthodox >\n",
      "In line 33, nenance: suggested correction is < penance >\n",
      "In line 33, thg: suggested correction is < th >\n",
      "In line 34, sln: suggested correction is < sly >\n",
      "In line 34, rgs: suggested correction is < rags >\n",
      "In line 38, williaij: suggested correction is < william >\n",
      "In line 38, eu: suggested correction is < e >\n",
      "In line 40, fcsf: suggested correction is < ff >\n",
      "In line 40, ber: suggested correction is < be >\n",
      "In line 42, unorthodoxy: suggested correction is < orthodox >\n",
      "In line 42, thpt: suggested correction is < that >\n",
      "In line 42, the word < senbrnrgs > was not found (no suggested correction)\n",
      "In line 44, fascism: suggested correction is < fascia >\n",
      "In line 62, loo: suggested correction is < look >\n",
      "In line 65, ththn: suggested correction is < then >\n",
      "In line 65, scbell: suggested correction is < bell >\n",
      "In line 65, ife: suggested correction is < if >\n",
      "In line 65, yktcn: suggested correction is < skin >\n",
      "In line 65, thl: suggested correction is < th >\n",
      "In line 66, thi: suggested correction is < this >\n",
      "In line 68, saij: suggested correction is < said >\n",
      "In line 69, defendants: suggested correction is < defendant >\n",
      "In line 69, cornr: suggested correction is < corner >\n",
      "In line 69, nists: suggested correction is < fists >\n",
      "In line 72, ro: suggested correction is < o >\n",
      "In line 74, ath: suggested correction is < at >\n",
      "In line 75, tti: suggested correction is < ti >\n",
      "In line 75, rg: suggested correction is < g >\n",
      "In line 75, acrific: suggested correction is < pacific >\n",
      "In line 77, korea: suggested correction is < area >\n",
      "In line 78, ro: suggested correction is < o >\n",
      "In line 78, doatli: suggested correction is < coat >\n",
      "In line 81, ith: suggested correction is < it >\n",
      "In line 81, ry: suggested correction is < cry >\n",
      "In line 81, kl: suggested correction is < l >\n",
      "In line 81, ech: suggested correction is < each >\n",
      "In line 82, rb: suggested correction is < b >\n",
      "In line 82, the word < ghmhvestigat > was not found (no suggested correction)\n",
      "In line 82, nb: suggested correction is < b >\n",
      "In line 82, rg: suggested correction is < g >\n",
      "In line 83, rosenbt: suggested correction is < rodent >\n",
      "In line 83, rgs: suggested correction is < rags >\n",
      "In line 84, coriritted: suggested correction is < committed >\n",
      "In line 86, fighti: suggested correction is < fight >\n",
      "In line 88, bths: suggested correction is < baths >\n",
      "In line 88, tchf: suggested correction is < th >\n",
      "In line 91, ro: suggested correction is < o >\n",
      "In line 91, ijb: suggested correction is < i >\n",
      "In line 92, telegrnm: suggested correction is < telegram >\n",
      "In line 92, jillia: suggested correction is < william >\n",
      "In line 92, patt: suggested correction is < part >\n",
      "In line 92, rson: suggested correction is < son >\n",
      "In line 93, ecretdry: suggested correction is < secretary >\n",
      "In line 95, purview: suggested correction is < purves >\n",
      "In line 95, rder: suggested correction is < order >\n",
      "In line 99, gor: suggested correction is < or >\n",
      "In line 99, dthethg: suggested correction is < teeth >\n",
      "In line 99, ared: suggested correction is < are >\n",
      "In line 99, ro: suggested correction is < o >\n",
      "In line 99, enb: suggested correction is < en >\n",
      "In line 99, rg: suggested correction is < g >\n",
      "In line 100, sacc: suggested correction is < sac >\n",
      "In line 100, vthnz: suggested correction is < then >\n",
      "In line 100, dri: suggested correction is < dr >\n",
      "In line 100, yfu: suggested correction is < you >\n",
      "In line 101, ile: suggested correction is < lie >\n",
      "In line 101, rosi: suggested correction is < rose >\n",
      "In line 101, rg: suggested correction is < g >\n",
      "In line 102, fnir: suggested correction is < fair >\n",
      "In line 102, jhy: suggested correction is < why >\n",
      "In line 102, azi: suggested correction is < ai >\n",
      "In line 103, fascist: suggested correction is < fascia >\n",
      "In line 104, nb: suggested correction is < b >\n",
      "-----\n",
      "total words checked: 700\n",
      "total unknown words: 3\n",
      "total potential errors found: 94\n"
     ]
    }
   ],
   "source": [
    "# from http://www.columbia.edu/acis/cria/rosenberg/sample/\n",
    "correct_document(\"testdata/OCRsample.txt\", dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Serial code for context-based improvement, based on Hidden Markov Model. Integrated with code above - probabilities calculated when generating main dictionary (using same corpus).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_emission_prob(edit_dist, poisson_lambda=0.01):\n",
    "    \n",
    "    # Poisson(k, l), where k = edit distance and l=0.01\n",
    "    # TODO - validate lambda parameter (taken from Verena's code\n",
    "    \n",
    "    return math.log(poisson.pmf(edit_dist, poisson_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_start_prob(word, start_prob, default_start_prob):\n",
    "    if word in start_prob:\n",
    "        return start_prob[word]\n",
    "    else:\n",
    "        return default_start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transition_prob(cur_word, prev_word, transition_prob, default_transition_prob):\n",
    "    if (cur_word, prev_word) in transition_prob:\n",
    "        return transition_prob[(cur_word, prev_word)]\n",
    "    else:\n",
    "        return default_transition_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_belief(prev_word, prev_belief):\n",
    "    if prev_word in prev_belief:\n",
    "        return prev_belief[prev_word]\n",
    "    else:\n",
    "        return math.log(math.exp(min(prev_belief.values()))/2.) # TODO - confirm default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi_num(words, dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                transition_prob, default_transition_prob, \\\n",
    "                num_word_suggestions=5000):\n",
    "    \n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    path_context = []\n",
    "    path_word = []\n",
    "    \n",
    "    # FOR TESTING - DELETE EVENTUALLY\n",
    "    if type(words) != list:\n",
    "        words = re.findall('[a-z]+', words.lower())  # separate by words by non-alphabetical characters\n",
    "        \n",
    "    # Character level correction\n",
    "    corrections = get_suggestions(words[0], dictionary, longest_word_length, silent=True, min_count=1)\n",
    "\n",
    "    if len(corrections) > num_word_suggestions:\n",
    "        corrections = corrections[0:num_word_suggestions]\n",
    "    if len(corrections) > 0:\n",
    "        path_word.append(corrections[0][0])  # string of most frequent word tuple\n",
    "\n",
    "    # Initialize base cases (t == 0)\n",
    "    for sug_word in corrections:\n",
    "        \n",
    "        # compute the value for all possible starting states\n",
    "        V[0][sug_word[0]] = math.exp(get_start_prob(sug_word[0], start_prob, default_start_prob) \\\n",
    "                                     + get_emission_prob(sug_word[1][1]))\n",
    "        \n",
    "        # remember all the different paths (here its only one state so far)\n",
    "        path[sug_word[0]] = [sug_word[0]]\n",
    " \n",
    "    # normalize for numerical stability\n",
    "    path_temp_sum = sum(V[0].values())\n",
    "    V[0].update({k: math.log(v/path_temp_sum) for k, v in V[0].items()})\n",
    "    prev_corrections = [i[0] for i in corrections]\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        path_context = [max(V[0], key=lambda i: V[0][i])]\n",
    "        return path_word, path_context\n",
    "\n",
    "    # Run Viterbi for t > 0\n",
    "    for t in range(1, len(words)):\n",
    "\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        \n",
    "        # Character level correction\n",
    "        corrections = get_suggestions(words[t], dictionary, longest_word_length, silent=True, min_count=1)\n",
    "\n",
    "        if len(corrections) > num_word_suggestions:\n",
    "            corrections = corrections[0:num_word_suggestions]\n",
    "        if len(corrections) > 0:\n",
    "            path_word.append(corrections[0][0])  # string of most frequent word tuple\n",
    " \n",
    "        for sug_word in corrections:\n",
    "        \n",
    "            sug_word_emission_prob = get_emission_prob(sug_word[1][1])\n",
    "            \n",
    "            # compute the values coming from all possible previous states, only keep the maximum\n",
    "            (prob, word) = max((get_belief(prev_word, V[t-1]) \\\n",
    "                            + get_transition_prob(sug_word[0], prev_word, transition_prob, default_transition_prob) \\\n",
    "                            + sug_word_emission_prob, prev_word) for prev_word in prev_corrections)\n",
    "\n",
    "            # save the maximum value for each state\n",
    "            V[t][sug_word[0]] = math.exp(prob)\n",
    "            # remember the path we came from to get this maximum value\n",
    "            new_path[sug_word[0]] = path[word] + [sug_word[0]]\n",
    "            \n",
    "        # normalize for numerical stability\n",
    "        path_temp_sum = sum(V[t].values())\n",
    "        V[t].update({k: math.log(v/path_temp_sum) for k, v in V[t].items()})\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "        # Don't need to remember the old paths\n",
    "        path = new_path\n",
    "     \n",
    "    (prob, word) = max((V[t][sug_word[0]], sug_word[0]) for sug_word in corrections)\n",
    "    path_context = path[word]\n",
    "\n",
    "    assert len(path_word) == len(path_context)\n",
    "\n",
    "    return path_word, path_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence:  ther sa pile of clothsing on the side of thee train treks\n",
      "Word-level check:  the sa pile of clothing on the side of thee train trees\n",
      "Context-level check:  the a pile of clothing on the side of the train tres\n",
      "CPU times: user 10.9 s, sys: 117 ms, total: 11.1 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentence = \"ther sa pile of clothsing on the side of thee train treks\"\n",
    "word_check, context_check = viterbi_num(sentence, \\\n",
    "            dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob)\n",
    "print 'Original sentence: ', sentence\n",
    "print 'Word-level check: ', \" \".join(word_check)\n",
    "print 'Context-level check: ', \" \".join(context_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fwd_bkw(words, dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                transition_prob, default_transition_prob, \\\n",
    "                num_word_suggestions=5000):\n",
    "\n",
    "# (x, states, a_0, a, e, end_st):\n",
    "    \n",
    "    # FOR TESTING - DELETE EVENTUALLY\n",
    "    if type(words) != list:\n",
    "        words = re.findall('[a-z]+', words.lower())  # separate by words by non-alphabetical characters\n",
    "        \n",
    "    path_word = []\n",
    "        \n",
    "    L = len(words)\n",
    " \n",
    "    # forward part of the algorithm\n",
    "\n",
    "    fwd = []\n",
    "    f_prev = {}\n",
    "    \n",
    "    for i, word_i in enumerate(words):\n",
    "\n",
    "        # Character level correction\n",
    "        corrections = get_suggestions(word_i, dictionary, longest_word_length, silent=True, min_count=1)\n",
    "\n",
    "        if len(corrections) > num_word_suggestions:\n",
    "            corrections = corrections[0:num_word_suggestions]\n",
    "        if len(corrections) > 0:\n",
    "            path_word.append(corrections[0][0])  # string of most frequent word tuple\n",
    "        \n",
    "        f_curr = {}\n",
    "        for st in corrections:\n",
    "\n",
    "            if i == 0:\n",
    "                # base case for the forward part\n",
    "                prev_f_sum = get_start_prob(st[0], start_prob, default_start_prob)\n",
    "            else:\n",
    "                prev_f_sum = math.log(sum(math.exp(get_belief(k, f_prev) \\\n",
    "                            + get_transition_prob(st[0], k, transition_prob, default_transition_prob)) \\\n",
    "                                 for k in prev_corrections))\n",
    " \n",
    "            f_curr[st] = get_emission_prob(st[1][1]) + prev_f_sum\n",
    " \n",
    "        fwd.append(f_curr)\n",
    "        f_prev = f_curr\n",
    "\n",
    "        prev_corrections = [i[0] for i in corrections]\n",
    " \n",
    "    p_fwd = math.log(sum(math.exp(f_curr[k] + get_transition_prob(end_st, k, transition_prob, default_transition_prob)) \\\n",
    "                for k in states))\n",
    " \n",
    "    # backward part of the algorithm\n",
    "\n",
    "    bkw = []\n",
    "    b_prev = {}\n",
    "\n",
    "    for i, x_i_plus in enumerate(reversed(words[1:]+(None,))):\n",
    "        b_curr = {}\n",
    "#         for st in states:\n",
    "#             if i == 0:\n",
    "#                 # base case for backward part\n",
    "#                 b_curr[st] = a[st][end_st]\n",
    "#             else:\n",
    "#                 b_curr[st] = sum(a[st][l]*e[l][x_i_plus]*b_prev[l] for l in states)\n",
    " \n",
    "#         bkw.insert(0,b_curr)\n",
    "#         b_prev = b_curr\n",
    " \n",
    "#     p_bkw = sum(a_0[l] * e[l][x[0]] * b_curr[l] for l in states)\n",
    " \n",
    "#     # merging the two parts\n",
    "#     posterior = []\n",
    "#     for i in range(L):\n",
    "#         posterior.append({st: fwd[i][st]*bkw[i][st]/p_fwd for st in states})\n",
    " \n",
    "#     assert p_fwd == p_bkw\n",
    "#     return fwd, bkw, posterior\n",
    "    return path_word, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-514-71e546550ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'sentence = \"ther sa pile of clothsing on the side of thee train treks\"\\nword_check, context_check = fwd_bkw(sentence, \\\\\\n            dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob)\\nprint \\'Original sentence: \\', sentence\\nprint \\'Word-level check: \\', \" \".join(word_check)\\nprint \\'Context-level check: \\', \" \".join(context_check)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2262\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-513-f494333cc271>\u001b[0m in \u001b[0;36mfwd_bkw\u001b[0;34m(words, dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob, num_word_suggestions)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprev_f_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_start_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_start_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mprev_f_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_belief\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_prev\u001b[0m\u001b[0;34m)\u001b[0m                             \u001b[0;34m+\u001b[0m \u001b[0mget_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransition_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_transition_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                                  \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprev_corrections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mf_curr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_emission_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprev_f_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-513-f494333cc271>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((k,))\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprev_f_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_start_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_start_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mprev_f_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_belief\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_prev\u001b[0m\u001b[0;34m)\u001b[0m                             \u001b[0;34m+\u001b[0m \u001b[0mget_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransition_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_transition_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                                  \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprev_corrections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mf_curr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_emission_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprev_f_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-453-b7a56365cce6>\u001b[0m in \u001b[0;36mget_belief\u001b[0;34m(prev_word, prev_belief)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprev_belief\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_belief\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO - confirm default value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sentence = \"ther sa pile of clothsing on the side of thee train treks\"\n",
    "word_check, context_check = fwd_bkw(sentence, \\\n",
    "            dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob)\n",
    "print 'Original sentence: ', sentence\n",
    "print 'Word-level check: ', \" \".join(word_check)\n",
    "print 'Context-level check: ', \" \".join(context_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correct_document_context(fname, dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                             transition_prob, default_transition_prob, \\\n",
    "                             context_threshold=5, num_word_suggestions=5000):\n",
    "    \n",
    "    doc_word_count = 0\n",
    "    unknown_word_count = 0\n",
    "    corrected_word_count = 0\n",
    "    mismatches = 0\n",
    "    \n",
    "    with open(fname) as file:\n",
    "        \n",
    "        for line in file:\n",
    "            \n",
    "            for sentence in line.split('.'):\n",
    "                \n",
    "                words = re.findall('[a-z]+', sentence.lower())  # separate by words by non-alphabetical characters\n",
    "                doc_word_count += len(words)\n",
    "                    \n",
    "                suggestion_w, suggestion_c = viterbi_num(words, dictionary, longest_word_length, \\\n",
    "                                                start_prob, default_start_prob, transition_prob, default_transition_prob)\n",
    "\n",
    "                for word_num in range(len(suggestion_w)):\n",
    "                \n",
    "                    if suggestion_w[word_num] is None:\n",
    "#                         print \"In line %i, the word < %s > was not found (no suggested corrections)\" % (i, doc_word)\n",
    "                        unknown_word_count += 1\n",
    "                    elif (suggestion_w[word_num]!=words[word_num]) or (suggestion_c[word_num]!=words[word_num]):\n",
    "#                         print \"In line %i, (%s %s) %s: word level correction is < %s >, context correction is < %s >\" \\\n",
    "#                                   % (i, word1, word2, doc_word, suggestion_w, suggestion_c)\n",
    "                        corrected_word_count += 1\n",
    "                        if suggestion_w[word_num]!=suggestion_c[word_num]:\n",
    "                            mismatches += 1\n",
    "        \n",
    "    print \"-----\"\n",
    "    print \"total words checked: %i\" % doc_word_count\n",
    "    print \"total unknown words: %i\" % unknown_word_count\n",
    "    print \"total potential errors found: %i\" % corrected_word_count\n",
    "    print \"total mismatches (word level vs. context): %i\" % mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>For testing:</b><p>\n",
    "Provide text file to correct, and give all best word suggestions (word level & context level) for all known words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 54.8 ms, total: 10.2 s\n",
      "Wall time: 10.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'sa',\n",
       "  'pile',\n",
       "  'of',\n",
       "  'clothing',\n",
       "  'on',\n",
       "  'the',\n",
       "  'side',\n",
       "  'of',\n",
       "  'thee',\n",
       "  'train',\n",
       "  'trees'],\n",
       " ['the',\n",
       "  'a',\n",
       "  'pile',\n",
       "  'of',\n",
       "  'clothing',\n",
       "  'on',\n",
       "  'the',\n",
       "  'side',\n",
       "  'of',\n",
       "  'the',\n",
       "  'train',\n",
       "  'tres'])"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "viterbi_num(\"ther sa pile of clothsing on the side of thee train treks\", \\\n",
    "            dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test.txt\n",
    "\n",
    "#this is a test\n",
    "#this is a test\n",
    "#here is a test\n",
    "#this is ax test\n",
    "#this is za test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "total words checked: 27\n",
      "total unknown words: 0\n",
      "total potential errors found: 27\n",
      "total mismatches (word level vs. context): 27\n"
     ]
    }
   ],
   "source": [
    "correct_document_context(\"testdata/test.txt\", \\\n",
    "                         dictionary, longest_word_length, start_prob, default_start_prob, \\\n",
    "                         transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['is', 'a', 'very'], ['i', 'sta', 'ted'])"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_num(\"is a very\", \\\n",
    "            dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['is', 'a', 'test'], ['i', 'sta', 'ted'])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_num(\"is a test\", \\\n",
    "            dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'test', 'this'], ['a', 'mere', 'dots'])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_num(\"a test this\", \\\n",
    "            dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'test', 'tube'], ['and', 'then', 'titi'])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi_num(\"a test tube\", \\\n",
    "            dictionary, longest_word_length, start_prob, default_start_prob, transition_prob, default_transition_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct_document_context(\"/Users/K-Lo/Desktop/OCRsample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct_document_context(\"/Users/K-Lo/Desktop/usingengsample.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct_document_context(\"/Users/K-Lo/Desktop/usingengsample.txt\", 50, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
